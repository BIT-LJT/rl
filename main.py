import torch
import torch.nn.functional as F
import numpy as np
from env import SamplingEnv2_0
from env_simplified import SamplingEnvSimplified
from env_configurable import SamplingEnvConfigurable
from agent import TransformerEncoder, MADDPGAgent # Assuming agent.py now contains the optimized versions
from agent_enhanced import MADDPGAgentEnhanced  # å¢å¼ºç‰ˆCriticæ”¯æŒ
from replay_buffer import PrioritizedReplayBuffer
from utils import render_env, plot_rewards, plot_reward_curve2, plot_agent_trajectories, debug_print
from lr_scheduler import LearningRateScheduler
import config
import os
import random

'''
åå°è¿è¡Œå‘½ä»¤ï¼š
CUDA_VISIBLE_DEVICES=3 nohup python main.py > output.txt 2>&1 &
'''
def set_seed(seed):
    """
    è®¾ç½®é¡¹ç›®ä¸­æ‰€æœ‰ç›¸å…³çš„éšæœºæ•°ç§å­ä»¥ç¡®ä¿å®éªŒçš„å¯å¤ç°æ€§ã€‚
    """
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)  # é€‚ç”¨äºå¤šGPUæƒ…å†µ
    
    # å…³é”®æ­¥éª¤ï¼šç¡®ä¿cuDNNçš„ç¡®å®šæ€§è¡Œä¸º
    # è¿™å¯èƒ½ä¼šå¯¹æ€§èƒ½æœ‰è½»å¾®å½±å“ï¼Œä½†å¯¹äºå¯å¤ç°æ€§è‡³å…³é‡è¦
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    
    print(f"âœ… æ‰€æœ‰éšæœºæ•°ç§å­å·²è®¾ç½®ä¸º: {seed}")


def get_local_obs(obs, agent_id, env):
    """
    Helper function to extract and normalize local observations for a single agent.
    """
    agent_pos_normalized = obs['agent_positions'][agent_id] / env.size
    agent_load_normalized = obs['agent_loads'][agent_id] / (env.agent_capacity[agent_id] + 1e-7)
    agent_energy_normalized = obs['agent_energy'][agent_id] / (env.agent_energy_max[agent_id] + 1e-7)
    
    # æ ¹æ®æ™ºèƒ½ä½“ç±»å‹ä½¿ç”¨ä¸åŒçš„æœ€å¤§å……ç”µæ—¶é—´è¿›è¡Œå½’ä¸€åŒ–
    max_charge_time = env.fast_charge_time if agent_id < 3 else env.slow_charge_time
    agent_charging_status_normalized = obs['agent_charging_status'][agent_id] / max_charge_time
    
    # æ·»åŠ æ¯ä¸ªä»»åŠ¡ç‚¹çš„å¤„ç†çŠ¶æ€ä¿¡æ¯ï¼ˆåŒæ­¥ç»™æ‰€æœ‰æ™ºèƒ½ä½“ï¼‰
    done_points_info = obs['done_points'].copy()  # 0è¡¨ç¤ºæœªå¤„ç†ï¼Œ1è¡¨ç¤ºå·²å¤„ç†ï¼ˆé‡‡é›†æˆ–è¶…æ—¶ï¼‰
    
    # æ·»åŠ å…¨å±€ä»»åŠ¡å®Œæˆä¿¡æ¯
    agent_obs_local_normalized = np.concatenate([
        agent_pos_normalized,
        [agent_load_normalized, agent_energy_normalized, obs['agent_task_status'][agent_id], 
         agent_charging_status_normalized, obs['task_completion_ratio'], 
         obs['all_tasks_completed'], obs['total_loaded_agents']],
        done_points_info  # æ–°å¢ï¼šæ¯ä¸ªä»»åŠ¡ç‚¹çš„å¤„ç†çŠ¶æ€ï¼ˆé‡‡é›†æˆ–è¶…æ—¶ï¼‰
    ])
    return agent_obs_local_normalized

def main():
    set_seed(config.RANDOM_SEED)
    # è®¾ç½®è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    debug_print(f"Using device: {device}")
    
    # è¾“å‡ºé€šä¿¡æ¨¡å¼é…ç½®
    comm_mode = "å³æ—¶é€šä¿¡(t)" if config.ENABLE_IMMEDIATE_COMMUNICATION else "å»¶è¿Ÿé€šä¿¡(t-1)"
    debug_print(f"ğŸ”„ æ™ºèƒ½ä½“é€šä¿¡æ¨¡å¼: {comm_mode}")
    
    # åˆ›å»ºæ–‡ä»¶å¤¹ - æ”¯æŒé…ç½®çš„è¾“å‡ºç›®å½•
    trajectories_dir = getattr(config, 'TRAJECTORIES_DIR', "trajectories")
    reward_plots_dir = getattr(config, 'REWARD_PLOTS_DIR', "reward_plots")
    
    if not os.path.exists(trajectories_dir):
        os.makedirs(trajectories_dir)
    if not os.path.exists(reward_plots_dir):
        os.makedirs(reward_plots_dir)

    # æ ¹æ®é…ç½®é€‰æ‹©ç¯å¢ƒç‰ˆæœ¬
    if config.ENVIRONMENT_TYPE == "simplified":
        env = SamplingEnvSimplified(num_points=config.NUM_POINTS, num_agents=config.NUM_AGENTS)
        debug_print("ğŸ”§ ä½¿ç”¨ç®€åŒ–ç‰ˆç¯å¢ƒ (åŸºç¡€è°ƒè¯•æ¨¡å¼)")
        env_type = "ç®€åŒ–ç‰ˆ"
    elif config.ENVIRONMENT_TYPE == "configurable":
        env = SamplingEnvConfigurable(num_points=config.NUM_POINTS, num_agents=config.NUM_AGENTS)
        debug_print("ğŸ§ª ä½¿ç”¨å¯é…ç½®å¥–åŠ±ç¯å¢ƒ (å¢é‡å¼å®éªŒæ¨¡å¼)")
        env_type = f"å¯é…ç½®ç‰ˆ-{config.REWARD_EXPERIMENT_LEVEL}"
    else:  # "full" or fallback
        env = SamplingEnv2_0(num_points=config.NUM_POINTS, num_agents=config.NUM_AGENTS)
        debug_print("ğŸš€ ä½¿ç”¨å®Œæ•´ç‰ˆç¯å¢ƒ (ç”Ÿäº§æ¨¡å¼)")
        env_type = "å®Œæ•´ç‰ˆ"
    num_agents = config.NUM_AGENTS
    num_points = config.NUM_POINTS
    action_dim = env.action_dim
    
    # å®šä¹‰çŠ¶æ€å’Œè§‚æµ‹ç»´åº¦
    agent_local_obs_dim = 9 + num_points  # [pos(2), load(1), energy(1), task_status(1), charge_status(1), task_completion_ratio(1), all_tasks_completed(1), total_loaded_agents(1)] + [done_points(30):ä»»åŠ¡ç‚¹å¤„ç†çŠ¶æ€]
    point_feature_dim = 5  # [pos(2), samples(1), time_windows(1), priority(1)]
    
    # æ™ºèƒ½ä½“çš„å®Œæ•´è§‚æµ‹ç»´åº¦ = æœ¬åœ°è§‚æµ‹ + transformerç‰¹å¾ + å…¶ä»–æ™ºèƒ½ä½“å±€éƒ¨è§‚æµ‹
    # è¿™ä¸ªç»´åº¦ç°åœ¨ç”±Agentå†…éƒ¨ç®¡ç†
    
    # å®ä¾‹åŒ–æ™ºèƒ½ä½“ - æ ¹æ®é…ç½®é€‰æ‹©æ ‡å‡†æˆ–å¢å¼ºç‰ˆæ¶æ„
    if config.USE_ENHANCED_CRITIC:
        debug_print("ğŸ”¬ ä½¿ç”¨å¢å¼ºç‰ˆCriticæ¶æ„ (æ¥æ”¶æ‰€æœ‰æ™ºèƒ½ä½“åŠ¨ä½œä¿¡æ¯)")
        AgentClass = MADDPGAgentEnhanced
    else:
        debug_print("ğŸ¯ ä½¿ç”¨æ ‡å‡†MADDPGæ¶æ„")
        AgentClass = MADDPGAgent
    
    agents = [AgentClass(
        agent_local_obs_dim=agent_local_obs_dim,
        point_feature_dim=point_feature_dim, 
        num_points=num_points,
        num_agents=num_agents,
        action_dim=action_dim,
        lr_actor=config.LR_ACTOR, 
        lr_critic=config.LR_CRITIC
    ).to(device) for _ in range(num_agents)]
    
    debug_print(f"ğŸ§  å·²åˆ›å»º{num_agents}ä¸ªé›†æˆTransformerçš„æ™ºèƒ½ä½“ï¼Œæ”¯æŒç«¯åˆ°ç«¯è®­ç»ƒ")

    replay_buffers = [PrioritizedReplayBuffer(
        capacity=config.CAPACITY, 
        alpha=config.PER_ALPHA, 
        beta=config.PER_BETA, 
        beta_increment=config.PER_BETA_INCREMENT
    ) for _ in range(num_agents)]
    
    # åˆå§‹åŒ–å­¦ä¹ ç‡è°ƒåº¦å™¨
    lr_scheduler = LearningRateScheduler(config.LR_ACTOR, config.LR_CRITIC)

    episode_rewards_log = []
    step_counter = 0
    noise_std = config.NOISE_STD_START

    for episode in range(config.NUM_EPISODES):
        # æ›´æ–°å­¦ä¹ ç‡
        if config.ENABLE_LR_SCHEDULING:
            actor_lr, critic_lr = lr_scheduler.get_learning_rates(episode)
            for agent in agents:
                agent.update_learning_rate(actor_lr, critic_lr)
            
            # è¾“å‡ºå­¦ä¹ ç‡ä¿¡æ¯
            if episode % 200 == 0:
                debug_print(f"Episode {episode}: Actor LR={actor_lr:.2e}, Critic LR={critic_lr:.2e}")
        
        obs = env.reset()
        done = False
        total_rewards = np.zeros(num_agents)

        # åˆå§‹åŒ–ä¸Šä¸€æ—¶åˆ»çš„å±€éƒ¨è§‚æµ‹ï¼ˆåœ¨ç¬¬ä¸€æ­¥æ—¶ä¼šè¢«å®é™…è§‚æµ‹è¦†ç›–ï¼‰
        last_local_obs = None

        num_steps = 0
        while (not done) and (num_steps < config.MAX_NUM_STEPS):
            num_steps += 1
            
            # å‡†å¤‡ä»»åŠ¡ç‚¹ç‰¹å¾
            point_features = np.hstack([
                obs['points'],
                obs['samples'].reshape(-1, 1),
                obs['time_windows'].reshape(-1, 1),
                obs['priority'].reshape(-1, 1)
            ])
            point_tensor = torch.FloatTensor(point_features).to(device)

            actions = []
            list_states_with_comm = [] # å­˜å‚¨åŒ…å«é€šä¿¡ä¿¡æ¯çš„å®Œæ•´çŠ¶æ€
            action_masks = obs['action_masks']

            # --- æ–°æ¶æ„: å‡†å¤‡æ™ºèƒ½ä½“å†³ç­–æ‰€éœ€çš„åŸå§‹æ•°æ® ---
            all_local_obs = []
            for i in range(num_agents):
                agent_obs_local_normalized = get_local_obs(obs, i, env)
                all_local_obs.append(agent_obs_local_normalized)
            
            # è°ƒè¯•ä¿¡æ¯ï¼šéªŒè¯ä»»åŠ¡ç‚¹çŠ¶æ€åŒæ­¥ï¼ˆæ¯100æ­¥æ‰“å°ä¸€æ¬¡ï¼‰
            if num_steps % 100 == 0:
                done_points_status = obs['done_points']
                processed_tasks = np.sum(done_points_status)
                total_tasks = len(done_points_status)
                debug_print(f"ğŸ”„ æ­¥éª¤ {num_steps}: ä»»åŠ¡ç‚¹çŠ¶æ€åŒæ­¥ - å·²å¤„ç†: {processed_tasks}/{total_tasks} ä¸ªä»»åŠ¡ç‚¹")
                debug_print(f"   å·²å¤„ç†çš„ä»»åŠ¡ç‚¹ID: {np.where(done_points_status == 1)[0].tolist()}")
                debug_print(f"   æ‰€æœ‰æ™ºèƒ½ä½“éƒ½èƒ½çœ‹åˆ°ç›¸åŒçš„ä»»åŠ¡ç‚¹çŠ¶æ€: {done_points_status.tolist()}")

            # --- ä½¿ç”¨æ–°çš„Agentæ¥å£è¿›è¡Œå†³ç­– (å†…ç½®Transformer) ---
            for i in range(num_agents):
                agent_local_obs_tensor = torch.FloatTensor(all_local_obs[i]).to(device)
                
                # è·å–å…¶ä»–æ™ºèƒ½ä½“çš„å±€éƒ¨è§‚æµ‹ - å¯é…ç½®çš„é€šä¿¡æ¨¡å¼
                if config.ENABLE_IMMEDIATE_COMMUNICATION:
                    # å³æ—¶é€šä¿¡æ¨¡å¼ï¼šä½¿ç”¨å½“å‰æ—¶åˆ»å…¶ä»–æ™ºèƒ½ä½“çš„çŠ¶æ€
                    other_agents_obs = np.concatenate([all_local_obs[j] for j in range(num_agents) if j != i])
                else:
                    # å»¶è¿Ÿé€šä¿¡æ¨¡å¼ï¼šä½¿ç”¨t-1æ—¶åˆ»å…¶ä»–æ™ºèƒ½ä½“çš„çŠ¶æ€ï¼ˆæ›´ç°å®ï¼‰
                    if last_local_obs is None:
                        # ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨å½“å‰æ‰€æœ‰æ™ºèƒ½ä½“çš„å±€éƒ¨è§‚æµ‹
                        other_agents_obs = np.concatenate([all_local_obs[j] for j in range(num_agents) if j != i])
                    else:
                        # åç»­æ­¥éª¤ï¼šä½¿ç”¨ä¸Šä¸€æ—¶åˆ»å…¶ä»–æ™ºèƒ½ä½“çš„å±€éƒ¨è§‚æµ‹
                        other_agents_obs = np.concatenate([last_local_obs[j] for j in range(num_agents) if j != i])
                
                other_agents_obs_tensor = torch.FloatTensor(other_agents_obs).to(device)
                mask_tensor = torch.BoolTensor(action_masks[i]).to(device)
                
                # è°ƒç”¨æ–°çš„Agentæ¥å£ - å†…éƒ¨ä¼šè°ƒç”¨Transformerå¹¶è¿›è¡Œç«¯åˆ°ç«¯å­¦ä¹ 
                with torch.no_grad():
                    action = agents[i].act(
                        agent_local_obs=agent_local_obs_tensor,
                        point_features=point_tensor, 
                        other_agents_obs=other_agents_obs_tensor,
                        noise_std=noise_std, 
                        action_mask=mask_tensor
                    )
                actions.append(action)
                
                # ä¸ºäº†å…¼å®¹ç°æœ‰çš„ç»éªŒå›æ”¾ï¼Œæˆ‘ä»¬ä»éœ€è¦æ„å»ºå®Œæ•´çš„çŠ¶æ€è¡¨ç¤º
                # ä½¿ç”¨Agentçš„è¾…åŠ©æ–¹æ³•ç”Ÿæˆå®Œæ•´çŠ¶æ€
                full_state = agents[i]._process_state(agent_local_obs_tensor, point_tensor, other_agents_obs_tensor)
                list_states_with_comm.append(full_state.cpu().numpy())

            # ä¿å­˜å½“å‰æ—¶åˆ»çš„å±€éƒ¨è§‚æµ‹ä¾›ä¸‹ä¸€æ—¶åˆ»ä½¿ç”¨
            current_local_obs = all_local_obs.copy()
            next_obs, rewards, done = env.step(actions, num_steps, config.MAX_NUM_STEPS)
            
            # --- æ„å»ºä¸‹ä¸€çŠ¶æ€è¡¨ç¤ºç”¨äºç»éªŒå­˜å‚¨ ---
            next_point_features = np.hstack([
                next_obs['points'],
                next_obs['samples'].reshape(-1, 1),
                next_obs['time_windows'].reshape(-1, 1),
                next_obs['priority'].reshape(-1, 1)
            ])
            next_point_tensor = torch.FloatTensor(next_point_features).to(device)
            
            # è·å–ä¸‹ä¸€æ—¶åˆ»æ‰€æœ‰æ™ºèƒ½ä½“çš„å±€éƒ¨è§‚æµ‹
            all_next_local_obs = []
            for i in range(num_agents):
                next_agent_obs_local_normalized = get_local_obs(next_obs, i, env)
                all_next_local_obs.append(next_agent_obs_local_normalized)

            # --- å­˜å‚¨ç»éªŒåˆ°å›æ”¾ç¼“å†²åŒº ---
            for i in range(num_agents):
                state_to_store = list_states_with_comm[i]
                
                # æ„é€ ä¸‹ä¸€æ—¶åˆ»çš„å®Œæ•´çŠ¶æ€è¡¨ç¤º
                next_agent_local_obs_tensor = torch.FloatTensor(all_next_local_obs[i]).to(device)
                
                # è·å–ä¸‹ä¸€çŠ¶æ€çš„é€šä¿¡ä¿¡æ¯ - ç¡®ä¿çŠ¶æ€è½¬ç§»çš„æ—¶é—´ä¸€è‡´æ€§
                if config.ENABLE_IMMEDIATE_COMMUNICATION:
                    # å³æ—¶é€šä¿¡æ¨¡å¼ï¼šä½¿ç”¨t+1æ—¶åˆ»å…¶ä»–æ™ºèƒ½ä½“çš„çŠ¶æ€ (all_next_local_obs)
                    next_other_agents_obs = np.concatenate([all_next_local_obs[j] for j in range(num_agents) if j != i])
                else:
                    # å»¶è¿Ÿé€šä¿¡æ¨¡å¼ï¼šä½¿ç”¨tæ—¶åˆ»å…¶ä»–æ™ºèƒ½ä½“çš„çŠ¶æ€ (current_local_obs)
                    next_other_agents_obs = np.concatenate([current_local_obs[j] for j in range(num_agents) if j != i])
                
                next_other_agents_obs_tensor = torch.FloatTensor(next_other_agents_obs).to(device)
                
                # ä½¿ç”¨Agentçš„è¾…åŠ©æ–¹æ³•ç”Ÿæˆä¸‹ä¸€çŠ¶æ€çš„å®Œæ•´è¡¨ç¤º
                with torch.no_grad():
                    next_state_with_comm = agents[i]._process_state(
                        next_agent_local_obs_tensor, 
                        next_point_tensor, 
                        next_other_agents_obs_tensor
                    ).cpu().numpy()

                # ä¸ºå¢å¼ºç‰ˆCriticå‡†å¤‡å…¨å±€åŠ¨ä½œä¿¡æ¯
                all_current_actions = np.array(actions) if config.USE_ENHANCED_CRITIC else None
                
                replay_buffers[i].push(
                    state_to_store, 
                    actions[i], 
                    rewards[i], 
                    next_state_with_comm, 
                    float(done),
                    all_current_actions,    # æ‰€æœ‰æ™ºèƒ½ä½“å½“å‰åŠ¨ä½œ
                    None                    # next_all_actionså°†åœ¨è®­ç»ƒæ—¶åŠ¨æ€è®¡ç®—
                )

            # æ›´æ–°ä¸Šä¸€æ—¶åˆ»çš„å±€éƒ¨è§‚æµ‹
            last_local_obs = current_local_obs

            # --- è®­ç»ƒæ­¥éª¤ (ä½¿ç”¨ä¼˜å…ˆç»éªŒå›æ”¾) ---
            if step_counter > config.BATCH_SIZE and step_counter % config.UPDATE_EVERY == 0:
                for agent_id in range(num_agents):
                    if len(replay_buffers[agent_id]) > config.BATCH_SIZE:
                        # ä»ä¼˜å…ˆç»éªŒå›æ”¾æ± é‡‡æ · - é€‚é…å¢å¼ºç‰ˆCriticçš„æ–°æ ¼å¼
                        sample_result = replay_buffers[agent_id].sample(config.BATCH_SIZE)
                        states, actions_b, rewards_b, next_states, dones_b, all_actions_b, next_all_actions_b, indices, is_weights = sample_result
                        
                        # æ ‡å‡†åŒ–å¥–åŠ±
                        rewards_b = (rewards_b - rewards_b.mean()) / (rewards_b.std() + 1e-7)
                        
                        # è½¬ç§»åˆ°è®¾å¤‡
                        states = states.to(device)
                        actions_b = actions_b.to(device) 
                        rewards_b = rewards_b.to(device)
                        next_states = next_states.to(device)
                        dones_b = dones_b.to(device)
                        is_weights = is_weights.to(device)
                        
                        # æ ¹æ®é…ç½®é€‰æ‹©ä¸åŒçš„æ›´æ–°æ–¹å¼
                        if config.USE_ENHANCED_CRITIC:
                            # å¢å¼ºç‰ˆCriticï¼šä¼ é€’å…¨å±€åŠ¨ä½œä¿¡æ¯
                            all_actions_b = all_actions_b.to(device)
                            
                            # åŠ¨æ€è®¡ç®—æ‰€æœ‰æ™ºèƒ½ä½“åœ¨ä¸‹ä¸€çŠ¶æ€çš„åŠ¨ä½œï¼ˆä½¿ç”¨ç›®æ ‡ç½‘ç»œï¼‰
                            next_all_actions_list = []
                            with torch.no_grad():
                                for temp_agent_id in range(num_agents):
                                    # ä½¿ç”¨æ¯ä¸ªæ™ºèƒ½ä½“çš„ç›®æ ‡actorç½‘ç»œè®¡ç®—ä¸‹ä¸€åŠ¨ä½œ
                                    next_action_logits = agents[temp_agent_id].actor_target(next_states)
                                    next_action = torch.argmax(next_action_logits, dim=1)
                                    next_all_actions_list.append(next_action)
                            
                            # å°†åˆ—è¡¨è½¬æ¢ä¸ºTensor [batch_size, num_agents]
                            next_all_actions_b = torch.stack(next_all_actions_list, dim=1).to(device)
                            
                            # Debugæ‰“å°ï¼ˆä»…åœ¨ç¬¬ä¸€æ¬¡æ›´æ–°æ—¶ï¼‰
                            if step_counter == config.BATCH_SIZE + 1 and agent_id == 0:
                                debug_print(f"ğŸ”§ å¢å¼ºç‰ˆCriticä¿®å¤ç”Ÿæ•ˆï¼šåŠ¨æ€è®¡ç®—next_all_actions shape: {next_all_actions_b.shape}")
                            
                            td_errors = agents[agent_id].update(
                                agent_id, states, actions_b, rewards_b, next_states, dones_b, 
                                all_actions_b, next_all_actions_b, is_weights
                            )
                        else:
                            # æ ‡å‡†MADDPGï¼šåŸæœ‰æ›´æ–°æ–¹å¼
                            td_errors = agents[agent_id].update(states, actions_b, rewards_b, next_states, dones_b, action_dim, is_weights)
                        
                        # æ›´æ–°ä¼˜å…ˆçº§
                        replay_buffers[agent_id].update_priorities(indices, td_errors)

            step_counter += 1
            total_rewards += rewards
            obs = next_obs

        noise_std = max(config.NOISE_STD_END, noise_std * config.NOISE_DECAY)
        charge_counts = env.agent_charge_counts
        
        # è·å–å½“å‰å­¦ä¹ ç‡ç”¨äºæ˜¾ç¤º
        lr_info = ""
        if config.ENABLE_LR_SCHEDULING:
            current_actor_lr, current_critic_lr = lr_scheduler.get_current_rates()
            lr_info = f", actor_lr = {current_actor_lr:.2e}, critic_lr = {current_critic_lr:.2e}"
        
        debug_print(f"Episode {episode}: total_rewards = {total_rewards}, noise = {noise_std:.4f}, charge_counts = {charge_counts}{lr_info}")
        
        # è¾“å‡ºä»»åŠ¡ç‚¹çŠ¶æ€åŒæ­¥çš„æœ€ç»ˆç»“æœ
        final_done_points = obs['done_points']
        processed_count = np.sum(final_done_points)
        total_count = len(final_done_points)
        debug_print(f"   ğŸ“Š [{env_type}ç¯å¢ƒ] ä»»åŠ¡ç‚¹å¤„ç†çŠ¶æ€: {processed_count}/{total_count} ä¸ªä»»åŠ¡ç‚¹å·²å¤„ç† (å¤„ç†ç‡: {processed_count/total_count*100:.1f}%)")
        if processed_count == total_count:
            debug_print(f"   ğŸ‰ æ‰€æœ‰ä»»åŠ¡ç‚¹å‡å·²å¤„ç†ï¼Transformerç«¯åˆ°ç«¯å­¦ä¹ å·²æ¿€æ´»ï¼Œä»»åŠ¡ç‚¹çŠ¶æ€å·²åŒæ­¥")
        
        episode_rewards_log.append(total_rewards.tolist())
        
        # å®šæœŸè¾“å‡ºåä½œåˆ†ææ‘˜è¦
        if (episode + 1) % 100 == 0:
            env.debug_print_collaboration_summary()
        
        # è®°å½•æ¯ä¸ªæ™ºèƒ½ä½“çš„æœ€ç»ˆè·¯å¾„ç‚¹
        for i in range(num_agents):
            final_position = obs['agent_positions'][i]
            if env.agent_paths[i]:
                 env.agent_paths[i].append(final_position)

        if (episode + 1) % 50 == 0:
            plot_agent_trajectories(env.agent_paths, env.points, env.central_station, episode_id=episode+1, foldername=trajectories_dir)

        if (episode + 1) % 100 == 0 and episode > 0:
            debug_print(f"\n--- Plotting rewards at episode {episode + 1} ---\n")
            plot_rewards(episode_rewards_log, filename=f"{reward_plots_dir}/reward_curve_ep{episode+1}.png")
            plot_reward_curve2(episode_rewards_log, filename=f"{reward_plots_dir}/reward_curve2_ep{episode+1}.png")

    debug_print("\n--- Plotting final rewards ---") # è¾“å‡ºæœ€ç»ˆå¥–åŠ±æ›²çº¿
    plot_rewards(episode_rewards_log, filename=f"{reward_plots_dir}/reward_curve_final.png")
    plot_reward_curve2(episode_rewards_log, filename=f"{reward_plots_dir}/reward_curve2_final.png")

    # ä¿å­˜æ•°æ®æ–‡ä»¶åˆ°è¾“å‡ºç›®å½•
    output_dir = getattr(config, 'OUTPUT_DIR', ".")
    
    all_rewards = np.array(episode_rewards_log)
    rewards_log_path = os.path.join(output_dir, 'rewards_log.npy')
    np.save(rewards_log_path, all_rewards)
    debug_print(f"ğŸ“Š å¥–åŠ±æ•°æ®å·²ä¿å­˜: {rewards_log_path}")
    
    # è¾“å‡ºæœ€ç»ˆåä½œåˆ†ææŠ¥å‘Š
    debug_print("\n" + "ğŸ¯" * 20)
    debug_print("æœ€ç»ˆåä½œåˆ†ææŠ¥å‘Š")
    debug_print("ğŸ¯" * 20)
    env.debug_print_collaboration_summary()
    
    # ä¿å­˜åä½œåˆ†ææ•°æ®
    final_analytics = env.get_collaboration_analytics()
    collaboration_path = os.path.join(output_dir, 'collaboration_analytics.npy')
    np.save(collaboration_path, final_analytics)
    debug_print(f"ğŸ“Š åä½œåˆ†ææ•°æ®å·²ä¿å­˜: {collaboration_path}")
    
    # === å¢é‡å¼å®éªŒç»“æœä¿å­˜ ===
    if config.ENVIRONMENT_TYPE == "configurable":
        from experiment_analyzer import ExperimentAnalyzer
        
        analyzer = ExperimentAnalyzer()
        
        # å‡†å¤‡è®­ç»ƒé…ç½®ä¿¡æ¯
        training_config = {
            'num_episodes': config.NUM_EPISODES,
            'num_agents': num_agents,
            'num_points': num_points,
            'batch_size': config.BATCH_SIZE,
            'lr_actor': config.LR_ACTOR,
            'lr_critic': config.LR_CRITIC,
            'use_enhanced_critic': config.USE_ENHANCED_CRITIC,
            'communication_mode': "immediate" if config.ENABLE_IMMEDIATE_COMMUNICATION else "delayed"
        }
        
        # è®¡ç®—æœ€ç»ˆæ€§èƒ½æŒ‡æ ‡
        final_rewards = np.array(episode_rewards_log)
        if final_rewards.ndim > 1:
            final_performance = np.mean(np.sum(final_rewards[-100:], axis=1))
            convergence_episode = len(final_rewards)
        else:
            final_performance = np.mean(final_rewards[-100:])
            convergence_episode = len(final_rewards)
        
        final_metrics = {
            'final_performance': final_performance,
            'convergence_episode': convergence_episode,
            'total_episodes': len(episode_rewards_log),
            'final_conflict_rate': final_analytics.get('conflict_rate', 0) if final_analytics else 0,
            'experiment_level': config.REWARD_EXPERIMENT_LEVEL
        }
        
        # ä¿å­˜å®éªŒç»“æœ
        result_file = analyzer.save_experiment_result(
            experiment_level=config.REWARD_EXPERIMENT_LEVEL,
            episode_rewards=episode_rewards_log,
            collaboration_analytics=final_analytics,
            training_config=training_config,
            final_metrics=final_metrics
        )
        
        debug_print(f"\nğŸ¯ å¢é‡å¼å®éªŒç»“æœå·²ä¿å­˜ï¼Œå¯ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è¿›è¡Œåˆ†æ:")
        debug_print(f"   python experiment_analyzer.py")
        debug_print(f"   æˆ–åœ¨Pythonä¸­ï¼šfrom experiment_analyzer import analyze_latest_experiments; analyze_latest_experiments()")
    
if __name__ == "__main__":
    main()
