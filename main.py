import torch
import torch.nn.functional as F
import numpy as np
from env import SamplingEnv2_0
from env_simplified import SamplingEnvSimplified
from env_configurable import SamplingEnvConfigurable
from agent import TransformerEncoder, MADDPGAgent # Assuming agent.py now contains the optimized versions
from agent_enhanced import MADDPGAgentEnhanced  # å¢å¼ºç‰ˆCriticæ”¯æŒ
from replay_buffer import PrioritizedReplayBuffer
from utils import render_env, plot_rewards, plot_reward_curve2, plot_agent_trajectories, debug_print
from lr_scheduler import LearningRateScheduler
import config
import os
import random
from datetime import datetime
from torch.utils.tensorboard import SummaryWriter

'''
åå°è¿è¡Œå‘½ä»¤ï¼š
CUDA_VISIBLE_DEVICES=3 nohup python main.py > output.txt 2>&1 &
'''
def set_seed(seed):
    """
    è®¾ç½®é¡¹ç›®ä¸­æ‰€æœ‰ç›¸å…³çš„éšæœºæ•°ç§å­ä»¥ç¡®ä¿å®éªŒçš„å¯å¤ç°æ€§ã€‚
    """
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)  # é€‚ç”¨äºå¤šGPUæƒ…å†µ
    
    # å…³é”®æ­¥éª¤ï¼šç¡®ä¿cuDNNçš„ç¡®å®šæ€§è¡Œä¸º
    # è¿™å¯èƒ½ä¼šå¯¹æ€§èƒ½æœ‰è½»å¾®å½±å“ï¼Œä½†å¯¹äºå¯å¤ç°æ€§è‡³å…³é‡è¦
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    
    print(f"âœ… æ‰€æœ‰éšæœºæ•°ç§å­å·²è®¾ç½®ä¸º: {seed}")


def save_model_checkpoint(agents, episode, checkpoints_dir, config_dict, episode_rewards_log, noise_std):
    """
    ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹ï¼ŒåŒ…æ‹¬æ‰€æœ‰æ™ºèƒ½ä½“çš„ç½‘ç»œå‚æ•°å’Œè®­ç»ƒçŠ¶æ€
    
    Args:
        agents: æ™ºèƒ½ä½“åˆ—è¡¨
        episode: å½“å‰episodeæ•°
        checkpoints_dir: æ£€æŸ¥ç‚¹ä¿å­˜ç›®å½•
        config_dict: è®­ç»ƒé…ç½®å­—å…¸
        episode_rewards_log: å†å²å¥–åŠ±è®°å½•
        noise_std: å½“å‰å™ªå£°æ ‡å‡†å·®
    """
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    checkpoint_name = f"checkpoint_ep{episode}_{timestamp}"
    checkpoint_path = os.path.join(checkpoints_dir, checkpoint_name)
    
    if not os.path.exists(checkpoint_path):
        os.makedirs(checkpoint_path)
    
    # ä¿å­˜æ‰€æœ‰æ™ºèƒ½ä½“çš„æ¨¡å‹å‚æ•°
    for i, agent in enumerate(agents):
        agent_checkpoint = {
            'episode': episode,
            'agent_id': i,
            'actor_state_dict': agent.actor.state_dict(),
            'critic_state_dict': agent.critic.state_dict(),
            'actor_target_state_dict': agent.actor_target.state_dict(),
            'critic_target_state_dict': agent.critic_target.state_dict(),
            'actor_optimizer_state_dict': agent.actor_optimizer.state_dict(),
            'critic_optimizer_state_dict': agent.critic_optimizer.state_dict(),
            'transformer_state_dict': agent.transformer.state_dict() if hasattr(agent, 'transformer') else None,
            'current_learning_rates': agent.get_learning_rates() if hasattr(agent, 'get_learning_rates') else None
        }
        
        agent_file = os.path.join(checkpoint_path, f"agent_{i}.pth")
        torch.save(agent_checkpoint, agent_file)
    
    # ä¿å­˜è®­ç»ƒçŠ¶æ€å’Œé…ç½®ä¿¡æ¯
    training_state = {
        'episode': episode,
        'noise_std': noise_std,
        'config': config_dict,
        'episode_rewards_log': episode_rewards_log,
        'total_episodes_planned': config_dict.get('NUM_EPISODES', 3000),
        'training_progress': episode / config_dict.get('NUM_EPISODES', 3000) * 100,
        'checkpoint_timestamp': timestamp,
        'environment_type': config_dict.get('ENVIRONMENT_TYPE', 'full'),
        'random_seed': config_dict.get('RANDOM_SEED', 123)
    }
    
    training_file = os.path.join(checkpoint_path, "training_state.pth")
    torch.save(training_state, training_file)
    
    # ä¿å­˜å¯è¯»çš„é…ç½®æ–‡ä»¶
    config_file = os.path.join(checkpoint_path, "config_info.txt")
    with open(config_file, 'w', encoding='utf-8') as f:
        f.write(f"æ¨¡å‹æ£€æŸ¥ç‚¹ä¿¡æ¯\n")
        f.write(f"=" * 50 + "\n")
        f.write(f"ä¿å­˜æ—¶é—´: {timestamp}\n")
        f.write(f"è®­ç»ƒè½®æ•°: {episode}\n")
        f.write(f"è®­ç»ƒè¿›åº¦: {episode / config_dict.get('NUM_EPISODES', 3000) * 100:.1f}%\n")
        f.write(f"å½“å‰å™ªå£°: {noise_std:.6f}\n")
        f.write(f"ç¯å¢ƒç±»å‹: {config_dict.get('ENVIRONMENT_TYPE', 'full')}\n")
        f.write(f"éšæœºç§å­: {config_dict.get('RANDOM_SEED', 123)}\n")
        f.write(f"æ™ºèƒ½ä½“æ•°é‡: {len(agents)}\n")
        if episode_rewards_log:
            recent_rewards = episode_rewards_log[-10:] if len(episode_rewards_log) >= 10 else episode_rewards_log
            avg_recent = np.mean([np.sum(r) if isinstance(r, (list, np.ndarray)) else r for r in recent_rewards])
            f.write(f"è¿‘æœŸå¹³å‡å¥–åŠ±: {avg_recent:.2f}\n")
        f.write(f"\nè®­ç»ƒé…ç½®å‚æ•°:\n")
        for key, value in config_dict.items():
            f.write(f"  {key}: {value}\n")
    
    debug_print(f"ğŸ’¾ æ¨¡å‹æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")
    debug_print(f"   åŒ…å«: {len(agents)}ä¸ªæ™ºèƒ½ä½“ + è®­ç»ƒçŠ¶æ€ + é…ç½®ä¿¡æ¯")
    
    return checkpoint_path

def load_model_checkpoint(agents, checkpoint_path, device):
    """
    ä»æ£€æŸ¥ç‚¹åŠ è½½æ¨¡å‹å‚æ•°
    
    Args:
        agents: æ™ºèƒ½ä½“åˆ—è¡¨
        checkpoint_path: æ£€æŸ¥ç‚¹è·¯å¾„
        device: è®¾å¤‡ (cuda/cpu)
    
    Returns:
        training_state: è®­ç»ƒçŠ¶æ€å­—å…¸
    """
    if not os.path.exists(checkpoint_path):
        raise FileNotFoundError(f"æ£€æŸ¥ç‚¹è·¯å¾„ä¸å­˜åœ¨: {checkpoint_path}")
    
    # åŠ è½½è®­ç»ƒçŠ¶æ€
    training_file = os.path.join(checkpoint_path, "training_state.pth")
    if os.path.exists(training_file):
        training_state = torch.load(training_file, map_location=device)
    else:
        training_state = None
    
    # åŠ è½½æ¯ä¸ªæ™ºèƒ½ä½“çš„å‚æ•°
    loaded_agents = 0
    for i, agent in enumerate(agents):
        agent_file = os.path.join(checkpoint_path, f"agent_{i}.pth")
        if os.path.exists(agent_file):
            checkpoint = torch.load(agent_file, map_location=device)
            
            # åŠ è½½ç½‘ç»œå‚æ•°
            agent.actor.load_state_dict(checkpoint['actor_state_dict'])
            agent.critic.load_state_dict(checkpoint['critic_state_dict'])
            agent.actor_target.load_state_dict(checkpoint['actor_target_state_dict'])
            agent.critic_target.load_state_dict(checkpoint['critic_target_state_dict'])
            
            # åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€
            agent.actor_optimizer.load_state_dict(checkpoint['actor_optimizer_state_dict'])
            agent.critic_optimizer.load_state_dict(checkpoint['critic_optimizer_state_dict'])
            
            # åŠ è½½Transformerå‚æ•°ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if checkpoint['transformer_state_dict'] and hasattr(agent, 'transformer'):
                agent.transformer.load_state_dict(checkpoint['transformer_state_dict'])
            
            loaded_agents += 1
        else:
            debug_print(f"âš ï¸ æœªæ‰¾åˆ°æ™ºèƒ½ä½“{i}çš„æ£€æŸ¥ç‚¹æ–‡ä»¶: {agent_file}")
    
    debug_print(f"ğŸ“‚ æ¨¡å‹æ£€æŸ¥ç‚¹åŠ è½½å®Œæˆ: {checkpoint_path}")
    debug_print(f"   æˆåŠŸåŠ è½½: {loaded_agents}/{len(agents)} ä¸ªæ™ºèƒ½ä½“")
    
    if training_state:
        debug_print(f"   è®­ç»ƒè½®æ•°: {training_state.get('episode', 'unknown')}")
        debug_print(f"   å™ªå£°æ ‡å‡†å·®: {training_state.get('noise_std', 'unknown')}")
        debug_print(f"   ç¯å¢ƒç±»å‹: {training_state.get('environment_type', 'unknown')}")
    
    return training_state

def get_local_obs(obs, agent_id, env):
    """
    Helper function to extract and normalize local observations for a single agent.
    """
    agent_pos_normalized = obs['agent_positions'][agent_id] / env.size
    agent_load_normalized = obs['agent_loads'][agent_id] / (env.agent_capacity[agent_id] + 1e-7)
    agent_energy_normalized = obs['agent_energy'][agent_id] / (env.agent_energy_max[agent_id] + 1e-7)
    
    # æ ¹æ®æ™ºèƒ½ä½“ç±»å‹ä½¿ç”¨ä¸åŒçš„æœ€å¤§å……ç”µæ—¶é—´è¿›è¡Œå½’ä¸€åŒ–
    max_charge_time = env.fast_charge_time if agent_id < 3 else env.slow_charge_time
    agent_charging_status_normalized = obs['agent_charging_status'][agent_id] / max_charge_time
    
    # æ·»åŠ æ¯ä¸ªä»»åŠ¡ç‚¹çš„å¤„ç†çŠ¶æ€ä¿¡æ¯ï¼ˆåŒæ­¥ç»™æ‰€æœ‰æ™ºèƒ½ä½“ï¼‰
    done_points_info = obs['done_points'].copy()  # 0è¡¨ç¤ºæœªå¤„ç†ï¼Œ1è¡¨ç¤ºå·²å¤„ç†ï¼ˆé‡‡é›†æˆ–è¶…æ—¶ï¼‰
    
    # --- æ–°å¢: æ™ºèƒ½ä½“æ„å›¾ä¿¡æ¯ï¼ˆè‡ªå·±çš„ä¸Šä¸€æ—¶åˆ»åŠ¨ä½œï¼‰---
    # å°†åŠ¨ä½œå½’ä¸€åŒ–åˆ° [0, 1] åŒºé—´ï¼Œæ–¹ä¾¿ç¥ç»ç½‘ç»œå¤„ç†
    own_last_action_normalized = obs['agent_last_actions'][agent_id] / (env.action_dim - 1) if config.ENABLE_AGENT_INTENTION_OBS else 0.0
    
    # æ·»åŠ å…¨å±€ä»»åŠ¡å®Œæˆä¿¡æ¯
    agent_obs_local_normalized = np.concatenate([
        agent_pos_normalized,
        [agent_load_normalized, agent_energy_normalized, obs['agent_task_status'][agent_id], 
         agent_charging_status_normalized, obs['task_completion_ratio'], 
         obs['all_tasks_completed'], obs['total_loaded_agents'], own_last_action_normalized],  # æ–°å¢è‡ªå·±çš„ä¸Šä¸€æ—¶åˆ»åŠ¨ä½œ
        done_points_info  # æ–°å¢ï¼šæ¯ä¸ªä»»åŠ¡ç‚¹çš„å¤„ç†çŠ¶æ€ï¼ˆé‡‡é›†æˆ–è¶…æ—¶ï¼‰
    ])
    return agent_obs_local_normalized

def main():
    set_seed(config.RANDOM_SEED)
    # è®¾ç½®è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    debug_print(f"Using device: {device}")
    
    # è¾“å‡ºé€šä¿¡æ¨¡å¼é…ç½®
    comm_mode = "å³æ—¶é€šä¿¡(t)" if config.ENABLE_IMMEDIATE_COMMUNICATION else "å»¶è¿Ÿé€šä¿¡(t-1)"
    debug_print(f"ğŸ”„ æ™ºèƒ½ä½“é€šä¿¡æ¨¡å¼: {comm_mode}")
    
    # åˆ›å»ºæ–‡ä»¶å¤¹ - æ”¯æŒé…ç½®çš„è¾“å‡ºç›®å½•
    trajectories_dir = getattr(config, 'TRAJECTORIES_DIR', "trajectories")
    reward_plots_dir = getattr(config, 'REWARD_PLOTS_DIR', "reward_plots")
    checkpoints_dir = getattr(config, 'CHECKPOINTS_DIR', "checkpoints")
    
    if not os.path.exists(trajectories_dir):
        os.makedirs(trajectories_dir)
    if not os.path.exists(reward_plots_dir):
        os.makedirs(reward_plots_dir)
    if not os.path.exists(checkpoints_dir):
        os.makedirs(checkpoints_dir)
    
    # --- æ–°å¢: è®¾ç½®TensorBoardæ—¥å¿—è®°å½• ---
    writer = None
    if config.ENABLE_TENSORBOARD:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        tensorboard_dir = f"runs/MADDPG_{config.ENVIRONMENT_TYPE}_{timestamp}_seed{config.RANDOM_SEED}"
        if not os.path.exists("runs"):
            os.makedirs("runs")
        
        writer = SummaryWriter(tensorboard_dir)
        debug_print(f"ğŸ“Š TensorBoardæ—¥å¿—ç›®å½•: {tensorboard_dir}")
        debug_print(f"ğŸ’¡ æŸ¥çœ‹è®­ç»ƒæƒ…å†µ: tensorboard --logdir=runs --port=6006")

    # æ ¹æ®é…ç½®é€‰æ‹©ç¯å¢ƒç‰ˆæœ¬
    if config.ENVIRONMENT_TYPE == "simplified":
        env = SamplingEnvSimplified(num_points=config.NUM_POINTS, num_agents=config.NUM_AGENTS)
        debug_print("ğŸ”§ ä½¿ç”¨ç®€åŒ–ç‰ˆç¯å¢ƒ (åŸºç¡€è°ƒè¯•æ¨¡å¼)")
        env_type = "ç®€åŒ–ç‰ˆ"
    elif config.ENVIRONMENT_TYPE == "configurable":
        env = SamplingEnvConfigurable(num_points=config.NUM_POINTS, num_agents=config.NUM_AGENTS)
        debug_print("ğŸ§ª ä½¿ç”¨å¯é…ç½®å¥–åŠ±ç¯å¢ƒ (å¢é‡å¼å®éªŒæ¨¡å¼)")
        env_type = f"å¯é…ç½®ç‰ˆ-{config.REWARD_EXPERIMENT_LEVEL}"
    else:  # "full" or fallback
        env = SamplingEnv2_0(num_points=config.NUM_POINTS, num_agents=config.NUM_AGENTS)
        debug_print("ğŸš€ ä½¿ç”¨å®Œæ•´ç‰ˆç¯å¢ƒ (ç”Ÿäº§æ¨¡å¼)")
        env_type = "å®Œæ•´ç‰ˆ"
    num_agents = config.NUM_AGENTS
    num_points = config.NUM_POINTS
    action_dim = env.action_dim
    
    # å®šä¹‰çŠ¶æ€å’Œè§‚æµ‹ç»´åº¦
    # --- ä¿®æ”¹: å¢åŠ æ™ºèƒ½ä½“æ„å›¾ç»´åº¦ ---
    intention_dim = 1 if config.ENABLE_AGENT_INTENTION_OBS else 0  # è‡ªå·±çš„ä¸Šä¸€æ—¶åˆ»åŠ¨ä½œ
    agent_local_obs_dim = 9 + intention_dim + num_points  # [pos(2), load(1), energy(1), task_status(1), charge_status(1), task_completion_ratio(1), all_tasks_completed(1), total_loaded_agents(1), own_last_action(1)] + [done_points(30):ä»»åŠ¡ç‚¹å¤„ç†çŠ¶æ€]
    
    debug_print(f"ğŸ§  æ™ºèƒ½ä½“æ„å›¾è§‚æµ‹: {'å¯ç”¨' if config.ENABLE_AGENT_INTENTION_OBS else 'ç¦ç”¨'}")
    debug_print(f"â±ï¸ å»¶è¿Ÿç­–ç•¥æ›´æ–°: {'å¯ç”¨' if config.ENABLE_DELAYED_POLICY_UPDATES else 'ç¦ç”¨'} (Critic:Actor = {config.ACTOR_UPDATE_FREQUENCY}:1)" if config.USE_ENHANCED_CRITIC else "")
    point_feature_dim = 5  # [pos(2), samples(1), time_windows(1), priority(1)]
    
    # æ™ºèƒ½ä½“çš„å®Œæ•´è§‚æµ‹ç»´åº¦ = æœ¬åœ°è§‚æµ‹ + transformerç‰¹å¾ + å…¶ä»–æ™ºèƒ½ä½“å±€éƒ¨è§‚æµ‹
    # è¿™ä¸ªç»´åº¦ç°åœ¨ç”±Agentå†…éƒ¨ç®¡ç†
    
    # å®ä¾‹åŒ–æ™ºèƒ½ä½“ - æ ¹æ®é…ç½®é€‰æ‹©æ ‡å‡†æˆ–å¢å¼ºç‰ˆæ¶æ„
    if config.USE_ENHANCED_CRITIC:
        debug_print("ğŸ”¬ ä½¿ç”¨å¢å¼ºç‰ˆCriticæ¶æ„ (æ¥æ”¶æ‰€æœ‰æ™ºèƒ½ä½“åŠ¨ä½œä¿¡æ¯)")
        AgentClass = MADDPGAgentEnhanced
    else:
        debug_print("ğŸ¯ ä½¿ç”¨æ ‡å‡†MADDPGæ¶æ„")
        AgentClass = MADDPGAgent
    
    agents = [AgentClass(
        agent_local_obs_dim=agent_local_obs_dim,
        point_feature_dim=point_feature_dim, 
        num_points=num_points,
        num_agents=num_agents,
        action_dim=action_dim,
        lr_actor=config.LR_ACTOR, 
        lr_critic=config.LR_CRITIC
    ).to(device) for _ in range(num_agents)]
    
    debug_print(f"ğŸ§  å·²åˆ›å»º{num_agents}ä¸ªé›†æˆTransformerçš„æ™ºèƒ½ä½“ï¼Œæ”¯æŒç«¯åˆ°ç«¯è®­ç»ƒ")

    replay_buffers = [PrioritizedReplayBuffer(
        capacity=config.CAPACITY, 
        alpha=config.PER_ALPHA, 
        beta=config.PER_BETA, 
        beta_increment=config.PER_BETA_INCREMENT
    ) for _ in range(num_agents)]
    
    # åˆå§‹åŒ–å­¦ä¹ ç‡è°ƒåº¦å™¨
    lr_scheduler = LearningRateScheduler(config.LR_ACTOR, config.LR_CRITIC)

    episode_rewards_log = []
    step_counter = 0
    # --- æ–°å¢: å»¶è¿Ÿç­–ç•¥æ›´æ–°è®¡æ•°å™¨ ---
    update_counter = 0  # è·Ÿè¸ªæ€»æ›´æ–°æ¬¡æ•°ï¼Œç”¨äºå»¶è¿Ÿç­–ç•¥æ›´æ–°
    noise_std = config.NOISE_STD_START

    for episode in range(config.NUM_EPISODES):
        # æ›´æ–°å­¦ä¹ ç‡
        if config.ENABLE_LR_SCHEDULING:
            actor_lr, critic_lr = lr_scheduler.get_learning_rates(episode)
            for agent in agents:
                agent.update_learning_rate(actor_lr, critic_lr)
            
            # è¾“å‡ºå­¦ä¹ ç‡ä¿¡æ¯
            if episode % 200 == 0:
                debug_print(f"Episode {episode}: Actor LR={actor_lr:.2e}, Critic LR={critic_lr:.2e}")
        
        obs = env.reset()
        done = False
        total_rewards = np.zeros(num_agents)

        # --- ä¿®æ”¹ç‚¹1: åœ¨episodeå¼€å§‹æ—¶åˆå§‹åŒ–æŸå¤±è®°å½•åˆ—è¡¨ ---
        # ç¡®ä¿æŸå¤±å€¼åœ¨æ•´ä¸ªepisodeä¸­æŒç»­ç´¯ç§¯ï¼Œè€Œä¸æ˜¯è¢«é‡å¤é‡ç½®
        critic_losses_episode = {i: [] for i in range(num_agents)}
        actor_losses_episode = {i: [] for i in range(num_agents)}
        
        # --- æ–°å¢: åˆå§‹åŒ–æ¢¯åº¦èŒƒæ•°è®°å½•åˆ—è¡¨ ---
        critic_grad_norms_episode = {i: [] for i in range(num_agents)}
        actor_grad_norms_episode = {i: [] for i in range(num_agents)}

        # åˆå§‹åŒ–ä¸Šä¸€æ—¶åˆ»çš„å±€éƒ¨è§‚æµ‹ï¼ˆåœ¨ç¬¬ä¸€æ­¥æ—¶ä¼šè¢«å®é™…è§‚æµ‹è¦†ç›–ï¼‰
        last_local_obs = None

        num_steps = 0
        while (not done) and (num_steps < config.MAX_NUM_STEPS):
            num_steps += 1
            
            # å‡†å¤‡ä»»åŠ¡ç‚¹ç‰¹å¾
            point_features = np.hstack([
                obs['points'],
                obs['samples'].reshape(-1, 1),
                obs['time_windows'].reshape(-1, 1),
                obs['priority'].reshape(-1, 1)
            ])
            point_tensor = torch.FloatTensor(point_features).to(device)

            actions = []
            list_states_with_comm = [] # å­˜å‚¨åŒ…å«é€šä¿¡ä¿¡æ¯çš„å®Œæ•´çŠ¶æ€
            action_masks = obs['action_masks']

            # --- æ–°æ¶æ„: å‡†å¤‡æ™ºèƒ½ä½“å†³ç­–æ‰€éœ€çš„åŸå§‹æ•°æ® ---
            all_local_obs = []
            for i in range(num_agents):
                agent_obs_local_normalized = get_local_obs(obs, i, env)
                all_local_obs.append(agent_obs_local_normalized)
            
            # è°ƒè¯•ä¿¡æ¯ï¼šéªŒè¯ä»»åŠ¡ç‚¹çŠ¶æ€åŒæ­¥ï¼ˆæ¯100æ­¥æ‰“å°ä¸€æ¬¡ï¼‰
            if num_steps % 100 == 0:
                done_points_status = obs['done_points']
                processed_tasks = np.sum(done_points_status)
                total_tasks = len(done_points_status)
                debug_print(f"ğŸ”„ æ­¥éª¤ {num_steps}: ä»»åŠ¡ç‚¹çŠ¶æ€åŒæ­¥ - å·²å¤„ç†: {processed_tasks}/{total_tasks} ä¸ªä»»åŠ¡ç‚¹")
                debug_print(f"   å·²å¤„ç†çš„ä»»åŠ¡ç‚¹ID: {np.where(done_points_status == 1)[0].tolist()}")
                debug_print(f"   æ‰€æœ‰æ™ºèƒ½ä½“éƒ½èƒ½çœ‹åˆ°ç›¸åŒçš„ä»»åŠ¡ç‚¹çŠ¶æ€: {done_points_status.tolist()}")

            # --- ä½¿ç”¨æ–°çš„Agentæ¥å£è¿›è¡Œå†³ç­– (å†…ç½®Transformer) ---
            for i in range(num_agents):
                agent_local_obs_tensor = torch.FloatTensor(all_local_obs[i]).to(device)
                
                # è·å–å…¶ä»–æ™ºèƒ½ä½“çš„å±€éƒ¨è§‚æµ‹ - å¯é…ç½®çš„é€šä¿¡æ¨¡å¼
                if config.ENABLE_IMMEDIATE_COMMUNICATION:
                    # å³æ—¶é€šä¿¡æ¨¡å¼ï¼šä½¿ç”¨å½“å‰æ—¶åˆ»å…¶ä»–æ™ºèƒ½ä½“çš„çŠ¶æ€
                    other_agents_obs = np.concatenate([all_local_obs[j] for j in range(num_agents) if j != i])
                else:
                    # å»¶è¿Ÿé€šä¿¡æ¨¡å¼ï¼šä½¿ç”¨t-1æ—¶åˆ»å…¶ä»–æ™ºèƒ½ä½“çš„çŠ¶æ€ï¼ˆæ›´ç°å®ï¼‰
                    if last_local_obs is None:
                        # ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨å½“å‰æ‰€æœ‰æ™ºèƒ½ä½“çš„å±€éƒ¨è§‚æµ‹
                        other_agents_obs = np.concatenate([all_local_obs[j] for j in range(num_agents) if j != i])
                    else:
                        # åç»­æ­¥éª¤ï¼šä½¿ç”¨ä¸Šä¸€æ—¶åˆ»å…¶ä»–æ™ºèƒ½ä½“çš„å±€éƒ¨è§‚æµ‹
                        other_agents_obs = np.concatenate([last_local_obs[j] for j in range(num_agents) if j != i])
                
                other_agents_obs_tensor = torch.FloatTensor(other_agents_obs).to(device)
                mask_tensor = torch.BoolTensor(action_masks[i]).to(device)
                
                # è°ƒç”¨æ–°çš„Agentæ¥å£ - å†…éƒ¨ä¼šè°ƒç”¨Transformerå¹¶è¿›è¡Œç«¯åˆ°ç«¯å­¦ä¹ 
                with torch.no_grad():
                    action = agents[i].act(
                        agent_local_obs=agent_local_obs_tensor,
                        point_features=point_tensor, 
                        other_agents_obs=other_agents_obs_tensor,
                        noise_std=noise_std, 
                        action_mask=mask_tensor
                    )
                actions.append(action)
                
                # ä¸ºäº†å…¼å®¹ç°æœ‰çš„ç»éªŒå›æ”¾ï¼Œæˆ‘ä»¬ä»éœ€è¦æ„å»ºå®Œæ•´çš„çŠ¶æ€è¡¨ç¤º
                # ä½¿ç”¨Agentçš„è¾…åŠ©æ–¹æ³•ç”Ÿæˆå®Œæ•´çŠ¶æ€
                full_state = agents[i]._process_state(agent_local_obs_tensor, point_tensor, other_agents_obs_tensor)
                list_states_with_comm.append(full_state.cpu().numpy())

            # ä¿å­˜å½“å‰æ—¶åˆ»çš„å±€éƒ¨è§‚æµ‹ä¾›ä¸‹ä¸€æ—¶åˆ»ä½¿ç”¨
            current_local_obs = all_local_obs.copy()
            next_obs, rewards, done = env.step(actions, num_steps, config.MAX_NUM_STEPS)
            
            # --- æ„å»ºä¸‹ä¸€çŠ¶æ€è¡¨ç¤ºç”¨äºç»éªŒå­˜å‚¨ ---
            next_point_features = np.hstack([
                next_obs['points'],
                next_obs['samples'].reshape(-1, 1),
                next_obs['time_windows'].reshape(-1, 1),
                next_obs['priority'].reshape(-1, 1)
            ])
            next_point_tensor = torch.FloatTensor(next_point_features).to(device)
            
            # è·å–ä¸‹ä¸€æ—¶åˆ»æ‰€æœ‰æ™ºèƒ½ä½“çš„å±€éƒ¨è§‚æµ‹
            all_next_local_obs = []
            for i in range(num_agents):
                next_agent_obs_local_normalized = get_local_obs(next_obs, i, env)
                all_next_local_obs.append(next_agent_obs_local_normalized)

            # --- å­˜å‚¨ç»éªŒåˆ°å›æ”¾ç¼“å†²åŒº ---
            for i in range(num_agents):
                state_to_store = list_states_with_comm[i]
                
                # æ„é€ ä¸‹ä¸€æ—¶åˆ»çš„å®Œæ•´çŠ¶æ€è¡¨ç¤º
                next_agent_local_obs_tensor = torch.FloatTensor(all_next_local_obs[i]).to(device)
                
                # è·å–ä¸‹ä¸€çŠ¶æ€çš„é€šä¿¡ä¿¡æ¯ - ç¡®ä¿çŠ¶æ€è½¬ç§»çš„æ—¶é—´ä¸€è‡´æ€§
                if config.ENABLE_IMMEDIATE_COMMUNICATION:
                    # å³æ—¶é€šä¿¡æ¨¡å¼ï¼šä½¿ç”¨t+1æ—¶åˆ»å…¶ä»–æ™ºèƒ½ä½“çš„çŠ¶æ€ (all_next_local_obs)
                    next_other_agents_obs = np.concatenate([all_next_local_obs[j] for j in range(num_agents) if j != i])
                else:
                    # å»¶è¿Ÿé€šä¿¡æ¨¡å¼ï¼šä½¿ç”¨tæ—¶åˆ»å…¶ä»–æ™ºèƒ½ä½“çš„çŠ¶æ€ (current_local_obs)
                    next_other_agents_obs = np.concatenate([current_local_obs[j] for j in range(num_agents) if j != i])
                
                next_other_agents_obs_tensor = torch.FloatTensor(next_other_agents_obs).to(device)
                
                # ä½¿ç”¨Agentçš„è¾…åŠ©æ–¹æ³•ç”Ÿæˆä¸‹ä¸€çŠ¶æ€çš„å®Œæ•´è¡¨ç¤º
                with torch.no_grad():
                    next_state_with_comm = agents[i]._process_state(
                        next_agent_local_obs_tensor, 
                        next_point_tensor, 
                        next_other_agents_obs_tensor
                    ).cpu().numpy()

                # ä¸ºå¢å¼ºç‰ˆCriticå‡†å¤‡å…¨å±€åŠ¨ä½œä¿¡æ¯
                all_current_actions = np.array(actions) if config.USE_ENHANCED_CRITIC else None
                
                replay_buffers[i].push(
                    state_to_store, 
                    actions[i], 
                    rewards[i], 
                    next_state_with_comm, 
                    float(done),
                    all_current_actions,    # æ‰€æœ‰æ™ºèƒ½ä½“å½“å‰åŠ¨ä½œ
                    None                    # next_all_actionså°†åœ¨è®­ç»ƒæ—¶åŠ¨æ€è®¡ç®—
                )

            # æ›´æ–°ä¸Šä¸€æ—¶åˆ»çš„å±€éƒ¨è§‚æµ‹
            last_local_obs = current_local_obs

            # --- è®­ç»ƒæ­¥éª¤ (ä½¿ç”¨ä¼˜å…ˆç»éªŒå›æ”¾) ---
            if step_counter > config.BATCH_SIZE and step_counter % config.UPDATE_EVERY == 0:
                update_counter += 1  # ä¿®å¤: å°†æ›´æ–°è®¡æ•°å™¨ç§»åˆ°æ™ºèƒ½ä½“å¾ªç¯å¤–ï¼Œæ‰€æœ‰æ™ºèƒ½ä½“å…±äº«åŒä¸€ä¸ªæ›´æ–°è½®æ¬¡
                
                for agent_id in range(num_agents):
                    if len(replay_buffers[agent_id]) > config.BATCH_SIZE:
                        # æ·»åŠ è°ƒè¯•ä¿¡æ¯ï¼šç¡®è®¤æ™ºèƒ½ä½“è¿›å…¥æ›´æ–°æµç¨‹
                        if update_counter % 1000 == 0:
                            debug_print(f"ğŸ”§ æ™ºèƒ½ä½“{agent_id}è¿›å…¥æ›´æ–°æµç¨‹ (update_counter={update_counter})")
                        
                        # ä»ä¼˜å…ˆç»éªŒå›æ”¾æ± é‡‡æ · - é€‚é…å¢å¼ºç‰ˆCriticçš„æ–°æ ¼å¼
                        sample_result = replay_buffers[agent_id].sample(config.BATCH_SIZE)
                        states, actions_b, rewards_b, next_states, dones_b, all_actions_b, next_all_actions_b, indices, is_weights = sample_result
                        
                        # æ ‡å‡†åŒ–å¥–åŠ±
                        rewards_b = (rewards_b - rewards_b.mean()) / (rewards_b.std() + 1e-7)
                        
                        # è½¬ç§»åˆ°è®¾å¤‡
                        states = states.to(device)
                        actions_b = actions_b.to(device) 
                        rewards_b = rewards_b.to(device)
                        next_states = next_states.to(device)
                        dones_b = dones_b.to(device)
                        is_weights = is_weights.to(device)
                        
                        # æ ¹æ®é…ç½®é€‰æ‹©ä¸åŒçš„æ›´æ–°æ–¹å¼
                        if config.USE_ENHANCED_CRITIC:
                            # å¢å¼ºç‰ˆCriticï¼šä¼ é€’å…¨å±€åŠ¨ä½œä¿¡æ¯
                            all_actions_b = all_actions_b.to(device)
                            
                            # åŠ¨æ€è®¡ç®—æ‰€æœ‰æ™ºèƒ½ä½“åœ¨ä¸‹ä¸€çŠ¶æ€çš„åŠ¨ä½œï¼ˆä½¿ç”¨ç›®æ ‡ç½‘ç»œï¼‰
                            next_all_actions_list = []
                            with torch.no_grad():
                                for temp_agent_id in range(num_agents):
                                    # ä½¿ç”¨æ¯ä¸ªæ™ºèƒ½ä½“çš„ç›®æ ‡actorç½‘ç»œè®¡ç®—ä¸‹ä¸€åŠ¨ä½œ
                                    next_action_logits = agents[temp_agent_id].actor_target(next_states)
                                    next_action = torch.argmax(next_action_logits, dim=1)
                                    next_all_actions_list.append(next_action)
                            
                            # å°†åˆ—è¡¨è½¬æ¢ä¸ºTensor [batch_size, num_agents]
                            next_all_actions_b = torch.stack(next_all_actions_list, dim=1).to(device)
                            
                            # Debugæ‰“å°ï¼ˆä»…åœ¨ç¬¬ä¸€æ¬¡æ›´æ–°æ—¶ï¼‰
                            if step_counter == config.BATCH_SIZE + 1 and agent_id == 0:
                                debug_print(f"ğŸ”§ å¢å¼ºç‰ˆCriticä¿®å¤ç”Ÿæ•ˆï¼šåŠ¨æ€è®¡ç®—next_all_actions shape: {next_all_actions_b.shape}")
                            
                            # --- TD3å»¶è¿Ÿç­–ç•¥æ›´æ–°æœºåˆ¶ ---
                            if config.ENABLE_DELAYED_POLICY_UPDATES:
                                # æ€»æ˜¯æ›´æ–°Critic
                                td_errors, critic_loss, critic_grad_norm = agents[agent_id].update_critic(
                                    agent_id, states, actions_b, rewards_b, next_states, dones_b, 
                                    all_actions_b, next_all_actions_b, is_weights
                                )
                                
                                # --- ä¿®æ”¹ç‚¹2: ç´¯ç§¯CriticæŸå¤±å’Œæ¢¯åº¦èŒƒæ•°åˆ°episodeçº§åˆ«åˆ—è¡¨ ---
                                critic_losses_episode[agent_id].append(critic_loss)
                                critic_grad_norms_episode[agent_id].append(critic_grad_norm)
                                
                                # æ·»åŠ è°ƒè¯•ä¿¡æ¯ï¼ˆæ¯1000æ¬¡æ›´æ–°è¾“å‡ºä¸€æ¬¡ï¼‰
                                if update_counter % 1000 == 0 and agent_id == 0:
                                    debug_print(f"ğŸ”§ å»¶è¿Ÿæ›´æ–°ç¬¬{update_counter}è½®: æ‰€æœ‰æ™ºèƒ½ä½“Criticå·²æ›´æ–°")
                                
                                # åªåœ¨ç‰¹å®šé¢‘ç‡ä¸‹æ›´æ–°Actorå’Œç›®æ ‡ç½‘ç»œ
                                actor_loss, actor_grad_norm = None, None
                                should_update_actor = update_counter % config.ACTOR_UPDATE_FREQUENCY == 0
                                if should_update_actor:
                                    actor_loss, actor_grad_norm = agents[agent_id].update_actor_and_targets(
                                        agent_id, states, all_actions_b, is_weights
                                    )
                                    
                                    # --- ä¿®æ”¹ç‚¹2: ç´¯ç§¯ActoræŸå¤±å’Œæ¢¯åº¦èŒƒæ•°åˆ°episodeçº§åˆ«åˆ—è¡¨ ---
                                    actor_losses_episode[agent_id].append(actor_loss)
                                    actor_grad_norms_episode[agent_id].append(actor_grad_norm)
                                    
                                    # æ·»åŠ è°ƒè¯•ä¿¡æ¯
                                    if update_counter % 1000 == 0 and agent_id == 0:
                                        debug_print(f"ğŸ¯ å»¶è¿Ÿæ›´æ–°ç¬¬{update_counter}è½®: æ‰€æœ‰æ™ºèƒ½ä½“Actorå·²æ›´æ–°")
                                
                                # --- TensorBoardè¯Šæ–­ä¿¡æ¯è®°å½• ---
                                if writer is not None and update_counter % config.TENSORBOARD_DIAGNOSTIC_INTERVAL == 0:
                                    agent_type = "fast" if agent_id < 3 else "heavy"
                                    writer.add_scalar(f'Loss/Agent_{agent_id}_{agent_type}_Critic_Loss', 
                                                    critic_loss, update_counter)
                                    writer.add_scalar(f'Gradients/Agent_{agent_id}_{agent_type}_Critic_Grad_Norm', 
                                                    critic_grad_norm, update_counter)
                                    if actor_loss is not None:
                                        writer.add_scalar(f'Loss/Agent_{agent_id}_{agent_type}_Actor_Loss', 
                                                        actor_loss, update_counter)
                                        writer.add_scalar(f'Gradients/Agent_{agent_id}_{agent_type}_Actor_Grad_Norm', 
                                                        actor_grad_norm, update_counter)
                                    
                                    # è®°å½•æ›´æ–°é¢‘ç‡ç»Ÿè®¡
                                    writer.add_scalar('Training/Update_Counter', update_counter, step_counter)
                                    writer.add_scalar('Training/Actor_Update_Ratio', 
                                                    (update_counter // config.ACTOR_UPDATE_FREQUENCY) / update_counter, 
                                                    update_counter)
                            else:
                                # ä¼ ç»Ÿç»Ÿä¸€æ›´æ–°æ–¹å¼
                                diagnostics = agents[agent_id].update(
                                    agent_id, states, actions_b, rewards_b, next_states, dones_b, 
                                    all_actions_b, next_all_actions_b, is_weights
                                )
                                td_errors = diagnostics['td_errors']
                                
                                # --- ä¿®æ”¹ç‚¹2: ç´¯ç§¯æŸå¤±å’Œæ¢¯åº¦èŒƒæ•°åˆ°episodeçº§åˆ«åˆ—è¡¨ ---
                                critic_losses_episode[agent_id].append(diagnostics['critic_loss'])
                                actor_losses_episode[agent_id].append(diagnostics['actor_loss'])
                                critic_grad_norms_episode[agent_id].append(diagnostics['critic_grad_norm'])
                                actor_grad_norms_episode[agent_id].append(diagnostics['actor_grad_norm'])
                                
                                # TensorBoardè¯Šæ–­ä¿¡æ¯è®°å½•
                                if writer is not None and update_counter % config.TENSORBOARD_DIAGNOSTIC_INTERVAL == 0:
                                    agent_type = "fast" if agent_id < 3 else "heavy"
                                    writer.add_scalar(f'Loss/Agent_{agent_id}_{agent_type}_Critic_Loss', 
                                                    diagnostics['critic_loss'], update_counter)
                                    writer.add_scalar(f'Loss/Agent_{agent_id}_{agent_type}_Actor_Loss', 
                                                    diagnostics['actor_loss'], update_counter)
                                    writer.add_scalar(f'Gradients/Agent_{agent_id}_{agent_type}_Critic_Grad_Norm', 
                                                    diagnostics['critic_grad_norm'], update_counter)
                                    writer.add_scalar(f'Gradients/Agent_{agent_id}_{agent_type}_Actor_Grad_Norm', 
                                                    diagnostics['actor_grad_norm'], update_counter)
                        else:
                            # æ ‡å‡†MADDPGï¼šåŸæœ‰æ›´æ–°æ–¹å¼
                            result = agents[agent_id].update(states, actions_b, rewards_b, next_states, dones_b, action_dim, is_weights)
                            
                            # --- ä¿®æ”¹ç‚¹2: æ ‡å‡†MADDPGä¹Ÿéœ€è¦è®°å½•æŸå¤± ---
                            if isinstance(result, dict):
                                # å¦‚æœè¿”å›çš„æ˜¯è¯Šæ–­å­—å…¸ï¼ˆæ–°ç‰ˆæœ¬ï¼‰
                                critic_losses_episode[agent_id].append(result['critic_loss'])
                                actor_losses_episode[agent_id].append(result['actor_loss'])
                                td_errors = result['td_errors']
                            else:
                                # å¦‚æœè¿”å›çš„æ˜¯td_errorsï¼ˆæ—§ç‰ˆæœ¬ï¼‰ï¼Œè®°å½•é»˜è®¤å€¼
                                td_errors = result
                                # å¯¹äºæ²¡æœ‰æŸå¤±ä¿¡æ¯çš„æƒ…å†µï¼Œæˆ‘ä»¬æ·»åŠ è°ƒè¯•ä¿¡æ¯
                                if episode % 100 == 0 and agent_id == 0:
                                    debug_print(f"âš ï¸ æ ‡å‡†MADDPGæœªè¿”å›æŸå¤±ä¿¡æ¯ï¼Œå»ºè®®ä½¿ç”¨å¢å¼ºç‰ˆCritic")
                        
                        # æ›´æ–°ä¼˜å…ˆçº§
                        replay_buffers[agent_id].update_priorities(indices, td_errors)

            step_counter += 1
            total_rewards += rewards
            obs = next_obs

        noise_std = max(config.NOISE_STD_END, noise_std * config.NOISE_DECAY)
        charge_counts = env.agent_charge_counts
        
        # è·å–å½“å‰å­¦ä¹ ç‡ç”¨äºæ˜¾ç¤º
        lr_info = ""
        if config.ENABLE_LR_SCHEDULING:
            current_actor_lr, current_critic_lr = lr_scheduler.get_current_rates()
            lr_info = f", actor_lr = {current_actor_lr:.2e}, critic_lr = {current_critic_lr:.2e}"
        
        debug_print(f"Episode {episode}: total_rewards = {total_rewards}, noise = {noise_std:.4f}, charge_counts = {charge_counts}{lr_info}")
        
        # è¾“å‡ºä»»åŠ¡ç‚¹çŠ¶æ€åŒæ­¥çš„æœ€ç»ˆç»“æœ
        final_done_points = obs['done_points']
        processed_count = np.sum(final_done_points)
        total_count = len(final_done_points)
        debug_print(f"   ğŸ“Š [{env_type}ç¯å¢ƒ] ä»»åŠ¡ç‚¹å¤„ç†çŠ¶æ€: {processed_count}/{total_count} ä¸ªä»»åŠ¡ç‚¹å·²å¤„ç† (å¤„ç†ç‡: {processed_count/total_count*100:.1f}%)")
        if processed_count == total_count:
            debug_print(f"   ğŸ‰ æ‰€æœ‰ä»»åŠ¡ç‚¹å‡å·²å¤„ç†ï¼Transformerç«¯åˆ°ç«¯å­¦ä¹ å·²æ¿€æ´»ï¼Œä»»åŠ¡ç‚¹çŠ¶æ€å·²åŒæ­¥")
        
        episode_rewards_log.append(total_rewards.tolist())
        
        # --- æ–°å¢: æ¨¡å‹æ£€æŸ¥ç‚¹ä¿å­˜ ---
        if config.ENABLE_MODEL_CHECKPOINTING and (episode + 1) % config.MODEL_SAVE_INTERVAL == 0:
            # å‡†å¤‡é…ç½®å­—å…¸
            config_dict = {
                'NUM_EPISODES': config.NUM_EPISODES,
                'NUM_AGENTS': num_agents,
                'NUM_POINTS': num_points,
                'BATCH_SIZE': config.BATCH_SIZE,
                'LR_ACTOR': config.LR_ACTOR,
                'LR_CRITIC': config.LR_CRITIC,
                'ENVIRONMENT_TYPE': config.ENVIRONMENT_TYPE,
                'RANDOM_SEED': config.RANDOM_SEED,
                'USE_ENHANCED_CRITIC': config.USE_ENHANCED_CRITIC,
                'ENABLE_DELAYED_POLICY_UPDATES': config.ENABLE_DELAYED_POLICY_UPDATES,
                'ACTOR_UPDATE_FREQUENCY': config.ACTOR_UPDATE_FREQUENCY,
                'REWARD_EXPERIMENT_LEVEL': config.REWARD_EXPERIMENT_LEVEL,
                'COMMUNICATION_MODE': "immediate" if config.ENABLE_IMMEDIATE_COMMUNICATION else "delayed"
            }
            
            # ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹
            checkpoint_path = save_model_checkpoint(
                agents=agents,
                episode=episode + 1,  # ä½¿ç”¨1-basedç¼–å·
                checkpoints_dir=checkpoints_dir,
                config_dict=config_dict,
                episode_rewards_log=episode_rewards_log,
                noise_std=noise_std
            )
        
        # --- æ–°å¢: æŸå¤±å’Œæ¢¯åº¦èŒƒæ•°è®°å½•è°ƒè¯•ä¿¡æ¯ ---
        if episode % 100 == 0:  # æ¯100ä¸ªepisodeè¾“å‡ºä¸€æ¬¡ç»Ÿè®¡
            debug_print(f"ğŸ“Š Episode {episode} æŸå¤±ä¸æ¢¯åº¦èŒƒæ•°è®°å½•ç»Ÿè®¡:")
            for i in range(num_agents):
                agent_type = "fast" if i < 3 else "heavy"
                critic_count = len(critic_losses_episode[i])
                actor_count = len(actor_losses_episode[i])
                critic_grad_count = len(critic_grad_norms_episode[i])
                actor_grad_count = len(actor_grad_norms_episode[i])
                debug_print(f"   æ™ºèƒ½ä½“{i}({agent_type}): Criticæ›´æ–°{critic_count}æ¬¡, Actoræ›´æ–°{actor_count}æ¬¡")
                
                # å¦‚æœæœ‰æŸå¤±æ•°æ®ï¼Œæ˜¾ç¤ºå¹³å‡å€¼
                if critic_count > 0:
                    avg_critic = np.mean(critic_losses_episode[i])
                    debug_print(f"     - å¹³å‡CriticæŸå¤±: {avg_critic:.4f}")
                if actor_count > 0:
                    avg_actor = np.mean(actor_losses_episode[i])
                    debug_print(f"     - å¹³å‡ActoræŸå¤±: {avg_actor:.4f}")
                
                # --- æ–°å¢: æ˜¾ç¤ºæ¢¯åº¦èŒƒæ•°ç»Ÿè®¡ ---
                if critic_grad_count > 0:
                    avg_critic_grad = np.mean(critic_grad_norms_episode[i])
                    debug_print(f"     - å¹³å‡Criticæ¢¯åº¦èŒƒæ•°: {avg_critic_grad:.4f}")
                if actor_grad_count > 0:
                    avg_actor_grad = np.mean(actor_grad_norms_episode[i])
                    debug_print(f"     - å¹³å‡Actoræ¢¯åº¦èŒƒæ•°: {avg_actor_grad:.4f}")
        
        # --- æ–°å¢: TensorBoardæŒ‡æ ‡è®°å½• ---
        if writer is not None and episode % config.TENSORBOARD_LOG_INTERVAL == 0:
            # 1. åŸºç¡€è®­ç»ƒæŒ‡æ ‡
            writer.add_scalar('Training/Total_Reward_Sum', np.sum(total_rewards), episode)
            writer.add_scalar('Training/Average_Reward', np.mean(total_rewards), episode)
            writer.add_scalar('Training/Noise_Std', noise_std, episode)
            writer.add_scalar('Training/Task_Completion_Rate', processed_count/total_count, episode)
            
            # 2. æ¯ä¸ªæ™ºèƒ½ä½“çš„å¥–åŠ±å’ŒæŸå¤±ï¼ˆä¿®æ”¹ç‚¹3ï¼šå®‰å…¨è®°å½•episodeç´¯ç§¯çš„æŸå¤±ï¼‰
            for i in range(num_agents):
                agent_type = "fast" if i < 3 else "heavy"
                writer.add_scalar(f'Agent_Rewards/Agent_{i}_{agent_type}', total_rewards[i], episode)
                writer.add_scalar(f'Agent_Performance/Charge_Count_Agent_{i}', charge_counts[i], episode)
                
                # --- ä¿®æ”¹ç‚¹3: å®‰å…¨åœ°è®°å½•ç´¯ç§¯çš„æŸå¤±å€¼å’Œæ¢¯åº¦èŒƒæ•° ---
                # åªæœ‰å½“æŸå¤±åˆ—è¡¨ä¸ä¸ºç©ºæ—¶ï¼Œæ‰è®¡ç®—å¹³å‡å€¼å¹¶è®°å½•
                if critic_losses_episode[i]:
                    avg_critic_loss = np.mean(critic_losses_episode[i])
                    writer.add_scalar(f'Episode_Loss/Agent_{i}_{agent_type}_Critic_Loss', avg_critic_loss, episode)
                    # è®°å½•æ›´æ–°æ¬¡æ•°
                    writer.add_scalar(f'Update_Count/Agent_{i}_{agent_type}_Critic_Updates', len(critic_losses_episode[i]), episode)
                    
                    # æ·»åŠ è°ƒè¯•ç¡®è®¤ï¼ˆæ¯100ä¸ªepisodeï¼‰
                    if episode % 100 == 0:
                        debug_print(f"   âœ… TensorBoardè®°å½• - æ™ºèƒ½ä½“{i} CriticæŸå¤±: {avg_critic_loss:.4f}")
                else:
                    # å¦‚æœæ²¡æœ‰CriticæŸå¤±æ•°æ®ï¼Œè¿™æ˜¯å¼‚å¸¸æƒ…å†µ
                    if episode % 100 == 0:
                        debug_print(f"   âŒ æ™ºèƒ½ä½“{i} æœ¬episodeæ— CriticæŸå¤±æ•°æ®ï¼")
                
                # --- æ–°å¢: è®°å½•Criticæ¢¯åº¦èŒƒæ•° ---
                if critic_grad_norms_episode[i]:
                    avg_critic_grad_norm = np.mean(critic_grad_norms_episode[i])
                    writer.add_scalar(f'Episode_Gradient_Norm/Agent_{i}_{agent_type}_Critic_Grad_Norm', avg_critic_grad_norm, episode)
                    
                    # æ·»åŠ è°ƒè¯•ç¡®è®¤ï¼ˆæ¯100ä¸ªepisodeï¼‰
                    if episode % 100 == 0:
                        debug_print(f"   âœ… TensorBoardè®°å½• - æ™ºèƒ½ä½“{i} Criticæ¢¯åº¦èŒƒæ•°: {avg_critic_grad_norm:.4f}")
                
                if actor_losses_episode[i]:
                    avg_actor_loss = np.mean(actor_losses_episode[i])
                    writer.add_scalar(f'Episode_Loss/Agent_{i}_{agent_type}_Actor_Loss', avg_actor_loss, episode)
                    # è®°å½•æ›´æ–°æ¬¡æ•°
                    writer.add_scalar(f'Update_Count/Agent_{i}_{agent_type}_Actor_Updates', len(actor_losses_episode[i]), episode)
                    
                    # æ·»åŠ è°ƒè¯•ç¡®è®¤ï¼ˆæ¯100ä¸ªepisodeï¼‰
                    if episode % 100 == 0:
                        debug_print(f"   âœ… TensorBoardè®°å½• - æ™ºèƒ½ä½“{i} ActoræŸå¤±: {avg_actor_loss:.4f}")
                else:
                    # å¦‚æœè¿™ä¸ªepisodeä¸­Actoræ²¡æœ‰æ›´æ–°ï¼Œè®°å½•0æ¬¡æ›´æ–°
                    writer.add_scalar(f'Update_Count/Agent_{i}_{agent_type}_Actor_Updates', 0, episode)
                    if episode % 100 == 0 and config.ENABLE_DELAYED_POLICY_UPDATES:
                        debug_print(f"   â„¹ï¸ æ™ºèƒ½ä½“{i} æœ¬episode Actoræœªæ›´æ–°ï¼ˆå»¶è¿Ÿç­–ç•¥æ›´æ–°æ­£å¸¸ï¼‰")
                
                # --- æ–°å¢: è®°å½•Actoræ¢¯åº¦èŒƒæ•° ---
                if actor_grad_norms_episode[i]:
                    avg_actor_grad_norm = np.mean(actor_grad_norms_episode[i])
                    writer.add_scalar(f'Episode_Gradient_Norm/Agent_{i}_{agent_type}_Actor_Grad_Norm', avg_actor_grad_norm, episode)
                    
                    # æ·»åŠ è°ƒè¯•ç¡®è®¤ï¼ˆæ¯100ä¸ªepisodeï¼‰
                    if episode % 100 == 0:
                        debug_print(f"   âœ… TensorBoardè®°å½• - æ™ºèƒ½ä½“{i} Actoræ¢¯åº¦èŒƒæ•°: {avg_actor_grad_norm:.4f}")
            
            # 3. è§’è‰²ä¸“ä¸šåŒ–åˆ†æ
            fast_rewards = np.mean(total_rewards[:3])  # å¿«é€Ÿæ™ºèƒ½ä½“å¹³å‡å¥–åŠ±
            heavy_rewards = np.mean(total_rewards[3:])  # é‡è½½æ™ºèƒ½ä½“å¹³å‡å¥–åŠ±
            writer.add_scalar('Role_Specialization/Fast_Agents_Avg_Reward', fast_rewards, episode)
            writer.add_scalar('Role_Specialization/Heavy_Agents_Avg_Reward', heavy_rewards, episode)
            writer.add_scalar('Role_Specialization/Reward_Specialization_Gap', heavy_rewards - fast_rewards, episode)
            
            # 4. å­¦ä¹ ç‡ç›‘æ§
            if config.ENABLE_LR_SCHEDULING:
                current_actor_lr, current_critic_lr = lr_scheduler.get_current_rates()
                writer.add_scalar('Learning_Rate/Actor_LR', current_actor_lr, episode)
                writer.add_scalar('Learning_Rate/Critic_LR', current_critic_lr, episode)
        
        # å®šæœŸè¾“å‡ºåä½œåˆ†ææ‘˜è¦å¹¶è®°å½•åˆ°TensorBoard
        if (episode + 1) % config.TENSORBOARD_COLLABORATION_INTERVAL == 0:
            env.debug_print_collaboration_summary()
            
            # --- æ–°å¢: è®°å½•åä½œåˆ†ææŒ‡æ ‡åˆ°TensorBoard ---
            if writer is not None:
                analytics = env.get_collaboration_analytics()
            
            # åä½œå†²çªæŒ‡æ ‡
            writer.add_scalar('Collaboration/Conflict_Rate', analytics['conflict_rate'], episode)
            writer.add_scalar('Collaboration/Total_Conflicts', analytics['total_conflicts'], episode)
            writer.add_scalar('Collaboration/Total_Decisions', analytics['total_decisions'], episode)
            
            # è§’è‰²ä¸“ä¸šåŒ–æŒ‡æ ‡
            fast_high_priority_ratios = []
            heavy_high_priority_ratios = []
            fast_utilizations = []
            heavy_utilizations = []
            
            for agent_id in range(num_agents):
                agent_key = f'agent_{agent_id}'
                role_data = analytics['role_specialization'][agent_key]
                load_data = analytics['load_efficiency'][agent_key]
                
                # è®°å½•æ¯ä¸ªæ™ºèƒ½ä½“çš„ä¸“ä¸šåŒ–ç¨‹åº¦
                writer.add_scalar(f'Specialization/Agent_{agent_id}_High_Priority_Ratio', 
                                role_data['high_priority_ratio'], episode)
                writer.add_scalar(f'Load_Efficiency/Agent_{agent_id}_Avg_Utilization', 
                                load_data['avg_utilization'], episode)
                writer.add_scalar(f'Load_Efficiency/Agent_{agent_id}_Empty_Return_Rate', 
                                load_data['empty_return_rate'], episode)
                
                # æ”¶é›†åˆ†ç»„æ•°æ®
                if agent_id < 3:  # å¿«é€Ÿæ™ºèƒ½ä½“
                    fast_high_priority_ratios.append(role_data['high_priority_ratio'])
                    if load_data['total_returns'] > 0:
                        fast_utilizations.append(load_data['avg_utilization'])
                else:  # é‡è½½æ™ºèƒ½ä½“
                    heavy_high_priority_ratios.append(role_data['high_priority_ratio'])
                    if load_data['total_returns'] > 0:
                        heavy_utilizations.append(load_data['avg_utilization'])
            
            # è®°å½•åˆ†ç»„ç»Ÿè®¡
            if fast_high_priority_ratios and heavy_high_priority_ratios:
                fast_avg_specialization = np.mean(fast_high_priority_ratios)
                heavy_avg_specialization = np.mean(heavy_high_priority_ratios)
                specialization_gap = fast_avg_specialization - heavy_avg_specialization
                
                writer.add_scalar('Role_Analysis/Fast_Agents_High_Priority_Ratio', fast_avg_specialization, episode)
                writer.add_scalar('Role_Analysis/Heavy_Agents_High_Priority_Ratio', heavy_avg_specialization, episode)
                writer.add_scalar('Role_Analysis/Specialization_Gap', specialization_gap, episode)
            
            if fast_utilizations and heavy_utilizations:
                fast_avg_util = np.mean(fast_utilizations)
                heavy_avg_util = np.mean(heavy_utilizations)
                efficiency_gap = heavy_avg_util - fast_avg_util
                
                writer.add_scalar('Load_Analysis/Fast_Agents_Avg_Utilization', fast_avg_util, episode)
                writer.add_scalar('Load_Analysis/Heavy_Agents_Avg_Utilization', heavy_avg_util, episode)
                writer.add_scalar('Load_Analysis/Efficiency_Gap', efficiency_gap, episode)
        
        # è®°å½•æ¯ä¸ªæ™ºèƒ½ä½“çš„æœ€ç»ˆè·¯å¾„ç‚¹
        for i in range(num_agents):
            final_position = obs['agent_positions'][i]
            if env.agent_paths[i]:
                 env.agent_paths[i].append(final_position)

        if (episode + 1) % 50 == 0:
            plot_agent_trajectories(env.agent_paths, env.points, env.central_station, episode_id=episode+1, foldername=trajectories_dir)

        if (episode + 1) % 100 == 0 and episode > 0:
            debug_print(f"\n--- Plotting rewards at episode {episode + 1} ---\n")
            plot_rewards(episode_rewards_log, filename=f"{reward_plots_dir}/reward_curve_ep{episode+1}.png")
            plot_reward_curve2(episode_rewards_log, filename=f"{reward_plots_dir}/reward_curve2_ep{episode+1}.png")

    debug_print("\n--- Plotting final rewards ---") # è¾“å‡ºæœ€ç»ˆå¥–åŠ±æ›²çº¿
    plot_rewards(episode_rewards_log, filename=f"{reward_plots_dir}/reward_curve_final.png")
    plot_reward_curve2(episode_rewards_log, filename=f"{reward_plots_dir}/reward_curve2_final.png")

    # ä¿å­˜æ•°æ®æ–‡ä»¶åˆ°è¾“å‡ºç›®å½•
    output_dir = getattr(config, 'OUTPUT_DIR', ".")
    
    all_rewards = np.array(episode_rewards_log)
    rewards_log_path = os.path.join(output_dir, 'rewards_log.npy')
    np.save(rewards_log_path, all_rewards)
    debug_print(f"ğŸ“Š å¥–åŠ±æ•°æ®å·²ä¿å­˜: {rewards_log_path}")
    
    # è¾“å‡ºæœ€ç»ˆåä½œåˆ†ææŠ¥å‘Š
    debug_print("\n" + "ğŸ¯" * 20)
    debug_print("æœ€ç»ˆåä½œåˆ†ææŠ¥å‘Š")
    debug_print("ğŸ¯" * 20)
    env.debug_print_collaboration_summary()
    
    # ä¿å­˜åä½œåˆ†ææ•°æ®
    final_analytics = env.get_collaboration_analytics()
    collaboration_path = os.path.join(output_dir, 'collaboration_analytics.npy')
    np.save(collaboration_path, final_analytics)
    debug_print(f"ğŸ“Š åä½œåˆ†ææ•°æ®å·²ä¿å­˜: {collaboration_path}")
    
    # === å¢é‡å¼å®éªŒç»“æœä¿å­˜ ===
    if config.ENVIRONMENT_TYPE == "configurable":
        from experiment_analyzer import ExperimentAnalyzer
        
        analyzer = ExperimentAnalyzer()
        
        # å‡†å¤‡è®­ç»ƒé…ç½®ä¿¡æ¯
        training_config = {
            'num_episodes': config.NUM_EPISODES,
            'num_agents': num_agents,
            'num_points': num_points,
            'batch_size': config.BATCH_SIZE,
            'lr_actor': config.LR_ACTOR,
            'lr_critic': config.LR_CRITIC,
            'use_enhanced_critic': config.USE_ENHANCED_CRITIC,
            'communication_mode': "immediate" if config.ENABLE_IMMEDIATE_COMMUNICATION else "delayed"
        }
        
        # è®¡ç®—æœ€ç»ˆæ€§èƒ½æŒ‡æ ‡
        final_rewards = np.array(episode_rewards_log)
        if final_rewards.ndim > 1:
            final_performance = np.mean(np.sum(final_rewards[-100:], axis=1))
            convergence_episode = len(final_rewards)
        else:
            final_performance = np.mean(final_rewards[-100:])
            convergence_episode = len(final_rewards)
        
        final_metrics = {
            'final_performance': final_performance,
            'convergence_episode': convergence_episode,
            'total_episodes': len(episode_rewards_log),
            'final_conflict_rate': final_analytics.get('conflict_rate', 0) if final_analytics else 0,
            'experiment_level': config.REWARD_EXPERIMENT_LEVEL
        }
        
        # ä¿å­˜å®éªŒç»“æœ
        result_file = analyzer.save_experiment_result(
            experiment_level=config.REWARD_EXPERIMENT_LEVEL,
            episode_rewards=episode_rewards_log,
            collaboration_analytics=final_analytics,
            training_config=training_config,
            final_metrics=final_metrics
        )
        
        debug_print(f"\nğŸ¯ å¢é‡å¼å®éªŒç»“æœå·²ä¿å­˜ï¼Œå¯ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è¿›è¡Œåˆ†æ:")
        debug_print(f"   python experiment_analyzer.py")
        debug_print(f"   æˆ–åœ¨Pythonä¸­ï¼šfrom experiment_analyzer import analyze_latest_experiments; analyze_latest_experiments()")
    
    # --- æ–°å¢: å…³é—­TensorBoardè®°å½•å™¨ ---
    if writer is not None:
        writer.close()
        debug_print(f"ğŸ“Š TensorBoardæ—¥å¿—å·²ä¿å­˜åˆ°: {tensorboard_dir}")
        debug_print(f"ğŸ’¡ å¯åŠ¨TensorBoardæŸ¥çœ‹: tensorboard --logdir=runs --port=6006")
    
if __name__ == "__main__":
    main()
