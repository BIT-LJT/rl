import numpy as np
import random
import queue
import config
from utils import debug_print
class SamplingEnv2_0:
    def __init__(self, num_points=30, num_agents=5):
        self.size = 5000
        self.num_points = num_points
        self.num_agents = num_agents
        self.action_dim = self.num_points + 2

        self.agent_capacity = np.array(config.agent_capacity)#æ™ºèƒ½ä½“å®¹é‡
        self.agent_speed = np.array(config.agent_speed)#æ™ºèƒ½ä½“é€Ÿåº¦
        self.agent_energy_max = np.array(config.agent_energy_max)#æ™ºèƒ½ä½“èƒ½é‡æœ€å¤§å€¼

        BASE_SPEED = np.max(self.agent_speed)#æ™ºèƒ½ä½“é€Ÿåº¦æœ€å¤§å€¼
        self.energy_cost_factor = BASE_SPEED / self.agent_speed#èƒ½é‡æ¶ˆè€—å› å­

        self.fast_charge_time = config.fast_charge_time
        self.slow_charge_time = config.slow_charge_time

        self.queue_done = queue.Queue()
        self.queue_new_point = queue.Queue()

        self.reset()

    def reset(self):
        self.points = np.random.randint(0, self.size, (self.num_points, 2))
        self.samples = np.random.randint(1, 6, self.num_points)
        self.priority = np.random.choice([1, 2, 3], self.num_points, p=[0.3, 0.4, 0.3])
        # æ›´ç´§è¿«çš„æ—¶é—´çª—å£è®¾è®¡ï¼šè®©é€Ÿåº¦ä¼˜åŠ¿æ›´æ˜æ˜¾
        self.time_windows = np.array([(3*60) if p == 1 else ((10*60) if p == 2 else (30*60)) for p in self.priority], dtype=float)
        # é«˜ä¼˜å…ˆçº§: 3åˆ†é’Ÿ (180ç§’) - çœŸæ­£ç´§è¿«
        # ä¸­ä¼˜å…ˆçº§: 10åˆ†é’Ÿ (600ç§’) - ä¸­ç­‰ç´§è¿«  
        # ä½ä¼˜å…ˆçº§: 30åˆ†é’Ÿ (1800ç§’) - ç›¸å¯¹å®½æ¾
        
        self.done_points = np.zeros(self.num_points)
        self.successfully_collected_points = np.zeros(self.num_points)
        self.traffic_map = np.ones((self.size // 500, self.size // 500))
        self.agent_positions = np.full((self.num_agents, 2), self.size // 2, dtype=float)
        self.agent_loads = np.zeros(self.num_agents)
        self.agent_energy = self.agent_energy_max.copy().astype(float)
        self.agent_samples = [[] for _ in range(self.num_agents)]
        self.agent_task_status = np.zeros(self.num_agents)
        
        self.agent_charging_status = np.zeros(self.num_agents, dtype=int)
        self.agent_charge_counts = np.zeros(self.num_agents, dtype=int)
        # çº¯å¥–åŠ±ç³»ç»Ÿï¼šç§»é™¤ä¼‘æ¯çŠ¶æ€ç®¡ç†ï¼Œè®©æ™ºèƒ½ä½“è‡ªä¸»å­¦ä¹ 
        self.agent_rest_reward_given = np.zeros(self.num_agents, dtype=bool)  # è¿½è¸ªä»»åŠ¡å®Œæˆåä¼‘æ¯å¥–åŠ±æ˜¯å¦å·²ç»™äºˆ
        
        # æ–°å¢: ç”¨äºå­˜å‚¨æ¯ä¸ªæ ·æœ¬çš„é€è¾¾ä¿¡æ¯
        self.delivery_info = []
        
        # åä½œåˆ†æç»Ÿè®¡å˜é‡
        self.conflict_count = 0  # å†²çªæ¬¡æ•°
        self.total_decisions = 0  # æ€»å†³ç­–æ¬¡æ•°
        self.agent_task_priorities = [[] for _ in range(self.num_agents)]  # æ¯ä¸ªæ™ºèƒ½ä½“å¤„ç†çš„ä»»åŠ¡ä¼˜å…ˆçº§
        self.agent_load_utilization = [[] for _ in range(self.num_agents)]  # æ¯ä¸ªæ™ºèƒ½ä½“çš„è½½é‡åˆ©ç”¨ç‡

        # æ™ºèƒ½ä½“çš„å®¶/åŸºåœ°ï¼šå‡ºå‘ç‚¹ã€å¸è½½ç‚¹ã€å……ç”µç‚¹ã€ä¼‘æ¯ç‚¹
        self.central_station = np.array([self.size // 2, self.size // 2], dtype=float)  # ä¹Ÿå¯ç§°ä¸º self.home_base

        self.time = 0
        self.spawn_count = 0
        self.new_points_num = 0
        self.point_last_visitor = {}
        self.agent_paths = [[] for _ in range(self.num_agents)]
        self.task_creation_time = np.full(self.num_points, self.time)
        
        # === ä»»åŠ¡è´£ä»»è¿½è¸ªç³»ç»Ÿ ===
        self.task_assignments = {}  # è®°å½•æ¯ä¸ªä»»åŠ¡ç‚¹å½“å‰çš„è´Ÿè´£æ™ºèƒ½ä½“
        self.task_assignment_history = {}  # è®°å½•ä»»åŠ¡åˆ†é…å†å²ï¼Œç”¨äºè°ƒè¯•
        self.task_assignment_timestamp = {}  # è®°å½•ä»»åŠ¡åˆ†é…çš„æ—¶é—´æˆ³
        
        # å¢å¼ºé²æ£’æ€§ï¼šåˆå§‹åŒ–ä»»åŠ¡å®Œæˆæ­¥éª¤æ ‡è®°ï¼Œé¿å…AttributeError
        self._tasks_completed_step = -1
        
        # æ–°å¢ï¼šè·Ÿè¸ªä¸Šä¸€æ­¥åŠ¨ä½œï¼Œç”¨äºå†³ç­–æ‘‡æ‘†æƒ©ç½š
        self.last_actions = [self.num_points] * self.num_agents  # åˆå§‹åŒ–ä¸ºå¾…æœºåŠ¨ä½œ

        return self._get_obs()

    def _get_action_masks(self):
        """
        ç”Ÿæˆä¸€ä¸ªå…¨é¢çš„åŠ¨ä½œæ©ç ï¼Œä¸»åŠ¨å±è”½æ‰€æœ‰å·²çŸ¥çš„æ— æ•ˆåŠ¨ä½œã€‚
        """
        masks = np.ones((self.num_agents, self.action_dim), dtype=bool)
        
        # 1. å±è”½æ‰€æœ‰å·²å®Œæˆçš„ä»»åŠ¡ç‚¹ (å…¨å±€è§„åˆ™)
        done_task_indices = np.where(self.done_points == 1)[0]
        if len(done_task_indices) > 0:
            masks[:, done_task_indices] = False

        for i in range(self.num_agents):
            # 2. å¦‚æœæ™ºèƒ½ä½“æ­£åœ¨å……ç”µï¼Œå®ƒåªèƒ½å¾…åœ¨åŸåœ°
            if self.agent_charging_status[i] > 0:
                masks[i, :] = False  # å±è”½æ‰€æœ‰å…¶ä»–åŠ¨ä½œ
                masks[i, self.num_points] = True  # åªå…è®¸"å¾…æœº/å……ç”µ"è¿™ä¸ªåŠ¨ä½œ
                continue  # å¤„ç†ä¸‹ä¸€ä¸ªæ™ºèƒ½ä½“

            # 3. æ£€æŸ¥å¹¶å±è”½ä¼šå¯¼è‡´è¶…è½½çš„ä»»åŠ¡ç‚¹
            current_load = self.agent_loads[i]
            agent_capacity = self.agent_capacity[i]
            for p_idx in range(self.num_points):
                # å¦‚æœè¯¥ä»»åŠ¡ç‚¹æœ¬èº«æ˜¯å¯é€‰çš„ (å°šæœªè¢«å…¨å±€è§„åˆ™å±è”½)
                if masks[i, p_idx]:
                    task_sample_size = self.samples[p_idx]
                    if current_load + task_sample_size > agent_capacity:
                        masks[i, p_idx] = False # å±è”½è¿™ä¸ªä¼šå¯¼è‡´è¶…è½½çš„ä»»åŠ¡
                        
        return masks

    def _get_obs(self):
        # è®¡ç®—å…¨å±€ä»»åŠ¡å®Œæˆä¿¡æ¯
        total_tasks = len(self.done_points)
        completed_tasks = np.sum(self.done_points)
        task_completion_ratio = completed_tasks / total_tasks if total_tasks > 0 else 0.0
        all_tasks_completed = float(np.all(self.done_points == 1))
        
        return {
            "agent_positions": self.agent_positions.copy(),
            "points": self.points.copy(),
            "samples": self.samples.copy(),
            "priority": self.priority.copy(),
            "time_windows": self.time_windows.copy(),
            "done_points": self.done_points.copy(),
            "agent_loads": self.agent_loads.copy(),
            "agent_energy": self.agent_energy.copy(),
            "traffic_map": self.traffic_map.copy(),
            "agent_task_status": self.agent_task_status.copy(),
            "agent_charging_status": self.agent_charging_status.copy(),
            "action_masks": self._get_action_masks(),
            # æ–°å¢å…¨å±€ä¿¡æ¯
            "task_completion_ratio": task_completion_ratio,
            "all_tasks_completed": all_tasks_completed,
            "total_loaded_agents": float(np.sum(self.agent_loads > 0)),
            # --- æ–°å¢: æ™ºèƒ½ä½“æ„å›¾ä¿¡æ¯ï¼ˆæ‰€æœ‰æ™ºèƒ½ä½“çš„ä¸Šä¸€æ—¶åˆ»åŠ¨ä½œï¼‰---
            "agent_last_actions": np.array(self.last_actions.copy()) if config.ENABLE_AGENT_INTENTION_OBS else np.array([self.num_points] * self.num_agents)
        }

    def _distance(self, pos1, pos2):
        dist = np.linalg.norm(pos1 - pos2)
        mid_point = (pos1 + pos2) / 2
        grid_x = int(mid_point[0] // 500)
        grid_y = int(mid_point[1] // 500)
        grid_x = np.clip(grid_x, 0, self.traffic_map.shape[0] - 1)
        grid_y = np.clip(grid_y, 0, self.traffic_map.shape[1] - 1)
        
        traffic_factor = self.traffic_map[grid_x, grid_y]
        return dist / traffic_factor

    def step(self, actions, current_step, max_steps):
        rewards = np.zeros(self.num_agents)
        actions = np.array(actions)
        
        # ç»Ÿè®¡æ€»å†³ç­–æ¬¡æ•°ï¼ˆæ¯ä¸ªæ™ºèƒ½ä½“çš„æ¯æ¬¡è¡ŒåŠ¨éƒ½ç®—ä¸€æ¬¡å†³ç­–ï¼‰
        self.total_decisions += len([a for a in actions if a < self.num_points])  # åªç»Ÿè®¡å®é™…çš„ä»»åŠ¡ç‚¹é€‰æ‹©
        
        R_COLLECT_BASE = 50.0#é‡‡é›†åŸºç¡€å¥–åŠ±
        R_RETURN_HOME_COEFF = 0.5#å›å®¶å¥–åŠ±ç³»æ•°
        R_FULL_LOAD_BONUS = 300.0#æ»¡è½½å¥–åŠ±
        R_COEFF_APPROACH_TASK = 0.1#æ¥è¿‘ç›®æ ‡ç‚¹å¥–åŠ±ç³»æ•°
        R_COEFF_APPROACH_HOME_LOADED = 0.05 #æ¥è¿‘ç›®æ ‡ç‚¹å¥–åŠ±
        R_COEFF_APPROACH_HOME_LOW_ENERGY = 0.1 #æ¥è¿‘åŸºåœ°å¥–åŠ±
        REWARD_SHAPING_CLIP = 10.0 #å¥–åŠ±å‰ªè£
        R_ROLE_SPEED_BONUS = 20.0#è§’è‰²é€Ÿåº¦å¥–åŠ±
        R_ROLE_CAPACITY_BONUS = 50.0#è§’è‰²è½½é‡å¥–åŠ±
        R_FAST_AGENT_HIGH_PRIORITY = 50.0  # å¿«é€Ÿæ™ºèƒ½ä½“å¤„ç†é«˜ä¼˜å…ˆçº§ä»»åŠ¡çš„ä¸“ä¸šåŒ–å¥–åŠ±ï¼ˆä»60.0è°ƒæ•´ä¸º50.0ï¼‰
        R_STAY_HOME_AFTER_COMPLETION = 10.0  # ä»»åŠ¡å®Œæˆåç•™åœ¨åŸºåœ°(central_station)çš„å¥–åŠ±
        R_SMART_RETURN_AFTER_COMPLETION = 50.0  # ä»»åŠ¡å®Œæˆåæ™ºèƒ½é€‰æ‹©å›å®¶(central_station)å¸è½½çš„å¥–åŠ±
        FINAL_BONUS_ACCOMPLISHED = 1000.0#æœ€ç»ˆå¥–åŠ±

        P_TIME_STEP = -0.1 #æ—¶é—´æ­¥æƒ©ç½š
        P_INACTIVITY = -10.0 #ä¸æ´»åŠ¨æƒ©ç½š
        P_ENERGY_DEPLETED = -50.0 #èƒ½é‡ä¸è¶³æƒ©ç½š
        P_TIMEOUT_BASE = -30.0 #è¶…æ—¶æƒ©ç½š
        P_EMPTY_RETURN = -20.0  # ç©ºè½½å›å®¶æƒ©ç½š
        P_SWING_PENALTY = -5.0  # æ–°å¢ï¼šå†³ç­–æ‘‡æ‘†æƒ©ç½š
        P_WASTED_MOVE_PENALTY = -1.0  # æ–°å¢ï¼šé€‰æ‹©å·²å®Œæˆä»»åŠ¡çš„æƒ©ç½š
        P_LOW_LOAD_HEAVY_AGENT = -15.0  # é«˜å®¹é‡æ™ºèƒ½ä½“ä½è½½è¿”å›æƒ©ç½š
        P_HEAVY_AGENT_HIGH_PRIORITY = -30.0  # é‡è½½æ™ºèƒ½ä½“å¤„ç†é«˜ä¼˜å…ˆçº§ä»»åŠ¡çš„æ•ˆç‡æƒ©ç½š
        P_NOT_RETURN_AFTER_COMPLETION = -20.0  # ä»»åŠ¡å®Œæˆåæœ‰è½½é‡ä½†ä¸å›å®¶(central_station)çš„æƒ©ç½š
        P_INVALID_TASK_ATTEMPT = -100.0  # å°è¯•æ‰§è¡Œå·²å®Œæˆä»»åŠ¡çš„é‡å¤§æƒ©ç½š
        P_OVERLOAD_ATTEMPT = -80.0  # å°è¯•è¶…è½½çš„é‡å¤§æƒ©ç½š
        P_POINTLESS_ACTION = -20.0  # æ— æ„ä¹‰è¡ŒåŠ¨çš„æƒ©ç½š
        P_TASK_COMPLETED_TIME_PENALTY = -5.0  # ä»»åŠ¡å®Œæˆåæ¯æ—¶é—´æ­¥çš„æƒ©ç½š
        FINAL_PENALTY_UNVISITED = -50.0#æœªè®¿é—®æƒ©ç½š
        FINAL_PENALTY_NOT_AT_BASE = -100.0#æœªå›å®¶æƒ©ç½š

        rewards += P_TIME_STEP
        
        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆï¼Œå¦‚æœæ˜¯ï¼Œç»™äºˆæ—¶é—´æƒ©ç½šä»¥æ¿€åŠ±å¿«é€Ÿè¿”å›
        all_tasks_completed = np.all(self.done_points == 1)
        if all_tasks_completed:
            for i in range(self.num_agents):
                # å¯¹æ‰€æœ‰æ™ºèƒ½ä½“æ–½åŠ æ—¶é—´æƒ©ç½šï¼Œæ¿€åŠ±å¿«é€Ÿç»“æŸä»»åŠ¡
                rewards[i] += P_TASK_COMPLETED_TIME_PENALTY
                # å¯¹ä»æœ‰è½½é‡çš„æ™ºèƒ½ä½“æ–½åŠ é¢å¤–æƒ©ç½š
                if self.agent_loads[i] > 0:
                    rewards[i] += P_TASK_COMPLETED_TIME_PENALTY * 2  # åŒå€æƒ©ç½š
        
        for i in range(self.num_agents):
            if self.agent_charging_status[i] > 0:
                self.agent_charging_status[i] -= 1
                if self.agent_charging_status[i] == 0:
                    self.agent_energy[i] = self.agent_energy_max[i]

        # æ™ºèƒ½å†²çªå¤„ç†æœºåˆ¶ï¼šåŸºäºä»»åŠ¡ä¼˜å…ˆçº§å’Œæ™ºèƒ½ä½“èƒ½åŠ›
        task_targets = actions[actions < self.num_points]
        unique_targets, counts = np.unique(task_targets, return_counts=True)
        conflicted_targets = unique_targets[counts > 1]
        
        # è®°å½•æ‰€æœ‰å·²åˆ†é…çš„ä»»åŠ¡ç‚¹ï¼Œé¿å…é‡å¤åˆ†é…
        assigned_tasks = set()

        for target_idx in conflicted_targets:
            competing_agent_indices = np.where(actions == target_idx)[0]
            # è®°å½•å†²çªæ¬¡æ•°ï¼ˆæ¯ä¸ªå†²çªç›®æ ‡ç‚¹è®¡ä¸ºä¸€æ¬¡å†²çªï¼‰
            self.conflict_count += 1
            
            # ä½¿ç”¨æ™ºèƒ½åˆ†é…ç®—æ³•é€‰æ‹©æœ€ä½³æ™ºèƒ½ä½“
            winner_global_idx = self._resolve_conflict_intelligently(target_idx, competing_agent_indices)
            
            # === è®°å½•è·èƒœè€…çš„ä»»åŠ¡è´£ä»» ===
            self._assign_task_responsibility(target_idx, winner_global_idx, current_step)
            assigned_tasks.add(target_idx)
            
            # æ”¹è¿›ï¼šä¸ºè´¥è€…æ™ºèƒ½æ’åºåå†åˆ†é…æ›¿ä»£ä»»åŠ¡
            losers = [agent_idx for agent_idx in competing_agent_indices if agent_idx != winner_global_idx]
            
            # æŒ‰ç…§æ™ºèƒ½ä½“å¯¹ä»»åŠ¡çš„é€‚åº”æ€§æ’åºï¼ˆè·ç¦»ã€èƒ½é‡ã€è½½é‡ç­‰ç»¼åˆè¯„åˆ†ï¼‰
            loser_priorities = []
            for agent_idx in losers:
                # è®¡ç®—è¯¥æ™ºèƒ½ä½“çš„ç»¼åˆç´§è¿«åº¦å¾—åˆ†
                agent_pos = self.agent_positions[agent_idx]
                energy_ratio = self.agent_energy[agent_idx] / self.agent_energy_max[agent_idx]
                load_ratio = self.agent_loads[agent_idx] / self.agent_capacity[agent_idx]
                
                # è·ç¦»åŸºåœ°çš„è·ç¦»ï¼ˆè·ç¦»è¶Šè¿œè¶Šéœ€è¦ä¼˜å…ˆå®‰æ’ï¼‰
                distance_to_base = self._distance(agent_pos, self.central_station)
                
                # ç»¼åˆä¼˜å…ˆçº§å¾—åˆ†ï¼ˆè¶Šé«˜è¶Šä¼˜å…ˆï¼‰
                priority_score = (
                    (1.0 - energy_ratio) * 0.4 +     # èƒ½é‡è¶Šå°‘è¶Šä¼˜å…ˆ
                    load_ratio * 0.3 +               # è½½é‡è¶Šå¤šè¶Šä¼˜å…ˆ
                    distance_to_base / (self.size * 1.414) * 0.3  # è·ç¦»åŸºåœ°è¶Šè¿œè¶Šä¼˜å…ˆ
                )
                loser_priorities.append((agent_idx, priority_score))
            
            # æŒ‰ä¼˜å…ˆçº§æ’åºï¼ˆé™åºï¼‰
            loser_priorities.sort(key=lambda x: x[1], reverse=True)
            
            # æŒ‰ä¼˜å…ˆçº§é¡ºåºä¸ºè´¥è€…åˆ†é…æ›¿ä»£ä»»åŠ¡
            for agent_idx, priority_score in loser_priorities:
                # ä½¿ç”¨æ™ºèƒ½åˆ†é…ç®—æ³•å¯»æ‰¾æœ€ä½³æ›¿ä»£ä»»åŠ¡
                alternative_task = self._find_best_alternative_task_intelligent(agent_idx, assigned_tasks)
                
                if alternative_task is not None:
                    actions[agent_idx] = alternative_task
                    # === è®°å½•æ›¿ä»£ä»»åŠ¡çš„è´£ä»»åˆ†é… ===
                    self._assign_task_responsibility(alternative_task, agent_idx, current_step)
                    assigned_tasks.add(alternative_task)  # è®°å½•å·²åˆ†é…
                    # å†²çªæƒ©ç½šå·²åˆ é™¤ï¼Œæ™ºèƒ½é‡åˆ†é…ä¸å†ç»™äºˆæƒ©ç½š
                    debug_print(f"ğŸ§  æ™ºèƒ½ä½“ {agent_idx} (ä¼˜å…ˆçº§:{priority_score:.3f}) æ™ºèƒ½é‡åˆ†é…åˆ°ä»»åŠ¡ç‚¹ {alternative_task}")
                else:
                    # æ²¡æœ‰æ›¿ä»£ä»»åŠ¡ï¼Œæ‰å›å®¶
                    actions[agent_idx] = self.num_points 
                    # å†²çªæƒ©ç½šå·²åˆ é™¤ï¼Œæ™ºèƒ½ä½“è¿”å›åŸºåœ°ä¸å†å—æƒ©ç½š
                    debug_print(f"ğŸ  æ™ºèƒ½ä½“ {agent_idx} å†²çªåæ— æ›¿ä»£ä»»åŠ¡ï¼Œè¿”å›åŸºåœ°")
        
        for i, target in enumerate(actions):
            if self.agent_charging_status[i] > 0:
                continue

            prev_pos = self.agent_positions[i].copy()
            self.agent_paths[i].append(prev_pos)
            
            # --- æ–°å¢: å†³ç­–æ‘‡æ‘†æƒ©ç½š ---
            last_target = self.last_actions[i]
            if last_target < self.num_points and target < self.num_points and last_target != target:
                rewards[i] += P_SWING_PENALTY
                debug_print(f"ğŸ”„ï¸ æ™ºèƒ½ä½“ {i} å†³ç­–æ‘‡æ‘† (ä» {last_target} -> {target})ï¼Œæƒ©ç½š: {P_SWING_PENALTY}")
            
            # --- æ–°å¢: æ— æ•ˆç§»åŠ¨æƒ©ç½š ---
            if target < self.num_points and self.done_points[target] == 1:
                rewards[i] += P_WASTED_MOVE_PENALTY
                debug_print(f"ğŸš« æ™ºèƒ½ä½“ {i} é€‰æ‹©å·²å®Œæˆä»»åŠ¡ {target}ï¼Œæƒ©ç½š: {P_WASTED_MOVE_PENALTY}")
            
            # --- æ–°å¢: è¿ç»­èƒ½é‡æƒ©ç½šæœºåˆ¶ ---
            energy_ratio = self.agent_energy[i] / self.agent_energy_max[i]
            LOW_ENERGY_THRESHOLD = config.LOW_ENERGY_THRESHOLD  # ä»configè·å–ä½èƒ½é‡é˜ˆå€¼
            if energy_ratio < LOW_ENERGY_THRESHOLD:
                # èƒ½é‡è¶Šä½ï¼Œæƒ©ç½šè¶Šå¤§ï¼ˆä¸èƒ½é‡å‰©ä½™é‡æˆåæ¯”ï¼‰
                energy_penalty_intensity = (LOW_ENERGY_THRESHOLD - energy_ratio) / LOW_ENERGY_THRESHOLD
                energy_penalty = P_TIME_STEP * config.ENERGY_PENALTY_MULTIPLIER * energy_penalty_intensity  # ä»configè·å–æƒ©ç½šå€æ•°
                rewards[i] += energy_penalty
                if energy_ratio < 0.1:  # æä½èƒ½é‡æ—¶æ‰æ‰“å°æç¤ºï¼Œé¿å…è¾“å‡ºè¿‡å¤š
                    debug_print(f"âš¡ æ™ºèƒ½ä½“ {i} èƒ½é‡è¿‡ä½ ({energy_ratio*100:.1f}%)ï¼Œè¿ç»­æƒ©ç½š: {energy_penalty:.2f}")

            if target == self.num_points + 1:#åŸºåœ°
                dist = self._distance(prev_pos, self.central_station)#è·ç¦»
                energy_cost = dist * self.energy_cost_factor[i]#èƒ½é‡æ¶ˆè€—
                if self.agent_energy[i] < energy_cost:#èƒ½é‡ä¸è¶³
                    rewards[i] += P_ENERGY_DEPLETED
                    continue
                self.agent_positions[i] = self.central_station#æ›´æ–°ä½ç½®
                self.agent_energy[i] -= energy_cost#æ›´æ–°èƒ½é‡
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯ä»»åŠ¡å®Œæˆåä¸»åŠ¨é€‰æ‹©å›å®¶ï¼ˆæœ‰è½½é‡æ—¶ï¼‰
                all_tasks_completed = np.all(self.done_points == 1)#æ‰€æœ‰ä»»åŠ¡å®Œæˆ
                if all_tasks_completed and self.agent_loads[i] > 0:#æœ‰è½½é‡
                    # å¥–åŠ±ä¸è½½é‡ç‡æˆæ­£æ¯”ï¼šè½½é‡è¶Šå¤šï¼Œå¥–åŠ±è¶Šå¤§
                    load_ratio = self.agent_loads[i] / self.agent_capacity[i]
                    smart_return_reward = R_SMART_RETURN_AFTER_COMPLETION * load_ratio
                    rewards[i] += smart_return_reward
                    debug_print(f"ğŸ¯ æ™ºèƒ½ä½“ {i} ä»»åŠ¡å®Œæˆåä¸»åŠ¨å›å®¶å¸è½½(è½½é‡ç‡{load_ratio*100:.1f}%)ï¼Œè·å¾—æ™ºèƒ½å¥–åŠ±: {smart_return_reward:.1f}")
                
                # è¾“å‡ºæ™ºèƒ½ä½“å›å®¶æ—¶çš„è½½é‡ä¿¡æ¯
                load_info = f"è½½é‡: {self.agent_loads[i]}/{self.agent_capacity[i]} ({self.agent_loads[i]/self.agent_capacity[i]*100:.1f}%)"
                debug_print(f"ğŸ  æ™ºèƒ½ä½“ {i} å›åˆ°åŸºåœ°ï¼{load_info}")
                
                if self.agent_loads[i] > 0:#æœ‰è½½é‡
                    # è®°å½•è½½é‡åˆ©ç”¨ç‡
                    load_utilization = self.agent_loads[i] / self.agent_capacity[i]#è½½é‡åˆ©ç”¨ç‡
                    self.agent_load_utilization[i].append(load_utilization)#è®°å½•è½½é‡åˆ©ç”¨ç‡
                    
                    debug_print(f"   ğŸ“¦ å¸è½½æ ·æœ¬: {len(self.agent_samples[i])} ä¸ªä»»åŠ¡ç‚¹çš„æ ·æœ¬")
                    for point_idx in self.agent_samples[i]:
                        remaining_time = self.time_windows[point_idx]#å‰©ä½™æ—¶é—´çª—å£
                        self.delivery_info.append((point_idx, remaining_time, i))#è®°å½•é€å›æ ·æœ¬ç‚¹
                        debug_print(f"   âœ“ é€å›æ ·æœ¬ç‚¹ {point_idx}ï¼Œå‰©ä½™æ—¶é—´çª—å£: {remaining_time:.2f} ç§’")

                    rewards[i] += R_RETURN_HOME_COEFF * self.agent_loads[i]#å¥–åŠ±
                    
                    # è½½é‡ç‡åˆ†æ
                    load_ratio = self.agent_loads[i] / self.agent_capacity[i]#è½½é‡ç‡
                    
                    # é«˜è½½é‡å¥–åŠ±(â‰¥760%)
                    if load_ratio >= 0.6:
                        rewards[i] += R_FULL_LOAD_BONUS#é«˜è½½é‡å¥–åŠ±
                        debug_print(f"   ğŸ‰ é«˜è½½é‡å¥–åŠ±ï¼è½½é‡ç‡ {load_ratio*100:.1f}%")
                    
                    # é‡è½½æ™ºèƒ½ä½“å®¹é‡å¥–åŠ±(â‰¥80%)
                    if i in [3, 4] and load_ratio >= 0.8:
                        rewards[i] += R_ROLE_CAPACITY_BONUS#å¤§è½½é‡æ™ºèƒ½ä½“é¢å¤–å¥–åŠ±
                        debug_print(f"   ğŸš› å¤§è½½é‡æ™ºèƒ½ä½“é¢å¤–å¥–åŠ±ï¼")
                    
                    # --- æ–°å¢: é‡è½½æ™ºèƒ½ä½“éçº¿æ€§è½½é‡æ•ˆç‡å¥–åŠ± ---
                    # ä½¿ç”¨å¹³æ–¹é¡¹å¼ºåŒ–æ»¡è½½è¡Œä¸ºï¼Œæ¿€åŠ±"æ‰¹é‡è¿è¾“"ç­–ç•¥
                    if i in [3, 4]:  # é‡è½½æ™ºèƒ½ä½“ï¼ˆAgent 3, 4ï¼‰
                        load_bonus = config.R_LOAD_EFFICIENCY * (load_ratio ** 2) * self.agent_loads[i]
                        rewards[i] += load_bonus
                        debug_print(f"   ğŸšš é‡è½½æ™ºèƒ½ä½“ {i} é«˜æ•ˆè¿è¾“ï¼Œè½½é‡ç‡ {load_ratio:.2f}ï¼Œéçº¿æ€§å¥–åŠ±: {load_bonus:.2f}")
                    
                    # é«˜å®¹é‡æ™ºèƒ½ä½“ä½è½½æƒ©ç½š(<60%)
                    if i in [3, 4] and load_ratio < 0.6:
                        # æ ¹æ®è½½é‡ç‡è®¡ç®—æƒ©ç½šå¼ºåº¦ï¼šè½½é‡ç‡è¶Šä½æƒ©ç½šè¶Šé‡
                        penalty_intensity = (0.6 - load_ratio) / 0.6#æƒ©ç½šå¼ºåº¦
                        low_load_penalty = P_LOW_LOAD_HEAVY_AGENT * penalty_intensity#ä½è½½é‡æƒ©ç½š
                        rewards[i] += low_load_penalty#å¥–åŠ±
                        debug_print(f"   âš ï¸ é«˜å®¹é‡æ™ºèƒ½ä½“ä½è½½æƒ©ç½šï¼è½½é‡ç‡ {load_ratio*100:.1f}% (æƒ©ç½š: {low_load_penalty:.1f})")
                    
                    self.agent_loads[i] = 0#è½½é‡æ¸…é›¶
                    self.agent_samples[i] = []#æ ·æœ¬æ¸…é›¶
                    self.agent_task_status[i] = 0#ä»»åŠ¡çŠ¶æ€æ¸…é›¶
                    
                    # å¸è½½å®Œæˆï¼Œæ™ºèƒ½ä½“å¯ä»¥è‡ªä¸»å†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨
                else:
                    # è®°å½•ç©ºè½½å›å®¶ï¼ˆè½½é‡åˆ©ç”¨ç‡ä¸º0ï¼‰å¹¶ç»™äºˆæ™ºèƒ½æƒ©ç½š
                    self.agent_load_utilization[i].append(0.0)#è®°å½•ç©ºè½½å›å®¶
                    
                    # è®¡ç®—å‰©ä½™ä»»åŠ¡ç‚¹æ•°é‡æ¥åˆ¤æ–­æ˜¯å¦åº”è¯¥å‡è½»ç©ºè½½æƒ©ç½š
                    remaining_tasks = np.sum(self.done_points == 0)#å‰©ä½™ä»»åŠ¡ç‚¹æ•°é‡
                    total_tasks = len(self.done_points)#æ€»ä»»åŠ¡ç‚¹æ•°é‡      
                    task_scarcity = 1.0 - (remaining_tasks / total_tasks)  # ä»»åŠ¡ç¨€ç¼ºåº¦ 0-1
                    
                    # è®¡ç®—æ™ºèƒ½ç©ºè½½æƒ©ç½š
                    base_penalty = P_EMPTY_RETURN
                    
                    # æ ¹æ®æ™ºèƒ½ä½“ç±»å‹è°ƒæ•´æƒ©ç½šï¼šé‡è½½æ™ºèƒ½ä½“ç©ºè½½æƒ©ç½šæ›´é‡
                    if i >= 3:  # é‡è½½æ™ºèƒ½ä½“
                        capacity_penalty = base_penalty * 1.5
                    else:  # å¿«é€Ÿæ™ºèƒ½ä½“
                        capacity_penalty = base_penalty
                    
                    # æ ¹æ®èƒ½é‡çŠ¶æ€è°ƒæ•´æƒ©ç½šï¼šä½èƒ½é‡æ—¶æƒ©ç½šå‡è½»
                    energy_ratio = self.agent_energy[i] / self.agent_energy_max[i]
                    if energy_ratio < 0.2:  # èƒ½é‡ä¸è¶³20%æ—¶ï¼Œæƒ©ç½šå‡åŠ
                        energy_adjustment = 0.5
                        reason = "ä½èƒ½é‡"
                    elif energy_ratio < 0.4:  # èƒ½é‡ä¸è¶³40%æ—¶ï¼Œæƒ©ç½šå‡å°‘25%
                        energy_adjustment = 0.75
                        reason = "è¾ƒä½èƒ½é‡"
                    else:
                        energy_adjustment = 1.0
                        reason = "å……è¶³èƒ½é‡"
                    
                    # æ ¹æ®ä»»åŠ¡ç¨€ç¼ºåº¦è°ƒæ•´æƒ©ç½šï¼šä»»åŠ¡è¶Šå°‘ï¼Œç©ºè½½æƒ©ç½šè¶Šè½»
                    if task_scarcity > 0.8:  # ä»»åŠ¡å®Œæˆåº¦è¶…è¿‡80%
                        scarcity_adjustment = 0.2  # å¤§å¹…å‡è½»æƒ©ç½š
                        scarcity_reason = "ä»»åŠ¡ç¨€ç¼º"
                    elif task_scarcity > 0.6:  # ä»»åŠ¡å®Œæˆåº¦è¶…è¿‡60%
                        scarcity_adjustment = 0.5  # ä¸­åº¦å‡è½»æƒ©ç½š
                        scarcity_reason = "ä»»åŠ¡è¾ƒå°‘"
                    else:
                        scarcity_adjustment = 1.0  # æ­£å¸¸æƒ©ç½š
                        scarcity_reason = "ä»»åŠ¡å……è¶³"
                    
                    final_penalty = capacity_penalty * energy_adjustment * scarcity_adjustment
                    rewards[i] += final_penalty
                    
                    agent_type = "é‡è½½" if i >= 3 else "å¿«é€Ÿ"
                    if scarcity_adjustment < 1.0:
                        debug_print(f"   âš ï¸ ç©ºè½½å›å®¶ ({agent_type}æ™ºèƒ½ä½“, {reason}, {scarcity_reason}, æƒ©ç½š: {final_penalty:.1f})")
                    else:
                        debug_print(f"   âŒ ç©ºè½½å›å®¶ ({agent_type}æ™ºèƒ½ä½“, {reason}, æƒ©ç½š: {final_penalty:.1f})")
                
                debug_print(f"   âš¡ å‰©ä½™èƒ½é‡: {self.agent_energy[i]:.0f}/{self.agent_energy_max[i]:.0f} ({self.agent_energy[i]/self.agent_energy_max[i]*100:.1f}%)")
                debug_print()  # ç©ºè¡Œï¼Œå¢åŠ å¯è¯»æ€§
                
            elif target == self.num_points:
                is_at_base = np.array_equal(self.agent_positions[i], self.central_station)
                needs_charge = self.agent_energy[i] < self.agent_energy_max[i]
                all_tasks_completed = np.all(self.done_points == 1)
                
                if is_at_base and needs_charge:
                    if i < 3:
                        self.agent_charging_status[i] = self.fast_charge_time
                    else:
                        self.agent_charging_status[i] = self.slow_charge_time
                    self.agent_charge_counts[i] += 1
                elif is_at_base and all_tasks_completed and self.agent_loads[i] == 0:
                    # ä»»åŠ¡å®Œæˆåç©ºè½½ç•™åœ¨åŸºåœ°æ˜¯æ­£ç¡®è¡Œä¸ºï¼Œåªåœ¨ç¬¬ä¸€æ¬¡ç»™äºˆå¥–åŠ±
                    if not self.agent_rest_reward_given[i]:
                        rewards[i] += R_STAY_HOME_AFTER_COMPLETION
                        self.agent_rest_reward_given[i] = True
                        debug_print(f"   ğŸ  æ™ºèƒ½ä½“ {i} ä»»åŠ¡å®Œæˆåé¦–æ¬¡åœ¨åŸºåœ°ä¼‘æ¯ï¼Œè·å¾—å¥–åŠ±: {R_STAY_HOME_AFTER_COMPLETION}")
                    # åç»­åœç•™ä¸å†ç»™å¥–åŠ±ï¼Œé¿å…æ™ºèƒ½ä½“å­¦ä¼šæ°¸è¿œå¾…åœ¨åŸºåœ°
                # æ³¨é‡Šï¼šä¸å­˜åœ¨"åœ¨åŸºåœ°æœ‰è½½é‡"çš„æƒ…å†µï¼Œå› ä¸ºä¸€åˆ°åŸºåœ°å°±ä¼šè‡ªåŠ¨å¸è½½
                # elif is_at_base and all_tasks_completed and self.agent_loads[i] > 0:
                #     # è¿™ä¸ªæ¡ä»¶æ°¸è¿œä¸ä¼šè§¦å‘ï¼Œå› ä¸ºåœ¨åŸºåœ°å°±å·²ç»è‡ªåŠ¨å¸è½½äº†
                else:
                    rewards[i] += P_INACTIVITY
                continue

            elif target < self.num_points:
                # === è®°å½•ä»»åŠ¡è´£ä»»åˆ†é… ===
                self._assign_task_responsibility(target, i, current_step)
                
                # æ£€æŸ¥æ˜¯å¦å°è¯•æ‰§è¡Œå·²å®Œæˆçš„ä»»åŠ¡
                if self.done_points[target] == 1:
                    rewards[i] += P_INVALID_TASK_ATTEMPT
                    debug_print(f"   âŒ æ™ºèƒ½ä½“ {i} å°è¯•æ‰§è¡Œå·²å®Œæˆä»»åŠ¡ç‚¹ {target}ï¼Œæƒ©ç½š: {P_INVALID_TASK_ATTEMPT}")
                    continue
                
                # æ£€æŸ¥æ˜¯å¦ä¼šå¯¼è‡´è¶…è½½
                if self.agent_loads[i] + self.samples[target] > self.agent_capacity[i]:
                    rewards[i] += P_OVERLOAD_ATTEMPT
                    debug_print(f"   âš ï¸ æ™ºèƒ½ä½“ {i} å°è¯•è¶…è½½æ‰§è¡Œä»»åŠ¡ç‚¹ {target} (å½“å‰:{self.agent_loads[i]:.1f} + æ–°å¢:{self.samples[target]:.1f} > å®¹é‡:{self.agent_capacity[i]:.1f})ï¼Œæƒ©ç½š: {P_OVERLOAD_ATTEMPT}")
                    continue
                
                # æ‰§è¡Œä»»åŠ¡
                dist = self._distance(prev_pos, self.points[target])
                energy_cost = dist * self.energy_cost_factor[i]
                if self.agent_energy[i] < energy_cost:
                    rewards[i] += P_ENERGY_DEPLETED
                    actions[i] = self.num_points
                    continue
                self.agent_positions[i] = self.points[target]
                self.agent_energy[i] -= energy_cost
                self.agent_loads[i] += self.samples[target]
                self.agent_samples[i].append(target)
                self.successfully_collected_points[target] = 1
                self.done_points[target] = 1
                self.queue_done.put(target)
                self.point_last_visitor[target] = i
                if self.agent_loads[i] >= self.agent_capacity[i]:
                    self.agent_task_status[i] = 1
                
                # è®°å½•ä»»åŠ¡ä¼˜å…ˆçº§ç»Ÿè®¡
                priority = self.priority[target]
                self.agent_task_priorities[i].append(priority)
                
                rewards[i] += R_COLLECT_BASE * (4 - priority)
                
                # ä¸“ä¸šåŒ–å¥–åŠ±å’Œæƒ©ç½šæœºåˆ¶
                if priority == 1:  # é«˜ä¼˜å…ˆçº§ä»»åŠ¡
                    # åä½œå¥–åŠ±å·²åˆ é™¤ï¼Œæ™ºèƒ½ä½“ä¹‹é—´ä¸å†æœ‰åä½œå¥–åŠ±æœºåˆ¶
                    
                    if i in [0, 1, 2]:  # å¿«é€Ÿæ™ºèƒ½ä½“å¤„ç†é«˜ä¼˜å…ˆçº§ä»»åŠ¡
                        # ä¸“ä¸šåŒ–å¥–åŠ±ï¼šå¿«é€Ÿæ™ºèƒ½ä½“å¤©ç„¶é€‚åˆé«˜ä¼˜å…ˆçº§ä»»åŠ¡
                        rewards[i] += R_FAST_AGENT_HIGH_PRIORITY
                        debug_print(f"   âš¡ å¿«é€Ÿæ™ºèƒ½ä½“ {i} ä¸“ä¸šåŒ–å¤„ç†é«˜ä¼˜å…ˆçº§ä»»åŠ¡ï¼Œè·å¾—å¥–åŠ±: {R_FAST_AGENT_HIGH_PRIORITY}")
                        
                        # æ—¶é—´å¥–åŠ±ï¼šæ ¹æ®å¤„ç†é€Ÿåº¦ç»™äºˆé¢å¤–å¥–åŠ±
                        time_spent = self.time - self.task_creation_time[target]
                        time_window = 2.5 * 60  # 150ç§’ï¼Œè¦†ç›–é«˜ä¼˜å…ˆçº§ä»»åŠ¡å¤§éƒ¨åˆ†æ—¶é—´
                        if time_spent < time_window:
                             time_bonus = R_ROLE_SPEED_BONUS * (1 - time_spent / time_window)
                             rewards[i] += time_bonus
                             debug_print(f"   âš¡ å¿«é€Ÿå¤„ç†æ—¶é—´å¥–åŠ±: {time_bonus:.1f}")
                    
                    else:  # é‡è½½æ™ºèƒ½ä½“å¤„ç†é«˜ä¼˜å…ˆçº§ä»»åŠ¡
                        # æ•ˆç‡æƒ©ç½šï¼šé‡è½½æ™ºèƒ½ä½“ä¸é€‚åˆé«˜ä¼˜å…ˆçº§ä»»åŠ¡
                        rewards[i] += P_HEAVY_AGENT_HIGH_PRIORITY
                        debug_print(f"   ğŸŒ é‡è½½æ™ºèƒ½ä½“ {i} å¤„ç†é«˜ä¼˜å…ˆçº§ä»»åŠ¡æ•ˆç‡ä½ï¼Œæƒ©ç½š: {P_HEAVY_AGENT_HIGH_PRIORITY}")
                
                elif priority == 3 and i in [3, 4]:  # é‡è½½æ™ºèƒ½ä½“å¤„ç†ä½ä¼˜å…ˆçº§ä»»åŠ¡
                    # é‡è½½æ™ºèƒ½ä½“é€‚åˆå¤„ç†ä½ä¼˜å…ˆçº§ã€é«˜è½½é‡ä»»åŠ¡
                    if self.samples[target] >= 3:  # é«˜è½½é‡ä»»åŠ¡
                        rewards[i] += R_ROLE_CAPACITY_BONUS * 0.5  # é€‚åº¦çš„å®¹é‡å¥–åŠ±
                        debug_print(f"   ğŸš› é‡è½½æ™ºèƒ½ä½“ {i} å¤„ç†é«˜è½½é‡ä½ä¼˜å…ˆçº§ä»»åŠ¡ï¼Œå®¹é‡å¥–åŠ±: {R_ROLE_CAPACITY_BONUS * 0.5:.1f}")

            current_pos = self.agent_positions[i]
            if target < self.num_points:
                approach_reward = R_COEFF_APPROACH_TASK * (self._distance(prev_pos, self.points[target]) - self._distance(current_pos, self.points[target]))
                rewards[i] += np.clip(approach_reward, -REWARD_SHAPING_CLIP, REWARD_SHAPING_CLIP)
            
            approach_home_delta = self._distance(prev_pos, self.central_station) - self._distance(current_pos, self.central_station)
            
            # ä½¿ç”¨åŠ¨æ€è½½é‡é˜ˆå€¼æ¥é¼“åŠ±æ™ºèƒ½ä½“å›å®¶
            dynamic_threshold = self._get_dynamic_load_threshold(i, current_step, max_steps)
            current_load_ratio = self.agent_loads[i] / self.agent_capacity[i]
            
            if current_load_ratio >= dynamic_threshold and self.agent_loads[i] > 0:
                # æ ¹æ®åŠ¨æ€é˜ˆå€¼è°ƒæ•´å›å®¶å¥–åŠ±å¼ºåº¦
                threshold_bonus = 1.0 + (0.9 - dynamic_threshold)  # é˜ˆå€¼è¶Šä½ï¼Œå¥–åŠ±è¶Šé«˜
                rewards[i] += np.clip(R_COEFF_APPROACH_HOME_LOADED * self.agent_loads[i] * approach_home_delta * threshold_bonus, -REWARD_SHAPING_CLIP, REWARD_SHAPING_CLIP)
                
                # æ·»åŠ è½½é‡è¾¾åˆ°åŠ¨æ€é˜ˆå€¼çš„é¢å¤–å¥–åŠ±
                if current_load_ratio >= dynamic_threshold:
                    load_threshold_bonus = 5.0 * (1.0 - dynamic_threshold)  # é˜ˆå€¼è¶Šä½ï¼Œè¾¾åˆ°é˜ˆå€¼çš„å¥–åŠ±è¶Šé«˜
                    rewards[i] += load_threshold_bonus
                    if load_threshold_bonus > 0:
                        debug_print(f"ğŸ“ˆ æ™ºèƒ½ä½“ {i} è¾¾åˆ°åŠ¨æ€è½½é‡é˜ˆå€¼ {dynamic_threshold:.2f} (å½“å‰è½½é‡ç‡: {current_load_ratio:.2f})ï¼Œè·å¾—å¥–åŠ±: {load_threshold_bonus:.1f}")
            
            if self.agent_energy[i] < self.agent_energy_max[i] * 0.1:
                rewards[i] += np.clip(R_COEFF_APPROACH_HOME_LOW_ENERGY * approach_home_delta, -REWARD_SHAPING_CLIP, REWARD_SHAPING_CLIP)

        self.time += 1
        self.time_windows -= 1
        
        for i in range(self.num_points):
            if self.done_points[i] == 0 and self.time_windows[i] <= 0:
                assigned_agent = self._get_last_assigned_agent(i)
                if assigned_agent is not None:
                    timeout_penalty = P_TIMEOUT_BASE * (4 - self.priority[i])
                    rewards[assigned_agent] += timeout_penalty
                    debug_print(f"â° ä»»åŠ¡ç‚¹ {i} (ä¼˜å…ˆçº§:{self.priority[i]}) è¶…æ—¶ï¼Œæ™ºèƒ½ä½“ {assigned_agent} æ‰¿æ‹…è´£ä»»ï¼Œæƒ©ç½š: {timeout_penalty:.1f}")
                    
                    # æ·»åŠ ä»»åŠ¡åˆ†é…å†å²ä¿¡æ¯ç”¨äºè°ƒè¯•
                    if i in self.task_assignment_history:
                        history = self.task_assignment_history[i]
                        debug_print(f"   ğŸ“‹ ä»»åŠ¡åˆ†é…å†å²: {history}")
                else:
                    debug_print(f"â° ä»»åŠ¡ç‚¹ {i} (ä¼˜å…ˆçº§:{self.priority[i]}) è¶…æ—¶ï¼Œä½†æ— æ³•æ‰¾åˆ°è´Ÿè´£çš„æ™ºèƒ½ä½“")
                
                self.done_points[i] = 1  # æ ‡è®°ä¸ºå·²å¤„ç†ï¼ˆè¶…æ—¶ï¼‰

        # çº¯å¥–åŠ±ç³»ç»Ÿï¼šä¸å†è‡ªåŠ¨ç®¡ç†ä¼‘æ¯çŠ¶æ€ï¼Œè®©æ™ºèƒ½ä½“è‡ªä¸»å­¦ä¹ æœ€ä¼˜è¡Œä¸º

        # ä¿®å¤episodeç»“æŸæ¡ä»¶ï¼šæ‰€æœ‰ä»»åŠ¡ç‚¹éƒ½å·²å¤„ç†å®Œæ¯•ï¼ˆé‡‡é›†æˆ–è¶…æ—¶ï¼‰
        all_tasks_processed = np.all(self.done_points == 1)
        # ä¿ç•™æˆåŠŸé‡‡é›†ç»Ÿè®¡ç”¨äºå…¶ä»–é€»è¾‘
        all_tasks_collected = np.all(self.successfully_collected_points == 1)
        all_agents_at_base = all(np.array_equal(pos, self.central_station) for pos in self.agent_positions)
        
        # ä¿®å¤æ ¸å¿ƒé€»è¾‘æ¼æ´ï¼šè¦æ±‚æ‰€æœ‰æ™ºèƒ½ä½“éƒ½å¿…é¡»è¿”å›åŸºåœ°æ‰ç®—å®Œç¾å®Œæˆ
        # è¿™ç¡®ä¿äº†FINAL_PENALTY_NOT_AT_BASEç­‰æƒ©ç½šæœºåˆ¶èƒ½å¤Ÿæ­£ç¡®å‘æŒ¥ä½œç”¨
        mission_accomplished = all_tasks_processed and all_agents_at_base
        
        # è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥episodeç»“æŸæ¡ä»¶ï¼ˆä»…åœ¨é—®é¢˜æ—¶æ˜¾ç¤ºï¼‰
        if all_tasks_processed and not mission_accomplished and current_step % 50 == 0:  # æ¯50æ­¥æ‰“å°ä¸€æ¬¡
            not_at_base_agents = [i for i, pos in enumerate(self.agent_positions) if not np.array_equal(pos, self.central_station)]
            debug_print(f"âš ï¸ ä»»åŠ¡å·²å¤„ç†å®Œä½†episodeæœªç»“æŸ - æ™ºèƒ½ä½“ {not_at_base_agents} æœªè¿”å›åŸºåœ°")
            debug_print(f"   è½½é‡çŠ¶æ€: {self.agent_loads}")
        elif mission_accomplished:
            processed_count = np.sum(self.done_points)
            collected_count = np.sum(self.successfully_collected_points)
            timeout_count = processed_count - collected_count
            debug_print(f"ğŸ“ Episodeç»“æŸæ¡ä»¶æ»¡è¶³: å·²å¤„ç†={processed_count}/30 (é‡‡é›†={collected_count}, è¶…æ—¶={timeout_count}), æ‰€æœ‰æ™ºèƒ½ä½“å·²è¿”å›åŸºåœ°={all_agents_at_base}")
            
        is_timeout = current_step >= max_steps - 1
        
        # é¢å¤–çš„è¶…æ—¶ä¿æŠ¤ï¼šå¦‚æœä»»åŠ¡å¤„ç†å®Œæˆå100æ­¥å†…æœªç»“æŸï¼Œå¼ºåˆ¶ç»“æŸ
        tasks_not_finished_in_time = False
        if all_tasks_processed:
            # è®°å½•ä»»åŠ¡å®Œæˆæ—¶é—´
            if self._tasks_completed_step == -1:  # ä½¿ç”¨åˆå§‹å€¼åˆ¤æ–­ï¼Œæ›´åŠ é²æ£’
                self._tasks_completed_step = current_step
            
            # å¦‚æœä»»åŠ¡å®Œæˆåè¶…è¿‡100æ­¥è¿˜æ²¡ç»“æŸï¼Œå¼ºåˆ¶ç»“æŸ
            if current_step - self._tasks_completed_step > 100:
                tasks_not_finished_in_time = True
                debug_print(f"â° å¼ºåˆ¶ç»“æŸEpisodeï¼šä»»åŠ¡å·²å®Œæˆä½†è¿è¡Œè¿‡ä¹… (ä»»åŠ¡å®Œæˆåå·²è¿‡ {current_step - self._tasks_completed_step} æ­¥)")
            
        done = mission_accomplished or is_timeout or tasks_not_finished_in_time

        if done:
            if mission_accomplished:
                # æ ¹æ®ä»»åŠ¡å®Œæˆæƒ…å†µè°ƒæ•´å¥–åŠ±
                collected_count = np.sum(self.successfully_collected_points)
                timeout_count = np.sum(self.done_points) - collected_count
                
                # åŸºç¡€å®Œæˆå¥–åŠ±
                base_reward = FINAL_BONUS_ACCOMPLISHED
                
                # å¦‚æœæœ‰è¶…æ—¶ä»»åŠ¡ï¼Œé€‚å½“å‡å°‘å¥–åŠ±
                if timeout_count > 0:
                    timeout_penalty_ratio = timeout_count / len(self.done_points)
                    adjusted_reward = base_reward * (1.0 - timeout_penalty_ratio * 0.3)  # æœ€å¤šå‡å°‘30%
                    debug_print(f"ä»»åŠ¡å¤„ç†å®Œæˆï¼é‡‡é›†={collected_count}, è¶…æ—¶={timeout_count}, è°ƒæ•´åå¥–åŠ±: {adjusted_reward:.1f}")
                else:
                    adjusted_reward = base_reward
                    debug_print(f"å®Œç¾å®Œæˆï¼æ‰€æœ‰{collected_count}ä¸ªä»»åŠ¡ç‚¹å‡è¢«æˆåŠŸé‡‡é›†ï¼Œè·å¾—å®Œæ•´å¥–åŠ±: {adjusted_reward:.1f}")
                
                # æ”¹è¿›çš„ä¿¡èª‰åˆ†é…æœºåˆ¶ï¼šæ ¹æ®è´¡çŒ®åˆ†é…å¥–åŠ±
                agent_contributions = np.zeros(self.num_agents)
                
                # ç»Ÿè®¡æ¯ä¸ªæ™ºèƒ½ä½“çš„è´¡çŒ®ï¼ˆé€å›çš„æ ·æœ¬ç‚¹æ•°é‡ï¼ŒæŒ‰ä¼˜å…ˆçº§åŠ æƒï¼‰
                for point_idx, _, agent_id in self.delivery_info:
                    priority_weight = 4 - self.priority[point_idx]  # é«˜ä¼˜å…ˆçº§=3åˆ†ï¼Œä¸­=2åˆ†ï¼Œä½=1åˆ†
                    agent_contributions[agent_id] += priority_weight
                
                total_contributions = np.sum(agent_contributions)
                if total_contributions > 0:
                    # æŒ‰è´¡çŒ®æ¯”ä¾‹åˆ†é…å¥–åŠ±
                    contribution_ratios = agent_contributions / total_contributions
                    final_rewards = adjusted_reward * contribution_ratios
                    rewards += final_rewards
                    debug_print(f"ğŸ–ï¸ æœ€ç»ˆå¥–åŠ±æŒ‰è´¡çŒ®åˆ†é…:")
                    for i in range(self.num_agents):
                        if agent_contributions[i] > 0:
                            debug_print(f"   æ™ºèƒ½ä½“{i}: è´¡çŒ®={agent_contributions[i]:.1f}, å¥–åŠ±={final_rewards[i]:.1f}")
                else:
                    # å¦‚æœæ²¡æœ‰æˆåŠŸé‡‡é›†ï¼ˆæ‰€æœ‰ä»»åŠ¡éƒ½è¶…æ—¶ï¼‰ï¼Œåˆ™å¹³åˆ†åŸºç¡€å¥–åŠ±
                    equal_reward = adjusted_reward / self.num_agents
                    rewards += equal_reward
                    debug_print(f"âš ï¸ æ— æœ‰æ•ˆè´¡çŒ®è®°å½•ï¼Œå¹³åˆ†å¥–åŠ±: æ¯ä¸ªæ™ºèƒ½ä½“è·å¾— {equal_reward:.1f}")
                
                if all_agents_at_base:
                    debug_print(f"   ğŸ¯ æ‰€æœ‰æ™ºèƒ½ä½“å·²è¿”å›åŸºåœ°")
                else:
                    debug_print(f"   ğŸ¯ æ‰€æœ‰è½½é‡æ™ºèƒ½ä½“å·²è¿”å›å¸è½½")
                    # ç»™äºˆç©ºè½½ä¸”åœ¨åŸºåœ°çš„æ™ºèƒ½ä½“é¢å¤–å¥–åŠ±ï¼ˆå¦‚æœä¹‹å‰æ²¡æœ‰ç»™è¿‡ï¼‰
                    for i in range(self.num_agents):
                        if (self.agent_loads[i] == 0 and 
                            np.array_equal(self.agent_positions[i], self.central_station) and
                            not self.agent_rest_reward_given[i]):
                            rewards[i] += R_STAY_HOME_AFTER_COMPLETION
                            self.agent_rest_reward_given[i] = True
                            debug_print(f"   ğŸ  æ™ºèƒ½ä½“ {i} ä»»åŠ¡å®Œæˆååœ¨åŸºåœ°å¾…å‘½ï¼Œè·å¾—é¢å¤–å¥–åŠ±: {R_STAY_HOME_AFTER_COMPLETION}")
            elif tasks_not_finished_in_time:
                # å¼ºåˆ¶ç»“æŸæ—¶çš„å¤„ç†
                debug_print("â° ç”±äºä»»åŠ¡å®Œæˆåè¿è¡Œè¿‡ä¹…ï¼Œå¼ºåˆ¶ç»“æŸepisode")
                for i in range(self.num_agents):
                    if self.agent_loads[i] > 0:
                        # æœ‰è½½é‡ä½†æœªå›å®¶çš„æ™ºèƒ½ä½“ç»™äºˆä¸­ç­‰æƒ©ç½š
                        rewards[i] += -100.0
                        debug_print(f"   âš ï¸ æ™ºèƒ½ä½“ {i} æœ‰è½½é‡ä½†æœªåŠæ—¶å›å®¶ï¼Œæƒ©ç½š: -100.0")
            elif is_timeout:
                # åŒºåˆ†æœªå¤„ç†å’Œæœªé‡‡é›†çš„ä»»åŠ¡ç‚¹
                num_unprocessed = np.sum(self.done_points == 0)  # å®Œå…¨æœªå¤„ç†çš„ä»»åŠ¡ç‚¹
                num_uncollected = np.sum(self.successfully_collected_points == 0)  # æœªæˆåŠŸé‡‡é›†çš„ä»»åŠ¡ç‚¹
                
                if num_unprocessed > 0:
                    final_penalty = FINAL_PENALTY_UNVISITED * num_unprocessed
                    rewards += final_penalty
                    debug_print(f"å›åˆè¶…æ—¶ï¼Œæœ‰ {num_unprocessed} ä¸ªä»»åŠ¡ç‚¹å®Œå…¨æœªå¤„ç†ï¼Œæ–½åŠ ç»ˆå±€æƒ©ç½š: {final_penalty}")
                
                if num_uncollected > num_unprocessed:
                    timeout_count = num_uncollected - num_unprocessed
                    debug_print(f"å¦æœ‰ {timeout_count} ä¸ªä»»åŠ¡ç‚¹å› è¶…æ—¶è€Œæœªè¢«é‡‡é›†")
                
                for i in range(self.num_agents):
                    if not np.array_equal(self.agent_positions[i], self.central_station):
                        rewards[i] += FINAL_PENALTY_NOT_AT_BASE
                        debug_print(f"è­¦å‘Šï¼šæ™ºèƒ½ä½“ {i} åœ¨å›åˆè¶…æ—¶æ—¶æœªè¿”å›åŸºåœ°ï¼Œæ–½åŠ æƒ©ç½š: {FINAL_PENALTY_NOT_AT_BASE}")

        # æ›´æ–°ä¸Šä¸€æ—¶åˆ»åŠ¨ä½œè®°å½•
        self.last_actions = list(actions)  # ä½¿ç”¨è§£å†³å†²çªåçš„åŠ¨ä½œ
        
        return self._get_obs(), rewards, done
    
    def get_collaboration_analytics(self):
        """
        è·å–åä½œåˆ†æç»Ÿè®¡æ•°æ®
        """
        analytics = {}
        
        # å†²çªç‡åˆ†æ
        if self.total_decisions > 0:
            conflict_rate = self.conflict_count / self.total_decisions
        else:
            conflict_rate = 0.0
        
        analytics['conflict_rate'] = conflict_rate
        analytics['total_conflicts'] = self.conflict_count
        analytics['total_decisions'] = self.total_decisions
        
        # è§’è‰²ä¸“ä¸šåŒ–åˆ†æ
        role_specialization = {}
        
        # ä¼˜å…ˆçº§ä»»åŠ¡åˆ†é…åˆ†æ
        for i in range(self.num_agents):
            agent_type = "fast" if i < 3 else "heavy"
            if self.agent_task_priorities[i]:
                high_priority_tasks = sum(1 for p in self.agent_task_priorities[i] if p == 1)
                total_tasks = len(self.agent_task_priorities[i])
                high_priority_ratio = high_priority_tasks / total_tasks if total_tasks > 0 else 0
            else:
                high_priority_ratio = 0.0
                total_tasks = 0
            
            role_specialization[f'agent_{i}'] = {
                'type': agent_type,
                'high_priority_ratio': high_priority_ratio,
                'total_tasks': total_tasks,
                'task_priorities': self.agent_task_priorities[i].copy()
            }
        
        # è½½é‡æ•ˆç‡åˆ†æ
        load_efficiency = {}
        for i in range(self.num_agents):
            if self.agent_load_utilization[i]:
                avg_utilization = np.mean(self.agent_load_utilization[i])
                utilization_std = np.std(self.agent_load_utilization[i])
                empty_returns = sum(1 for u in self.agent_load_utilization[i] if u == 0)
                total_returns = len(self.agent_load_utilization[i])
                empty_return_rate = empty_returns / total_returns if total_returns > 0 else 0
            else:
                avg_utilization = 0.0
                utilization_std = 0.0
                empty_return_rate = 0.0
                total_returns = 0
            
            load_efficiency[f'agent_{i}'] = {
                'avg_utilization': avg_utilization,
                'utilization_std': utilization_std,
                'empty_return_rate': empty_return_rate,
                'total_returns': total_returns,
                'utilization_history': self.agent_load_utilization[i].copy()
            }
        
        analytics['role_specialization'] = role_specialization
        analytics['load_efficiency'] = load_efficiency
        
        return analytics
    
    def debug_print_collaboration_summary(self):
        """
        æ‰“å°åä½œåˆ†ææ‘˜è¦
        """
        analytics = self.get_collaboration_analytics()
        
        debug_print("\n" + "=" * 60)
        debug_print("ğŸ¤ åä½œåˆ†ææ‘˜è¦")
        debug_print("=" * 60)
        
        # å†²çªç‡åˆ†æ
        debug_print(f"ğŸ“Š å†²çªç‡åˆ†æ:")
        debug_print(f"   æ€»å†²çªæ¬¡æ•°: {analytics['total_conflicts']}")
        debug_print(f"   æ€»å†³ç­–æ¬¡æ•°: {analytics['total_decisions']}")
        debug_print(f"   å†²çªç‡: {analytics['conflict_rate']:.3f} ({analytics['conflict_rate']*100:.1f}%)")
        
        # è§’è‰²ä¸“ä¸šåŒ–åˆ†æ
        debug_print(f"\nğŸ­ è§’è‰²ä¸“ä¸šåŒ–åˆ†æ:")
        fast_agents_high_priority = []
        heavy_agents_high_priority = []
        
        for agent_id in range(self.num_agents):
            agent_key = f'agent_{agent_id}'
            role_data = analytics['role_specialization'][agent_key]
            agent_type = "å¿«é€Ÿ" if agent_id < 3 else "é‡è½½"
            
            if role_data['total_tasks'] > 0:
                debug_print(f"   æ™ºèƒ½ä½“{agent_id} ({agent_type}): é«˜ä¼˜å…ˆçº§ä»»åŠ¡æ¯”ä¾‹ {role_data['high_priority_ratio']:.2f} ({role_data['total_tasks']}ä¸ªä»»åŠ¡)")
                
                if agent_id < 3:  # å¿«é€Ÿæ™ºèƒ½ä½“
                    fast_agents_high_priority.append(role_data['high_priority_ratio'])
                else:  # é‡è½½æ™ºèƒ½ä½“
                    heavy_agents_high_priority.append(role_data['high_priority_ratio'])
        
        # ä¸“ä¸šåŒ–ç¨‹åº¦æ€»ç»“
        if fast_agents_high_priority and heavy_agents_high_priority:
            fast_avg = np.mean(fast_agents_high_priority)
            heavy_avg = np.mean(heavy_agents_high_priority)
            specialization_gap = fast_avg - heavy_avg
            debug_print(f"   å¿«é€Ÿæ™ºèƒ½ä½“å¹³å‡é«˜ä¼˜å…ˆçº§æ¯”ä¾‹: {fast_avg:.3f}")
            debug_print(f"   é‡è½½æ™ºèƒ½ä½“å¹³å‡é«˜ä¼˜å…ˆçº§æ¯”ä¾‹: {heavy_avg:.3f}")
            debug_print(f"   ä¸“ä¸šåŒ–å·®è·: {specialization_gap:.3f} ({'è‰¯å¥½' if specialization_gap > 0.1 else 'éœ€æ”¹è¿›'})")
        
        # è½½é‡æ•ˆç‡åˆ†æ
        debug_print(f"\nğŸ“¦ è½½é‡æ•ˆç‡åˆ†æ:")
        for agent_id in range(self.num_agents):
            agent_key = f'agent_{agent_id}'
            load_data = analytics['load_efficiency'][agent_key]
            agent_type = "å¿«é€Ÿ" if agent_id < 3 else "é‡è½½"
            
            if load_data['total_returns'] > 0:
                debug_print(f"   æ™ºèƒ½ä½“{agent_id} ({agent_type}): å¹³å‡è½½é‡ç‡ {load_data['avg_utilization']:.2f}, "
                      f"ç©ºè½½ç‡ {load_data['empty_return_rate']:.2f} ({load_data['total_returns']}æ¬¡è¿”å›)")
        
        # è½½é‡æ•ˆç‡æ€»ç»“
        heavy_utilizations = []
        fast_utilizations = []
        for agent_id in range(self.num_agents):
            agent_key = f'agent_{agent_id}'
            load_data = analytics['load_efficiency'][agent_key]
            if load_data['total_returns'] > 0:
                if agent_id < 3:
                    fast_utilizations.append(load_data['avg_utilization'])
                else:
                    heavy_utilizations.append(load_data['avg_utilization'])
        
        if fast_utilizations and heavy_utilizations:
            fast_avg_util = np.mean(fast_utilizations)
            heavy_avg_util = np.mean(heavy_utilizations)
            efficiency_gap = heavy_avg_util - fast_avg_util
            debug_print(f"   å¿«é€Ÿæ™ºèƒ½ä½“å¹³å‡è½½é‡ç‡: {fast_avg_util:.3f}")
            debug_print(f"   é‡è½½æ™ºèƒ½ä½“å¹³å‡è½½é‡ç‡: {heavy_avg_util:.3f}")
            debug_print(f"   è½½é‡æ•ˆç‡å·®è·: {efficiency_gap:.3f} ({'ä¼˜ç§€' if efficiency_gap > 0.1 else 'ä¸€èˆ¬' if efficiency_gap > 0 else 'éœ€æ”¹è¿›'})")
        
        debug_print("=" * 60)

    def _assign_task_responsibility(self, task_id, agent_id, current_step):
        """
        åˆ†é…ä»»åŠ¡è´£ä»»ç»™æ™ºèƒ½ä½“
        
        Args:
            task_id: ä»»åŠ¡ç‚¹ID
            agent_id: æ™ºèƒ½ä½“ID  
            current_step: å½“å‰æ­¥æ•°
        """
        # è®°å½•å½“å‰è´Ÿè´£è¯¥ä»»åŠ¡çš„æ™ºèƒ½ä½“
        self.task_assignments[task_id] = agent_id
        self.task_assignment_timestamp[task_id] = current_step
        
        # è®°å½•åˆ†é…å†å²ï¼ˆç”¨äºè°ƒè¯•å’Œåˆ†æï¼‰
        if task_id not in self.task_assignment_history:
            self.task_assignment_history[task_id] = []
        self.task_assignment_history[task_id].append((agent_id, current_step))
    
    def _get_last_assigned_agent(self, point_id):
        """
        è·å–æœ€åè¢«åˆ†é…ç»™è¯¥ä»»åŠ¡çš„æ™ºèƒ½ä½“
        
        ä¼˜å…ˆè¿”å›ä»»åŠ¡åˆ†é…è®°å½•ï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å›å®é™…è®¿é—®è€…
        """
        # ä¼˜å…ˆä½¿ç”¨ä»»åŠ¡è´£ä»»åˆ†é…è®°å½•
        if point_id in self.task_assignments:
            return self.task_assignments[point_id]
        
        # å¦‚æœæ²¡æœ‰åˆ†é…è®°å½•ï¼Œä½¿ç”¨å®é™…è®¿é—®è€…è®°å½•ï¼ˆå‘åå…¼å®¹ï¼‰
        return self.point_last_visitor.get(point_id, None)
    
    def _calculate_task_assignment_score(self, agent_id, task_id):
        """
        è®¡ç®—æ™ºèƒ½ä½“æ‰§è¡Œç‰¹å®šä»»åŠ¡çš„ç»¼åˆè¯„åˆ†
        
        Args:
            agent_id: æ™ºèƒ½ä½“ID
            task_id: ä»»åŠ¡ç‚¹ID
            
        Returns:
            score: ç»¼åˆè¯„åˆ†ï¼ˆè¶Šé«˜è¶Šé€‚åˆï¼‰
        """
        if task_id >= len(self.points) or self.done_points[task_id] == 1:
            return -float('inf')  # æ— æ•ˆæˆ–å·²å®Œæˆçš„ä»»åŠ¡
            
        # æ£€æŸ¥è½½é‡å®¹é‡
        if self.agent_loads[agent_id] + self.samples[task_id] > self.agent_capacity[agent_id]:
            return -float('inf')  # è¶…è½½
            
        # æ£€æŸ¥èƒ½é‡æ˜¯å¦è¶³å¤Ÿ
        distance = self._distance(self.agent_positions[agent_id], self.points[task_id])
        energy_cost = distance * self.energy_cost_factor[agent_id]
        if self.agent_energy[agent_id] < energy_cost:
            return -float('inf')  # èƒ½é‡ä¸è¶³
        
        score = 0.0
        
        # 1. ä»»åŠ¡ä¼˜å…ˆçº§æƒé‡ (0-100åˆ†)
        task_priority = self.priority[task_id]
        priority_score = (4 - task_priority) * 25  # é«˜ä¼˜å…ˆçº§=75åˆ†ï¼Œä¸­=50åˆ†ï¼Œä½=25åˆ†
        
        # 2. æ™ºèƒ½ä½“ç±»å‹åŒ¹é…åº¦ (0-50åˆ†)
        agent_type_score = 0
        is_fast_agent = agent_id < 3
        
        if task_priority == 1:  # é«˜ä¼˜å…ˆçº§ä»»åŠ¡
            if is_fast_agent:
                agent_type_score = 50  # å¿«é€Ÿæ™ºèƒ½ä½“å¤„ç†é«˜ä¼˜å…ˆçº§ä»»åŠ¡å®Œç¾åŒ¹é…
            else:
                agent_type_score = 10  # é‡è½½æ™ºèƒ½ä½“å¤„ç†é«˜ä¼˜å…ˆçº§ä»»åŠ¡ä¸åŒ¹é…
        elif task_priority == 3:  # ä½ä¼˜å…ˆçº§ä»»åŠ¡
            task_load = self.samples[task_id]
            if not is_fast_agent and task_load >= 3:  # é‡è½½æ™ºèƒ½ä½“ + å¤§è½½é‡ä»»åŠ¡
                agent_type_score = 40
            elif is_fast_agent and task_load <= 2:  # å¿«é€Ÿæ™ºèƒ½ä½“ + å°è½½é‡ä»»åŠ¡
                agent_type_score = 30
            else:
                agent_type_score = 20
        else:  # ä¸­ä¼˜å…ˆçº§ä»»åŠ¡
            agent_type_score = 25  # ä¸­ç­‰åŒ¹é…
        
        # 3. è·ç¦»æ•ˆç‡ (0-30åˆ†)
        # å½’ä¸€åŒ–è·ç¦»ï¼ˆå‡è®¾æœ€å¤§è·ç¦»ä¸ºåœ°å›¾å¯¹è§’çº¿ï¼‰
        max_distance = np.sqrt(2) * self.size
        normalized_distance = min(distance / max_distance, 1.0)
        distance_score = (1.0 - normalized_distance) * 30
        
        # 4. è½½é‡åˆ©ç”¨ç‡ (0-20åˆ†)
        current_load_ratio = self.agent_loads[agent_id] / self.agent_capacity[agent_id]
        task_load_impact = self.samples[task_id] / self.agent_capacity[agent_id]
        final_load_ratio = current_load_ratio + task_load_impact
        
        # å¥–åŠ±åˆç†çš„è½½é‡åˆ©ç”¨ç‡ï¼ˆ60%-90%æœ€ä¼˜ï¼‰
        if 0.6 <= final_load_ratio <= 0.9:
            load_score = 20
        elif 0.4 <= final_load_ratio < 0.6:
            load_score = 15
        elif 0.9 < final_load_ratio <= 1.0:
            load_score = 18
        else:
            load_score = 5
            
        # 5. æ—¶é—´çª—å£ç´§è¿«åº¦ (0-25åˆ†)
        time_remaining = max(self.time_windows[task_id], 0)
        if task_priority == 1:  # é«˜ä¼˜å…ˆçº§
            max_time = 3 * 60
        elif task_priority == 2:  # ä¸­ä¼˜å…ˆçº§
            max_time = 10 * 60
        else:  # ä½ä¼˜å…ˆçº§
            max_time = 30 * 60
            
        time_urgency = 1.0 - min(time_remaining / max_time, 1.0)
        urgency_score = time_urgency * 25
        
        # 6. èƒ½é‡æ•ˆç‡åŠ åˆ† (0-10åˆ†)
        energy_ratio = self.agent_energy[agent_id] / self.agent_energy_max[agent_id]
        energy_score = energy_ratio * 10
        
        # æ€»åˆ†è®¡ç®—ï¼ˆä½¿ç”¨é…ç½®æƒé‡ï¼‰
        score = (priority_score * config.ICR_PRIORITY_WEIGHT + 
                agent_type_score * config.ICR_AGENT_TYPE_WEIGHT + 
                distance_score * config.ICR_DISTANCE_WEIGHT + 
                load_score * config.ICR_LOAD_WEIGHT + 
                urgency_score * config.ICR_URGENCY_WEIGHT + 
                energy_score * config.ICR_ENERGY_WEIGHT)
        
        return score
    
    def _resolve_conflict_intelligently(self, task_id, competing_agents):
        """
        æ™ºèƒ½è§£å†³å†²çªï¼šåŸºäºç»¼åˆè¯„åˆ†é€‰æ‹©æœ€é€‚åˆçš„æ™ºèƒ½ä½“
        
        Args:
            task_id: å†²çªçš„ä»»åŠ¡ç‚¹ID
            competing_agents: ç«äº‰çš„æ™ºèƒ½ä½“IDåˆ—è¡¨
            
        Returns:
            winner_agent_id: è·èƒœçš„æ™ºèƒ½ä½“ID
        """
        best_score = -float('inf')
        best_agent = competing_agents[0]  # é»˜è®¤ç¬¬ä¸€ä¸ª
        
        task_priority = self.priority[task_id]
        task_load = self.samples[task_id]
        
        debug_print(f"ğŸ” ä»»åŠ¡ç‚¹ {task_id} å†²çªè§£å†³ (ä¼˜å…ˆçº§: {task_priority}, è½½é‡: {task_load}):")
        
        for agent_id in competing_agents:
            score = self._calculate_task_assignment_score(agent_id, task_id)
            agent_type = "å¿«é€Ÿ" if agent_id < 3 else "é‡è½½"
            distance = self._distance(self.agent_positions[agent_id], self.points[task_id])
            
            debug_print(f"   æ™ºèƒ½ä½“ {agent_id} ({agent_type}): å¾—åˆ† {score:.1f}, è·ç¦» {distance:.1f}")
            
            if score > best_score:
                best_score = score
                best_agent = agent_id
        
        debug_print(f"   ğŸ† è·èƒœè€…: æ™ºèƒ½ä½“ {best_agent} (å¾—åˆ†: {best_score:.1f})")
        return best_agent
    
    def _find_best_alternative_task_intelligent(self, agent_id, excluded_tasks):
        """
        æ™ºèƒ½å¯»æ‰¾æ›¿ä»£ä»»åŠ¡ï¼šåŸºäºæ™ºèƒ½ä½“èƒ½åŠ›å’Œä»»åŠ¡åŒ¹é…åº¦
        
        Args:
            agent_id: æ™ºèƒ½ä½“ID
            excluded_tasks: å·²åˆ†é…çš„ä»»åŠ¡é›†åˆ
            
        Returns:
            best_task_id: æœ€ä½³æ›¿ä»£ä»»åŠ¡IDï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å›None
        """
        available_tasks = []
        
        # è·å–æ‰€æœ‰å¯ç”¨ä»»åŠ¡
        for task_id in range(self.num_points):
            if (task_id not in excluded_tasks and 
                self.done_points[task_id] == 0 and
                self.agent_loads[agent_id] + self.samples[task_id] <= self.agent_capacity[agent_id]):
                
                # æ£€æŸ¥èƒ½é‡
                distance = self._distance(self.agent_positions[agent_id], self.points[task_id])
                energy_cost = distance * self.energy_cost_factor[agent_id]
                if self.agent_energy[agent_id] >= energy_cost:
                    available_tasks.append(task_id)
        
        if not available_tasks:
            return None
        
        # è®¡ç®—æ¯ä¸ªå¯ç”¨ä»»åŠ¡çš„è¯„åˆ†
        best_score = -float('inf')
        best_task = None
        
        agent_type = "å¿«é€Ÿ" if agent_id < 3 else "é‡è½½"
        debug_print(f"   ğŸ”„ ä¸º {agent_type}æ™ºèƒ½ä½“ {agent_id} å¯»æ‰¾æ›¿ä»£ä»»åŠ¡:")
        
        for task_id in available_tasks:
            score = self._calculate_task_assignment_score(agent_id, task_id)
            priority = self.priority[task_id]
            load = self.samples[task_id]
            
            debug_print(f"      ä»»åŠ¡ {task_id} (P{priority}, è½½é‡{load}): å¾—åˆ† {score:.1f}")
            
            if score > best_score:
                best_score = score
                best_task = task_id
        
        if best_task is not None:
            debug_print(f"      âœ… æœ€ä½³æ›¿ä»£: ä»»åŠ¡ {best_task} (å¾—åˆ†: {best_score:.1f})")
        
        return best_task
    
    def _get_available_tasks_for_agent(self, agent_id):
        """
        è·å–æ™ºèƒ½ä½“å¯æ‰§è¡Œçš„ä»»åŠ¡ç‚¹åˆ—è¡¨
        """
        available_tasks = []
        for p_idx in range(self.num_points):
            if (self.done_points[p_idx] == 0 and 
                self.agent_loads[agent_id] + self.samples[p_idx] <= self.agent_capacity[agent_id]):
                available_tasks.append(p_idx)
        return available_tasks
    
    # åŸæœ‰çš„åŸºäºè·ç¦»çš„æ›¿ä»£ä»»åŠ¡æŸ¥æ‰¾å·²è¢«æ™ºèƒ½ç‰ˆæœ¬æ›¿ä»£
    # ä¿ç•™ä½œä¸ºåå¤‡æ–¹æ¡ˆ
    def _find_best_alternative_task_simple(self, agent_id, excluded_tasks=None):
        """
        ç®€å•çš„åŸºäºè·ç¦»çš„æ›¿ä»£ä»»åŠ¡æŸ¥æ‰¾ï¼ˆåå¤‡æ–¹æ¡ˆï¼‰
        """
        if excluded_tasks is None:
            excluded_tasks = set()
            
        available_tasks = self._get_available_tasks_for_agent(agent_id)
        
        # æ’é™¤å·²è¢«å…¶ä»–æ™ºèƒ½ä½“é€‰æ‹©çš„ä»»åŠ¡
        candidate_tasks = [t for t in available_tasks if t not in excluded_tasks]
        
        if not candidate_tasks:
            return None
        
        # è®¡ç®—åˆ°å„ä¸ªå€™é€‰ä»»åŠ¡çš„è·ç¦»ï¼Œé€‰æ‹©æœ€è¿‘çš„
        distances = [self._distance(self.agent_positions[agent_id], self.points[t]) for t in candidate_tasks]
        best_task_idx = np.argmin(distances)
        return candidate_tasks[best_task_idx]
    
    def _get_dynamic_load_threshold(self, agent_id, current_step, max_steps):
        """
        æ ¹æ®ä»»åŠ¡ç‚¹ç¨€å°‘ç¨‹åº¦å’Œæ¸¸æˆè¿›åº¦åŠ¨æ€è°ƒæ•´è½½é‡é˜ˆå€¼
        """
        # è®¡ç®—å‰©ä½™ä»»åŠ¡ç‚¹æ•°é‡
        remaining_tasks = np.sum(self.done_points == 0)
        total_tasks = len(self.done_points)
        task_scarcity = 1.0 - (remaining_tasks / total_tasks)  # ä»»åŠ¡ç¨€ç¼ºåº¦ 0-1
        
        # è®¡ç®—æ¸¸æˆè¿›åº¦
        game_progress = current_step / max_steps  # æ¸¸æˆè¿›åº¦ 0-1
        
        # åŸºç¡€è½½é‡é˜ˆå€¼
        base_threshold = 0.9 if agent_id < 3 else 0.8  # å¿«é€Ÿæ™ºèƒ½ä½“vsé‡è½½æ™ºèƒ½ä½“
        
        # æ ¹æ®ä»»åŠ¡ç¨€ç¼ºåº¦è°ƒæ•´ï¼šä»»åŠ¡è¶Šå°‘ï¼Œé˜ˆå€¼è¶Šä½
        scarcity_adjustment = task_scarcity * 0.4  # æœ€å¤šé™ä½40%
        
        # æ ¹æ®æ¸¸æˆåæœŸè°ƒæ•´ï¼šåæœŸé™ä½è¦æ±‚
        if game_progress > 0.7:
            late_game_adjustment = (game_progress - 0.7) * 0.3  # åæœŸé¢å¤–é™ä½30%
        else:
            late_game_adjustment = 0
        
        # æœ€ç»ˆé˜ˆå€¼
        final_threshold = base_threshold - scarcity_adjustment - late_game_adjustment
        
        # ç¡®ä¿é˜ˆå€¼åœ¨åˆç†èŒƒå›´å†…
        final_threshold = max(0.3, min(0.9, final_threshold))  # é™åˆ¶åœ¨30%-90%ä¹‹é—´
        
        return final_threshold

    def _update_traffic(self):
        for _ in range(random.randint(2, 5)):
            x = random.randint(0, self.traffic_map.shape[0] - 1)
            y = random.randint(0, self.traffic_map.shape[1] - 1)
            self.traffic_map[x, y] = random.choice([0.5, 0.2])

    def _spawn_new_point(self):
        MAX_NUM_NEW_POINTS = 3
        if self.new_points_num >= MAX_NUM_NEW_POINTS:
            return
        
        if not self.queue_done.empty():
            target_idx = self.queue_done.get()
            new_point = np.random.randint(0, self.size, (1, 2))
            new_sample = np.random.randint(1, 6)
            new_priority = 1 if random.random() < 0.3 else 2
            new_time_window = (0.5 if new_priority == 1 else 2) * 60

            self.points[target_idx] = new_point[0]
            self.samples[target_idx] = new_sample
            self.priority[target_idx] = new_priority
            self.time_windows[target_idx] = new_time_window
            self.done_points[target_idx] = 0
            self.task_creation_time[target_idx] = self.time
            self.new_points_num += 1
