"""
å¢å¼ºç‰ˆå¤šç§å­å®éªŒç»“æœåˆ†æå™¨

è¿™ä¸ªå·¥å…·ä¸“é—¨ç”¨äºåˆ†æå¤šç§å­æ‰¹é‡å®éªŒçš„ç»“æœï¼Œæä¾›ç»Ÿè®¡å¯é çš„æ€§èƒ½å¯¹æ¯”å’Œæ·±åº¦æ”¹è¿›å»ºè®®ã€‚

åŠŸèƒ½ç‰¹æ€§ï¼š
- åŠ è½½å¤šç§å­å®éªŒç»“æœï¼ˆæ”¯æŒexperiment_resultså’Œparallel_experiment_resultsï¼‰
- è®¡ç®—å¹³å‡å€¼å’Œæ ‡å‡†å·®
- ç”Ÿæˆå¸¦ç½®ä¿¡åŒºé—´çš„å¯è§†åŒ–å›¾è¡¨
- æä¾›ç§‘å­¦çš„ç»Ÿè®¡åˆ†ææŠ¥å‘Š
- æ”¯æŒæ˜¾è‘—æ€§æ£€éªŒ
- å­¦ä¹ æ›²çº¿æ·±åº¦åˆ†æï¼ˆæ”¶æ•›é€Ÿåº¦ã€ç¨³å®šæ€§ã€éœ‡è¡ç¨‹åº¦ï¼‰
- å¥–åŠ±ç»„ä»¶è´¡çŒ®åˆ†æ
- è¶…å‚æ•°æ•æ„Ÿæ€§åˆ†æ
- ç”Ÿæˆå…·ä½“çš„ä»£ç æ”¹è¿›å»ºè®®

ä½¿ç”¨æ–¹æ³•ï¼š
python multi_seed_analyzer.py --results_dir parallel_experiment_results
python multi_seed_analyzer.py --results_dir experiment_results
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import json
import os
import glob
from pathlib import Path
from datetime import datetime
from scipy import stats
import seaborn as sns

from reward_config import RewardExperimentConfig, EXPERIMENT_CONFIGS

# ç‹¬ç«‹çš„è°ƒè¯•æ‰“å°å‡½æ•°
def debug_print(*args, **kwargs):
    """ç‹¬ç«‹çš„æ‰“å°å‡½æ•°ï¼Œä¸ä¾èµ–config"""
    print(*args, **kwargs)

# è®¾ç½®ä¸­æ–‡å­—ä½“ - æ”¯æŒWindows/Mac/Linuxå¤šå¹³å°
import platform
import matplotlib.font_manager as fm

def setup_chinese_font():
    """è®¾ç½®ä¸­æ–‡å­—ä½“"""
    system = platform.system()
    
    if system == "Windows":
        # Windowsç³»ç»Ÿä¼˜å…ˆå­—ä½“
        chinese_fonts = ['Microsoft YaHei', 'SimHei', 'KaiTi', 'FangSong']
    elif system == "Darwin":  # macOS
        chinese_fonts = ['PingFang SC', 'Hiragino Sans GB', 'STSong']
    else:  # Linux
        chinese_fonts = ['WenQuanYi Micro Hei', 'DejaVu Sans']
    
    # æŸ¥æ‰¾ç³»ç»Ÿä¸­å¯ç”¨çš„ä¸­æ–‡å­—ä½“
    available_fonts = [f.name for f in fm.fontManager.ttflist]
    
    for font in chinese_fonts:
        if font in available_fonts:
            plt.rcParams['font.sans-serif'] = [font]
            debug_print(f"ğŸ”¤ ä½¿ç”¨å­—ä½“: {font}")
            break
    else:
        # å¦‚æœæ²¡æœ‰ä¸­æ–‡å­—ä½“ï¼Œä½¿ç”¨è‹±æ–‡æ ‡é¢˜
        plt.rcParams['font.sans-serif'] = ['Arial', 'DejaVu Sans']
        debug_print("âš ï¸ æœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œå°†ä½¿ç”¨è‹±æ–‡æ ‡é¢˜")
        return False
    
    plt.rcParams['axes.unicode_minus'] = False
    return True

# è®¾ç½®å­—ä½“
USE_CHINESE = setup_chinese_font()

class MultiSeedAnalyzer:
    """å¢å¼ºç‰ˆå¤šç§å­å®éªŒç»“æœåˆ†æå™¨"""
    
    def __init__(self, results_dir="parallel_experiment_results"):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            results_dir: å®éªŒç»“æœç›®å½•ï¼ˆæ”¯æŒexperiment_resultsæˆ–parallel_experiment_resultsï¼‰
        """
        self.results_dir = results_dir
        self.experiment_data = {}  # {level: {seed: data}}
        self.summary_stats = {}  # {level: {metric: {mean, std, ci}}}
        self.learning_analysis = {}  # {level: learning_curve_metrics}
        self.improvement_suggestions = {}  # {level: suggestions}
        
    def load_all_results(self):
        """åŠ è½½æ‰€æœ‰å¤šç§å­å®éªŒç»“æœ"""
        debug_print("ğŸ“Š åŠ è½½å¤šç§å­å®éªŒç»“æœ...")
        
        if not os.path.exists(self.results_dir):
            debug_print(f"âŒ ç»“æœç›®å½•ä¸å­˜åœ¨: {self.results_dir}")
            return
        
        debug_print(f"   ğŸ“ ç»“æœç›®å½•: {self.results_dir}")
        
        # æ£€æµ‹ç»“æœç›®å½•ç±»å‹
        if self._is_experiment_results_dir():
            total_loaded = self._load_experiment_results()
        elif self._is_parallel_experiment_results_dir():
            total_loaded = self._load_parallel_experiment_results()
        else:
            debug_print(f"âŒ æ— æ³•è¯†åˆ«çš„ç»“æœç›®å½•ç»“æ„: {self.results_dir}")
            return
        
        debug_print(f"ğŸ“Š æ€»è®¡åŠ è½½ {total_loaded} ä¸ªå®éªŒç»“æœ")
        
        # æ˜¾ç¤ºæ•°æ®æ‘˜è¦
        self._print_data_summary()
    
    def _is_experiment_results_dir(self):
        """æ£€æµ‹æ˜¯å¦ä¸ºexperiment_resultsç›®å½•ç»“æ„"""
        # æŸ¥æ‰¾.jsonå’Œ.npyæ–‡ä»¶
        json_files = glob.glob(os.path.join(self.results_dir, "*.json"))
        npy_files = glob.glob(os.path.join(self.results_dir, "*_rewards.npy"))
        return len(json_files) > 0 and len(npy_files) > 0
    
    def _is_parallel_experiment_results_dir(self):
        """æ£€æµ‹æ˜¯å¦ä¸ºparallel_experiment_resultsç›®å½•ç»“æ„"""
        # æŸ¥æ‰¾å­ç›®å½•ä¸­çš„rewards_log.npyæ–‡ä»¶
        pattern = os.path.join(self.results_dir, "*/*/rewards_log.npy")
        files = glob.glob(pattern)
        return len(files) > 0
    
    def _load_experiment_results(self):
        """åŠ è½½experiment_resultsç›®å½•ä¸­çš„ç»“æœ"""
        debug_print("   ğŸ“ æ£€æµ‹åˆ°experiment_resultsç›®å½•ç»“æ„")
        total_loaded = 0
        
        # è·å–æ‰€æœ‰jsonæ–‡ä»¶
        json_files = glob.glob(os.path.join(self.results_dir, "*.json"))
        
        for json_file in json_files:
            try:
                # è§£ææ–‡ä»¶åè·å–å®éªŒç­‰çº§
                filename = os.path.basename(json_file)
                parts = filename.replace('.json', '').split('_')
                level = parts[0]
                timestamp = '_'.join(parts[1:])
                
                # åŠ è½½JSONæ•°æ®
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # ä½¿ç”¨æ—¶é—´æˆ³ä½œä¸ºä¼ªç§å­ï¼ˆå•æ¬¡å®éªŒï¼‰
                seed = hash(timestamp) % 10000
                
                if level not in self.experiment_data:
                    self.experiment_data[level] = {}
                
                # æå–å¥–åŠ±æ•°æ®
                episode_rewards = np.array(data['episode_rewards'])
                
                # æå–åä½œåˆ†ææ•°æ®
                collaboration_analytics = data.get('collaboration_analytics', {})
                
                self.experiment_data[level][seed] = {
                    'rewards': episode_rewards,
                    'analytics': collaboration_analytics,
                    'file_path': json_file,
                    'timestamp': timestamp,
                    'config': data.get('reward_config', {}),
                    'final_metrics': data.get('final_metrics', {})
                }
                
                total_loaded += 1
                debug_print(f"     âœ… {level} ({timestamp}): å·²åŠ è½½ ({episode_rewards.shape})")
                
            except Exception as e:
                debug_print(f"     âŒ åŠ è½½å¤±è´¥ {json_file}: {e}")
        
        return total_loaded
    
    def _load_parallel_experiment_results(self):
        """åŠ è½½parallel_experiment_resultsç›®å½•ä¸­çš„ç»“æœ"""
        debug_print("   ğŸ“ æ£€æµ‹åˆ°parallel_experiment_resultsç›®å½•ç»“æ„")
        total_loaded = 0
        
        # éå†æ‰€æœ‰å®éªŒç­‰çº§ç›®å½•
        for level_dir in os.listdir(self.results_dir):
            level_path = os.path.join(self.results_dir, level_dir)
            
            if not os.path.isdir(level_path):
                continue
                
            if level_dir not in self.experiment_data:
                self.experiment_data[level_dir] = {}
            
            debug_print(f"   ğŸ“ å¤„ç†ç­‰çº§: {level_dir}")
            
            # æŸ¥æ‰¾æ‰€æœ‰å¥–åŠ±æ–‡ä»¶ï¼ˆåœ¨å­ç›®å½•ä¸­ï¼‰
            search_pattern = os.path.join(level_path, "*/rewards_log.npy")
            debug_print(f"     ğŸ” æœç´¢æ¨¡å¼: {search_pattern}")
            reward_files = glob.glob(search_pattern)
            debug_print(f"     ğŸ“„ æ‰¾åˆ° {len(reward_files)} ä¸ªå¥–åŠ±æ–‡ä»¶")
            
            for reward_file in reward_files:
                try:
                    # ä»ç›®å½•åæå–ç§å­ä¿¡æ¯
                    dir_name = os.path.basename(os.path.dirname(reward_file))
                    # æ ¼å¼: level_seedXXX_procXXX_timestamp
                    parts = dir_name.split('_')
                    seed_part = [p for p in parts if p.startswith('seed')]
                    
                    if seed_part:
                        seed = int(seed_part[0].replace('seed', ''))
                        
                        # åŠ è½½å¥–åŠ±æ•°æ®
                        try:
                            rewards = np.load(reward_file, allow_pickle=True)
                        except Exception as e:
                            debug_print(f"     âŒ å¥–åŠ±æ•°æ®åŠ è½½å¤±è´¥: {e}")
                            continue
                        
                        # æŸ¥æ‰¾å¯¹åº”çš„åä½œåˆ†ææ–‡ä»¶
                        analytics_file = reward_file.replace('rewards_log.npy', 'collaboration_analytics.npy')
                        analytics = None
                        if os.path.exists(analytics_file):
                            try:
                                analytics = np.load(analytics_file, allow_pickle=True).item()
                            except Exception as e:
                                debug_print(f"     âš ï¸ åä½œæ•°æ®åŠ è½½å¤±è´¥: {e}")
                        
                        self.experiment_data[level_dir][seed] = {
                            'rewards': rewards,
                            'analytics': analytics,
                            'file_path': reward_file
                        }
                        
                        total_loaded += 1
                        debug_print(f"     âœ… ç§å­ {seed}: å·²åŠ è½½ ({rewards.shape})")
                        
                except Exception as e:
                    debug_print(f"     âŒ åŠ è½½å¤±è´¥ {reward_file}: {e}")
        
        return total_loaded
        
    def _print_data_summary(self):
        """æ‰“å°æ•°æ®æ‘˜è¦"""
        debug_print("\nğŸ“‹ æ•°æ®æ‘˜è¦:")
        debug_print("-" * 60)
        
        for level, seeds_data in self.experiment_data.items():
            debug_print(f"   {level}: {len(seeds_data)} ä¸ªç§å­")
            if seeds_data:
                example_data = next(iter(seeds_data.values()))
                debug_print(f"     å¥–åŠ±ç»´åº¦: {example_data['rewards'].shape}")
                debug_print(f"     åä½œæ•°æ®: {'æœ‰' if example_data['analytics'] else 'æ— '}")
        
        debug_print("-" * 60)
    
    def calculate_statistics(self):
        """è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡"""
        debug_print("\nğŸ“Š è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡...")
        
        for level, seeds_data in self.experiment_data.items():
            if not seeds_data:
                continue
                
            debug_print(f"   ğŸ” åˆ†æç­‰çº§: {level}")
            
            # æ”¶é›†æ‰€æœ‰ç§å­çš„æ•°æ®
            all_rewards = []
            all_final_performance = []
            all_conflict_rates = []
            
            for seed, data in seeds_data.items():
                rewards = data['rewards']
                
                # è®¡ç®—æ€»å¥–åŠ±
                if rewards.ndim > 1:
                    total_rewards = np.sum(rewards, axis=1)
                else:
                    total_rewards = rewards
                
                all_rewards.append(total_rewards)
                
                # æœ€ç»ˆæ€§èƒ½ï¼ˆæœ€å100è½®çš„å¹³å‡ï¼‰
                final_perf = np.mean(total_rewards[-100:])
                all_final_performance.append(final_perf)
                
                # å†²çªç‡
                if data['analytics'] and 'conflict_rate' in data['analytics']:
                    all_conflict_rates.append(data['analytics']['conflict_rate'])
            
            # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
            stats_dict = {}
            
            # æœ€ç»ˆæ€§èƒ½ç»Ÿè®¡
            if all_final_performance:
                final_array = np.array(all_final_performance)
                stats_dict['final_performance'] = {
                    'mean': np.mean(final_array),
                    'std': np.std(final_array, ddof=1),  # ä½¿ç”¨æ ·æœ¬æ ‡å‡†å·®
                    'ci_95': self._calculate_confidence_interval(final_array),
                    'n': len(final_array)
                }
            
            # å†²çªç‡ç»Ÿè®¡
            if all_conflict_rates:
                conflict_array = np.array(all_conflict_rates)
                stats_dict['conflict_rate'] = {
                    'mean': np.mean(conflict_array),
                    'std': np.std(conflict_array, ddof=1),
                    'ci_95': self._calculate_confidence_interval(conflict_array),
                    'n': len(conflict_array)
                }
            
            # å¥–åŠ±æ›²çº¿ç»Ÿè®¡
            if all_rewards:
                # è®¡ç®—æ¯ä¸ªæ—¶é—´ç‚¹çš„å¹³å‡å€¼å’Œæ ‡å‡†å·®
                min_length = min(len(r) for r in all_rewards)
                truncated_rewards = [r[:min_length] for r in all_rewards]
                rewards_matrix = np.array(truncated_rewards)
                
                stats_dict['reward_curve'] = {
                    'mean': np.mean(rewards_matrix, axis=0),
                    'std': np.std(rewards_matrix, axis=0, ddof=1),
                    'ci_95_upper': np.percentile(rewards_matrix, 97.5, axis=0),
                    'ci_95_lower': np.percentile(rewards_matrix, 2.5, axis=0),
                    'n': len(all_rewards)
                }
            
            self.summary_stats[level] = stats_dict
            
            # æ‰“å°å…³é”®ç»Ÿè®¡ä¿¡æ¯
            if 'final_performance' in stats_dict:
                fp = stats_dict['final_performance']
                debug_print(f"     æœ€ç»ˆæ€§èƒ½: {fp['mean']:.1f} Â± {fp['std']:.1f} (n={fp['n']})")
            
            if 'conflict_rate' in stats_dict:
                cr = stats_dict['conflict_rate']
                debug_print(f"     å†²çªç‡: {cr['mean']:.3f} Â± {cr['std']:.3f} (n={cr['n']})")
    
    def analyze_learning_curves(self):
        """åˆ†æå­¦ä¹ æ›²çº¿ç‰¹å¾"""
        debug_print("\nğŸ“ˆ åˆ†æå­¦ä¹ æ›²çº¿ç‰¹å¾...")
        
        for level, seeds_data in self.experiment_data.items():
            if not seeds_data:
                continue
                
            debug_print(f"   ğŸ” åˆ†æç­‰çº§: {level}")
            
            # æ”¶é›†æ‰€æœ‰ç§å­çš„å¥–åŠ±æ›²çº¿
            all_curves = []
            for seed, data in seeds_data.items():
                rewards = data['rewards']
                if rewards.ndim > 1:
                    total_rewards = np.sum(rewards, axis=1)
                else:
                    total_rewards = rewards
                all_curves.append(total_rewards)
            
            if not all_curves:
                continue
            
            # è®¡ç®—å­¦ä¹ æ›²çº¿æŒ‡æ ‡
            curve_metrics = self._calculate_learning_metrics(all_curves)
            self.learning_analysis[level] = curve_metrics
            
            # æ‰“å°å…³é”®æŒ‡æ ‡
            debug_print(f"     æ”¶æ•›é€Ÿåº¦: {curve_metrics['convergence_episode']:.0f} è½®")
            debug_print(f"     å­¦ä¹ ç¨³å®šæ€§: {curve_metrics['stability_score']:.3f}")
            debug_print(f"     æ”¶æ•›åéœ‡è¡: {curve_metrics['oscillation_level']:.3f}")
    
    def _calculate_learning_metrics(self, reward_curves):
        """è®¡ç®—å­¦ä¹ æ›²çº¿æŒ‡æ ‡"""
        # ç»Ÿä¸€é•¿åº¦
        min_length = min(len(curve) for curve in reward_curves)
        truncated_curves = [curve[:min_length] for curve in reward_curves]
        curves_matrix = np.array(truncated_curves)
        
        # å¹³å‡æ›²çº¿
        mean_curve = np.mean(curves_matrix, axis=0)
        
        # 1. æ”¶æ•›é€Ÿåº¦åˆ†æ (è¾¾åˆ°æœ€ç»ˆæ€§èƒ½90%çš„è½®æ•°)
        final_performance = np.mean(mean_curve[-100:])  # æœ€å100è½®çš„å¹³å‡æ€§èƒ½
        threshold = final_performance * 0.9
        
        # ä½¿ç”¨ç§»åŠ¨å¹³å‡æ¥å¹³æ»‘æ›²çº¿
        window_size = 50
        smoothed_curve = np.convolve(mean_curve, np.ones(window_size)/window_size, mode='valid')
        
        convergence_episode = len(mean_curve)  # é»˜è®¤å€¼
        for i, value in enumerate(smoothed_curve):
            if value >= threshold:
                convergence_episode = i + window_size//2
                break
        
        # 2. å­¦ä¹ ç¨³å®šæ€§ (åæœŸæ–¹å·®çš„å€’æ•°)
        late_phase = mean_curve[len(mean_curve)//2:]  # ååŠæ®µ
        stability_score = 1.0 / (1.0 + np.var(late_phase))
        
        # 3. æ”¶æ•›åéœ‡è¡æ°´å¹³ (æœ€å1/3é˜¶æ®µçš„æ ‡å‡†å·®)
        final_third = curves_matrix[:, -len(mean_curve)//3:]
        oscillation_level = np.mean(np.std(final_third, axis=0))
        
        # 4. åˆå§‹å­¦ä¹ é€Ÿåº¦ (å‰20%é˜¶æ®µçš„æ”¹è¿›å¹…åº¦)
        initial_phase_length = len(mean_curve) // 5
        initial_improvement = mean_curve[initial_phase_length] - mean_curve[0]
        
        # 5. å­¦ä¹ æ•ˆç‡ (å•ä½æ—¶é—´å†…çš„æ€§èƒ½æå‡)
        learning_efficiency = (final_performance - mean_curve[0]) / len(mean_curve)
        
        return {
            'convergence_episode': convergence_episode,
            'stability_score': stability_score,
            'oscillation_level': oscillation_level,
            'initial_improvement': initial_improvement,
            'learning_efficiency': learning_efficiency,
            'final_performance': final_performance,
            'mean_curve': mean_curve
        }
    
    def generate_improvement_suggestions(self):
        """åŸºäºåˆ†æç»“æœç”Ÿæˆæ”¹è¿›å»ºè®®"""
        debug_print("\nğŸ’¡ ç”Ÿæˆæ”¹è¿›å»ºè®®...")
        
        for level, stats in self.summary_stats.items():
            suggestions = []
            
            # åŸºäºå­¦ä¹ æ›²çº¿åˆ†æçš„å»ºè®®
            if level in self.learning_analysis:
                learning = self.learning_analysis[level]
                
                # æ”¶æ•›é€Ÿåº¦å»ºè®®
                if learning['convergence_episode'] > 2000:
                    suggestions.append({
                        'category': 'æ”¶æ•›é€Ÿåº¦',
                        'issue': 'æ”¶æ•›é€Ÿåº¦è¾ƒæ…¢',
                        'suggestion': 'è€ƒè™‘æé«˜å­¦ä¹ ç‡æˆ–ä¼˜åŒ–å¥–åŠ±å‡½æ•°è®¾è®¡',
                        'code_change': 'config.LEARNING_RATE *= 1.5  # æé«˜å­¦ä¹ ç‡'
                    })
                
                # ç¨³å®šæ€§å»ºè®®
                if learning['stability_score'] < 0.7:
                    suggestions.append({
                        'category': 'å­¦ä¹ ç¨³å®šæ€§',
                        'issue': 'å­¦ä¹ è¿‡ç¨‹ä¸å¤Ÿç¨³å®š',
                        'suggestion': 'å‡å°‘å­¦ä¹ ç‡æˆ–å¢åŠ ç»éªŒå›æ”¾ç¼“å†²åŒºå¤§å°',
                        'code_change': 'config.LEARNING_RATE *= 0.8  # é™ä½å­¦ä¹ ç‡\nconfig.BUFFER_SIZE *= 2  # å¢åŠ ç¼“å†²åŒº'
                    })
                
                # éœ‡è¡å»ºè®®
                if learning['oscillation_level'] > 50:
                    suggestions.append({
                        'category': 'è®­ç»ƒéœ‡è¡',
                        'issue': 'è®­ç»ƒåæœŸéœ‡è¡è¾ƒå¤§',
                        'suggestion': 'å®æ–½å­¦ä¹ ç‡è¡°å‡æˆ–ç›®æ ‡ç½‘ç»œè½¯æ›´æ–°',
                        'code_change': 'config.LR_DECAY = 0.995  # å­¦ä¹ ç‡è¡°å‡\nconfig.TAU = 0.01  # è½¯æ›´æ–°å‚æ•°'
                    })
            
            # åŸºäºæ€§èƒ½å¯¹æ¯”çš„å»ºè®®
            if 'final_performance' in stats:
                fp = stats['final_performance']
                
                # å¦‚æœæœ‰åŸºå‡†æ¯”è¾ƒ
                if RewardExperimentConfig.BASIC in self.summary_stats:
                    basic_perf = self.summary_stats[RewardExperimentConfig.BASIC]['final_performance']['mean']
                    improvement = fp['mean'] - basic_perf
                    
                    if improvement < 0:
                        suggestions.append({
                            'category': 'æ€§èƒ½å›é€€',
                            'issue': f'ç›¸æ¯”åŸºå‡†æ€§èƒ½ä¸‹é™äº† {abs(improvement):.1f}',
                            'suggestion': 'æ£€æŸ¥å¥–åŠ±å‡½æ•°æƒé‡é…ç½®ï¼Œå¯èƒ½æŸäº›å¥–åŠ±ç»„ä»¶äº§ç”Ÿè´Ÿé¢å½±å“',
                            'code_change': f'# æ£€æŸ¥ {level} é…ç½®ä¸­çš„å¥–åŠ±æƒé‡\n# è€ƒè™‘é™ä½æˆ–ç§»é™¤è¡¨ç°ä¸ä½³çš„å¥–åŠ±ç»„ä»¶'
                        })
                    elif improvement < 50:
                        suggestions.append({
                            'category': 'å¾®å¼±æ”¹è¿›',
                            'issue': f'ç›¸æ¯”åŸºå‡†ä»…æå‡ {improvement:.1f}',
                            'suggestion': 'å¢åŠ è¯¥ç­‰çº§ç‰¹æœ‰å¥–åŠ±ç»„ä»¶çš„æƒé‡',
                            'code_change': f'# åœ¨ {level} é…ç½®ä¸­å¢åŠ å…³é”®å¥–åŠ±æƒé‡\n# ä¾‹å¦‚: COLLABORATION_REWARD_WEIGHT *= 1.5'
                        })
            
            # åŸºäºå†²çªç‡çš„å»ºè®®
            if 'conflict_rate' in stats:
                cr = stats['conflict_rate']
                if cr['mean'] > 0.3:
                    suggestions.append({
                        'category': 'åä½œä¼˜åŒ–',
                        'issue': f'å†²çªç‡è¿‡é«˜ ({cr["mean"]:.3f})',
                        'suggestion': 'å¢å¼ºåä½œå¥–åŠ±æœºåˆ¶æˆ–æ”¹è¿›å†²çªæ£€æµ‹ç®—æ³•',
                        'code_change': 'config.CONFLICT_PENALTY_WEIGHT *= 2  # å¢åŠ å†²çªæƒ©ç½š\nconfig.COLLABORATION_BONUS_WEIGHT *= 1.5  # å¢åŠ åä½œå¥–åŠ±'
                    })
            
            self.improvement_suggestions[level] = suggestions
            
            if suggestions:
                debug_print(f"   ğŸ¯ {level}: ç”Ÿæˆäº† {len(suggestions)} æ¡æ”¹è¿›å»ºè®®")
    
    def _calculate_confidence_interval(self, data, confidence=0.95):
        """è®¡ç®—ç½®ä¿¡åŒºé—´"""
        n = len(data)
        if n < 2:
            return (np.mean(data), np.mean(data))
        
        mean = np.mean(data)
        se = stats.sem(data)  # æ ‡å‡†è¯¯å·®
        t_value = stats.t.ppf((1 + confidence) / 2, df=n-1)
        margin_error = t_value * se
        
        return (mean - margin_error, mean + margin_error)
    
    def generate_statistical_report(self, output_file="multi_seed_report.txt"):
        """ç”Ÿæˆç»Ÿè®¡åˆ†ææŠ¥å‘Š"""
        debug_print(f"\nğŸ“ ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š: {output_file}")
        
        report_lines = []
        report_lines.append("=" * 80)
        report_lines.append("å¤šç§å­å¢é‡å¼å¥–åŠ±å®éªŒç»Ÿè®¡åˆ†ææŠ¥å‘Š")
        report_lines.append("=" * 80)
        report_lines.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append(f"ç»“æœç›®å½•: {self.results_dir}")
        report_lines.append("")
        
        # å®éªŒæ¦‚è¿°
        report_lines.append("ğŸ“Š å®éªŒæ¦‚è¿°")
        report_lines.append("-" * 60)
        total_experiments = sum(len(seeds) for seeds in self.experiment_data.values())
        report_lines.append(f"å®éªŒç­‰çº§æ•°: {len(self.experiment_data)}")
        report_lines.append(f"æ€»å®éªŒæ¬¡æ•°: {total_experiments}")
        report_lines.append("")
        
        # å„ç­‰çº§è¯¦ç»†åˆ†æ
        experiment_order = [
            RewardExperimentConfig.BASIC,
            RewardExperimentConfig.LOAD_EFFICIENCY,
            RewardExperimentConfig.ROLE_SPECIALIZATION,
            RewardExperimentConfig.COLLABORATION,
            RewardExperimentConfig.BEHAVIOR_SHAPING,
            RewardExperimentConfig.FULL
        ]
        
        performance_data = {}  # ç”¨äºå¢é‡æ•ˆæœåˆ†æ
        
        for level in experiment_order:
            if level not in self.summary_stats:
                continue
                
            stats = self.summary_stats[level]
            config = EXPERIMENT_CONFIGS[level]
            
            report_lines.append(f"ğŸ§ª å®éªŒç­‰çº§: {level.upper()}")
            report_lines.append("-" * 60)
            report_lines.append(f"æè¿°: {config.get_experiment_description()}")
            report_lines.append(f"ç§å­æ•°é‡: {len(self.experiment_data[level])}")
            report_lines.append("")
            
            # æ€§èƒ½æŒ‡æ ‡
            if 'final_performance' in stats:
                fp = stats['final_performance']
                ci_lower, ci_upper = fp['ci_95']
                report_lines.append("ğŸ“ˆ æœ€ç»ˆæ€§èƒ½åˆ†æ:")
                report_lines.append(f"   å¹³å‡å€¼: {fp['mean']:.2f}")
                report_lines.append(f"   æ ‡å‡†å·®: {fp['std']:.2f}")
                report_lines.append(f"   95%ç½®ä¿¡åŒºé—´: [{ci_lower:.2f}, {ci_upper:.2f}]")
                report_lines.append(f"   æ ·æœ¬æ•°: {fp['n']}")
                
                performance_data[level] = fp
            
            # åä½œæŒ‡æ ‡
            if 'conflict_rate' in stats:
                cr = stats['conflict_rate']
                ci_lower, ci_upper = cr['ci_95']
                report_lines.append("")
                report_lines.append("ğŸ¤ åä½œåˆ†æ:")
                report_lines.append(f"   å†²çªç‡å‡å€¼: {cr['mean']:.4f}")
                report_lines.append(f"   å†²çªç‡æ ‡å‡†å·®: {cr['std']:.4f}")
                report_lines.append(f"   95%ç½®ä¿¡åŒºé—´: [{ci_lower:.4f}, {ci_upper:.4f}]")
                report_lines.append(f"   æ ·æœ¬æ•°: {cr['n']}")
            
            report_lines.append("")
            report_lines.append("")
        
        # å¢é‡æ•ˆæœåˆ†æ
        if len(performance_data) > 1:
            report_lines.append("ğŸ“Š å¢é‡æ•ˆæœåˆ†æ")
            report_lines.append("-" * 60)
            
            prev_level = None
            prev_data = None
            
            for level in experiment_order:
                if level not in performance_data:
                    continue
                    
                current_data = performance_data[level]
                
                if prev_data is not None:
                    # è®¡ç®—æ€§èƒ½æå‡
                    improvement = current_data['mean'] - prev_data['mean']
                    improvement_pct = (improvement / abs(prev_data['mean'])) * 100
                    
                    # ç®€å•çš„ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒï¼ˆåŸºäºç½®ä¿¡åŒºé—´æ˜¯å¦é‡å ï¼‰
                    prev_ci_upper = prev_data['ci_95'][1]
                    current_ci_lower = current_data['ci_95'][0]
                    is_significant = current_ci_lower > prev_ci_upper
                    
                    significance_text = "æ˜¾è‘—" if is_significant else "ä¸æ˜¾è‘—"
                    
                    report_lines.append(f"{prev_level.upper()} â†’ {level.upper()}:")
                    report_lines.append(f"   æ€§èƒ½å˜åŒ–: {improvement:+.2f} ({improvement_pct:+.1f}%)")
                    report_lines.append(f"   ç»Ÿè®¡æ˜¾è‘—æ€§: {significance_text}")
                    report_lines.append("")
                
                prev_level = level
                prev_data = current_data
        
        # å­¦ä¹ æ›²çº¿åˆ†æ
        if self.learning_analysis:
            report_lines.append("ğŸ“ˆ å­¦ä¹ æ›²çº¿åˆ†æ")
            report_lines.append("-" * 60)
            
            for level in experiment_order:
                if level not in self.learning_analysis:
                    continue
                    
                learning = self.learning_analysis[level]
                report_lines.append(f"ğŸ” {level.upper()} å­¦ä¹ ç‰¹å¾:")
                report_lines.append(f"   æ”¶æ•›é€Ÿåº¦: {learning['convergence_episode']:.0f} è½®")
                report_lines.append(f"   å­¦ä¹ ç¨³å®šæ€§: {learning['stability_score']:.3f}")
                report_lines.append(f"   éœ‡è¡æ°´å¹³: {learning['oscillation_level']:.3f}")
                report_lines.append(f"   åˆå§‹æ”¹è¿›: {learning['initial_improvement']:.2f}")
                report_lines.append(f"   å­¦ä¹ æ•ˆç‡: {learning['learning_efficiency']:.4f}")
                report_lines.append("")
        
        # å…·ä½“æ”¹è¿›å»ºè®®
        if self.improvement_suggestions:
            report_lines.append("ğŸ› ï¸ å…·ä½“æ”¹è¿›å»ºè®®")
            report_lines.append("-" * 60)
            
            for level in experiment_order:
                if level not in self.improvement_suggestions:
                    continue
                    
                suggestions = self.improvement_suggestions[level]
                if not suggestions:
                    continue
                    
                report_lines.append(f"ğŸ¯ {level.upper()} æ”¹è¿›å»ºè®®:")
                
                for i, suggestion in enumerate(suggestions, 1):
                    report_lines.append(f"   {i}. {suggestion['category']}: {suggestion['issue']}")
                    report_lines.append(f"      å»ºè®®: {suggestion['suggestion']}")
                    report_lines.append(f"      ä»£ç ä¿®æ”¹: {suggestion['code_change']}")
                    report_lines.append("")
        
        # ç»“è®ºå’Œå»ºè®®
        report_lines.append("ğŸ’¡ ç»“è®ºå’Œå»ºè®®")
        report_lines.append("-" * 60)
        
        if performance_data:
            best_level = max(performance_data.keys(), key=lambda k: performance_data[k]['mean'])
            best_perf = performance_data[best_level]
            
            report_lines.append(f"ğŸ† æœ€ä½³è¡¨ç°ç­‰çº§: {best_level.upper()}")
            report_lines.append(f"   æœ€ç»ˆæ€§èƒ½: {best_perf['mean']:.2f} Â± {best_perf['std']:.2f}")
            
            if RewardExperimentConfig.BASIC in performance_data:
                basic_perf = performance_data[RewardExperimentConfig.BASIC]['mean']
                total_improvement = best_perf['mean'] - basic_perf
                total_improvement_pct = (total_improvement / abs(basic_perf)) * 100
                
                report_lines.append("")
                report_lines.append(f"ğŸš€ æ€»ä½“æå‡æ•ˆæœ:")
                report_lines.append(f"   ç›¸æ¯”BASICç­‰çº§æå‡: {total_improvement:+.2f} ({total_improvement_pct:+.1f}%)")
        
        # å­¦ä¹ æ›²çº¿æ´å¯Ÿ
        if self.learning_analysis:
            report_lines.append("")
            report_lines.append("ğŸ“Š å­¦ä¹ æ›²çº¿æ´å¯Ÿ:")
            
            # æ‰¾åˆ°æ”¶æ•›æœ€å¿«çš„ç­‰çº§
            fastest_convergence = min(self.learning_analysis.items(), 
                                    key=lambda x: x[1]['convergence_episode'])
            report_lines.append(f"   æœ€å¿«æ”¶æ•›: {fastest_convergence[0]} ({fastest_convergence[1]['convergence_episode']:.0f} è½®)")
            
            # æ‰¾åˆ°æœ€ç¨³å®šçš„ç­‰çº§
            most_stable = max(self.learning_analysis.items(), 
                            key=lambda x: x[1]['stability_score'])
            report_lines.append(f"   æœ€ç¨³å®šå­¦ä¹ : {most_stable[0]} (ç¨³å®šæ€§: {most_stable[1]['stability_score']:.3f})")
            
            # æ‰¾åˆ°å­¦ä¹ æ•ˆç‡æœ€é«˜çš„ç­‰çº§
            most_efficient = max(self.learning_analysis.items(), 
                               key=lambda x: x[1]['learning_efficiency'])
            report_lines.append(f"   æœ€é«˜æ•ˆç‡: {most_efficient[0]} (æ•ˆç‡: {most_efficient[1]['learning_efficiency']:.4f})")
        
        report_lines.append("")
        report_lines.append("ğŸ“‹ ç»¼åˆå»ºè®®:")
        report_lines.append("   1. å¯¹äºæ˜¾è‘—æ”¹è¿›çš„æ¨¡å—ï¼Œå¯è¿›ä¸€æ­¥è°ƒä¼˜å‚æ•°")
        report_lines.append("   2. å¯¹äºæ€§èƒ½ä¸‹é™çš„æ¨¡å—ï¼Œéœ€è¦æ£€æŸ¥å®ç°æˆ–è°ƒæ•´æƒé‡")
        report_lines.append("   3. è€ƒè™‘ç»„åˆæœ€æœ‰æ•ˆçš„å¥–åŠ±æ¨¡å—æ„å»ºæœ€ä¼˜ç­–ç•¥")
        report_lines.append("   4. å‚è€ƒæ”¶æ•›æœ€å¿«çš„é…ç½®æ¥ä¼˜åŒ–å­¦ä¹ ç‡è®¾ç½®")
        report_lines.append("   5. é‡‡ç”¨æœ€ç¨³å®šé…ç½®çš„å‚æ•°æ¥å‡å°‘è®­ç»ƒæ³¢åŠ¨")
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = os.path.join(self.results_dir, output_file)
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_lines))
        
        debug_print(f"ğŸ“Š ç»Ÿè®¡æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        
        # ä¹Ÿæ‰“å°å…³é”®ç»“è®º
        debug_print("\nğŸ’¡ å…³é”®å‘ç°:")
        if performance_data:
            for level, data in performance_data.items():
                ci_lower, ci_upper = data['ci_95']
                debug_print(f"   {level}: {data['mean']:.1f} Â± {data['std']:.1f} [{ci_lower:.1f}, {ci_upper:.1f}]")
    
    def plot_multi_seed_comparison(self, output_file="multi_seed_comparison.png"):
        """ç»˜åˆ¶å¤šç§å­å¯¹æ¯”å›¾è¡¨"""
        debug_print(f"\nğŸ“Š ç”Ÿæˆå¤šç§å­å¯¹æ¯”å›¾è¡¨: {output_file}")
        
        # è®¾ç½®å›¾è¡¨æ ·å¼
        plt.style.use('seaborn-v0_8')
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        
        # æ ¹æ®å­—ä½“æ”¯æŒé€‰æ‹©æ ‡é¢˜
        if USE_CHINESE:
            main_title = 'å¤šç§å­å¢é‡å¼å¥–åŠ±å®éªŒå¯¹æ¯”åˆ†æ'
            curve_title = 'è®­ç»ƒå¥–åŠ±æ›²çº¿å¯¹æ¯” (95%ç½®ä¿¡åŒºé—´)'
            performance_title = 'æœ€ç»ˆæ€§èƒ½åˆ†å¸ƒ (æœ€å100è½®å¹³å‡)'
            improvement_title = 'ç›¸å¯¹åŸºå‡†çš„æ€§èƒ½æå‡'
            conflict_title = 'å†²çªç‡å¯¹æ¯”'
            xlabel_episodes = 'è®­ç»ƒå›åˆ'
            ylabel_reward = 'æ€»å¥–åŠ±'
            ylabel_avg_reward = 'å¹³å‡æ€»å¥–åŠ±'
            ylabel_improvement = 'æ€§èƒ½æå‡'
            ylabel_conflict = 'å¹³å‡å†²çªç‡'
        else:
            main_title = 'Multi-Seed Incremental Reward Experiment Comparison'
            curve_title = 'Training Reward Curves (95% CI)'
            performance_title = 'Final Performance Distribution (Last 100 Episodes Avg)'
            improvement_title = 'Performance Improvement vs Baseline'
            conflict_title = 'Conflict Rate Comparison'
            xlabel_episodes = 'Episodes'
            ylabel_reward = 'Total Reward'
            ylabel_avg_reward = 'Average Total Reward'
            ylabel_improvement = 'Performance Improvement'
            ylabel_conflict = 'Average Conflict Rate'
        
        fig.suptitle(main_title, fontsize=16, fontweight='bold')
        
        # 1. å¸¦ç½®ä¿¡åŒºé—´çš„å¥–åŠ±æ›²çº¿å¯¹æ¯”
        ax1 = axes[0, 0]
        colors = plt.cm.Set1(np.linspace(0, 1, len(self.summary_stats)))
        
        for i, (level, stats) in enumerate(self.summary_stats.items()):
            if 'reward_curve' in stats:
                curve_stats = stats['reward_curve']
                x = np.arange(len(curve_stats['mean']))
                
                # ç»˜åˆ¶å¹³å‡çº¿
                ax1.plot(x, curve_stats['mean'], color=colors[i], 
                        label=f"{level} (n={curve_stats['n']})", linewidth=2)
                
                # ç»˜åˆ¶ç½®ä¿¡åŒºé—´
                ax1.fill_between(x, curve_stats['ci_95_lower'], curve_stats['ci_95_upper'],
                               color=colors[i], alpha=0.2)
        
        ax1.set_title(curve_title)
        ax1.set_xlabel(xlabel_episodes)
        ax1.set_ylabel(ylabel_reward)
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. æœ€ç»ˆæ€§èƒ½ç®±çº¿å›¾
        ax2 = axes[0, 1]
        final_performances = []
        level_names = []
        
        for level, seeds_data in self.experiment_data.items():
            level_performances = []
            for seed, data in seeds_data.items():
                rewards = data['rewards']
                if rewards.ndim > 1:
                    total_rewards = np.sum(rewards, axis=1)
                else:
                    total_rewards = rewards
                final_perf = np.mean(total_rewards[-100:])
                level_performances.append(final_perf)
            
            if level_performances:
                final_performances.append(level_performances)
                level_names.append(level)
        
        if final_performances:
            box_plot = ax2.boxplot(final_performances, labels=level_names, patch_artist=True)
            
            # è®¾ç½®é¢œè‰²
            for patch, color in zip(box_plot['boxes'], colors):
                patch.set_facecolor(color)
                patch.set_alpha(0.7)
            
            ax2.set_title(performance_title)
            ax2.set_ylabel(ylabel_avg_reward)
            ax2.tick_params(axis='x', rotation=45)
        
        # 3. æ€§èƒ½æå‡å¯¹æ¯”
        ax3 = axes[1, 0]
        if len(self.summary_stats) > 1:
            levels = list(self.summary_stats.keys())
            improvements = []
            improvement_errors = []
            
            # ä»¥ç¬¬ä¸€ä¸ªç­‰çº§ä¸ºåŸºå‡†
            baseline_stats = list(self.summary_stats.values())[0]
            if 'final_performance' in baseline_stats:
                baseline_mean = baseline_stats['final_performance']['mean']
                
                for stats in self.summary_stats.values():
                    if 'final_performance' in stats:
                        current_mean = stats['final_performance']['mean']
                        current_std = stats['final_performance']['std']
                        improvement = current_mean - baseline_mean
                        improvements.append(improvement)
                        improvement_errors.append(current_std)
                
                bars = ax3.bar(range(len(levels)), improvements, yerr=improvement_errors,
                             capsize=5, color=colors[:len(levels)], alpha=0.7)
                ax3.set_title(improvement_title)
                ax3.set_ylabel(ylabel_improvement)
                ax3.set_xticks(range(len(levels)))
                ax3.set_xticklabels(levels, rotation=45)
                ax3.axhline(y=0, color='black', linestyle='--', alpha=0.5)
                ax3.grid(True, alpha=0.3)
        
        # 4. å†²çªç‡å¯¹æ¯”
        ax4 = axes[1, 1]
        conflict_rates = []
        conflict_errors = []
        conflict_levels = []
        
        for level, stats in self.summary_stats.items():
            if 'conflict_rate' in stats:
                cr_stats = stats['conflict_rate']
                conflict_rates.append(cr_stats['mean'])
                conflict_errors.append(cr_stats['std'])
                conflict_levels.append(level)
        
        if conflict_rates:
            bars = ax4.bar(range(len(conflict_levels)), conflict_rates, 
                          yerr=conflict_errors, capsize=5, 
                          color=colors[:len(conflict_levels)], alpha=0.7)
            ax4.set_title(conflict_title)
            ax4.set_ylabel(ylabel_conflict)
            ax4.set_xticks(range(len(conflict_levels)))
            ax4.set_xticklabels(conflict_levels, rotation=45)
            ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        output_path = os.path.join(self.results_dir, output_file)
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        debug_print(f"ğŸ“Š å¤šç§å­å¯¹æ¯”å›¾è¡¨å·²ä¿å­˜: {output_path}")
    
    def plot_enhanced_analysis(self, output_file="enhanced_analysis.png"):
        """ç»˜åˆ¶å¢å¼ºåˆ†æå›¾è¡¨"""
        debug_print(f"\nğŸ“Š ç”Ÿæˆå¢å¼ºåˆ†æå›¾è¡¨: {output_file}")
        
        if not self.learning_analysis:
            debug_print("âŒ æ²¡æœ‰å­¦ä¹ æ›²çº¿åˆ†ææ•°æ®")
            return
        
        # è®¾ç½®å›¾è¡¨æ ·å¼
        plt.style.use('seaborn-v0_8')
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        
        # æ ¹æ®å­—ä½“æ”¯æŒé€‰æ‹©æ ‡é¢˜
        if USE_CHINESE:
            main_title = 'å¢å¼ºç‰ˆå­¦ä¹ æ›²çº¿åˆ†æ'
            convergence_title = 'æ”¶æ•›é€Ÿåº¦å¯¹æ¯”'
            stability_title = 'å­¦ä¹ ç¨³å®šæ€§å¯¹æ¯”'
            oscillation_title = 'éœ‡è¡æ°´å¹³å¯¹æ¯”'
            efficiency_title = 'å­¦ä¹ æ•ˆç‡å¯¹æ¯”'
            improvement_title = 'åˆå§‹æ”¹è¿›å¹…åº¦å¯¹æ¯”'
            curve_comparison_title = 'æ ‡å‡†åŒ–å­¦ä¹ æ›²çº¿å¯¹æ¯”'
        else:
            main_title = 'Enhanced Learning Curve Analysis'
            convergence_title = 'Convergence Speed Comparison'
            stability_title = 'Learning Stability Comparison'
            oscillation_title = 'Oscillation Level Comparison'
            efficiency_title = 'Learning Efficiency Comparison'
            improvement_title = 'Initial Improvement Comparison'
            curve_comparison_title = 'Normalized Learning Curves'
        
        fig.suptitle(main_title, fontsize=16, fontweight='bold')
        
        levels = list(self.learning_analysis.keys())
        colors = plt.cm.Set1(np.linspace(0, 1, len(levels)))
        
        # 1. æ”¶æ•›é€Ÿåº¦å¯¹æ¯”
        ax1 = axes[0, 0]
        convergence_speeds = [self.learning_analysis[level]['convergence_episode'] for level in levels]
        bars1 = ax1.bar(range(len(levels)), convergence_speeds, color=colors, alpha=0.7)
        ax1.set_title(convergence_title)
        ax1.set_ylabel('æ”¶æ•›è½®æ•°' if USE_CHINESE else 'Convergence Episodes')
        ax1.set_xticks(range(len(levels)))
        ax1.set_xticklabels(levels, rotation=45)
        ax1.grid(True, alpha=0.3)
        
        # åœ¨æŸ±å­ä¸Šæ˜¾ç¤ºæ•°å€¼
        for bar, value in zip(bars1, convergence_speeds):
            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(convergence_speeds)*0.01,
                    f'{value:.0f}', ha='center', va='bottom')
        
        # 2. å­¦ä¹ ç¨³å®šæ€§å¯¹æ¯”
        ax2 = axes[0, 1]
        stability_scores = [self.learning_analysis[level]['stability_score'] for level in levels]
        bars2 = ax2.bar(range(len(levels)), stability_scores, color=colors, alpha=0.7)
        ax2.set_title(stability_title)
        ax2.set_ylabel('ç¨³å®šæ€§åˆ†æ•°' if USE_CHINESE else 'Stability Score')
        ax2.set_xticks(range(len(levels)))
        ax2.set_xticklabels(levels, rotation=45)
        ax2.grid(True, alpha=0.3)
        
        # 3. éœ‡è¡æ°´å¹³å¯¹æ¯”
        ax3 = axes[0, 2]
        oscillation_levels = [self.learning_analysis[level]['oscillation_level'] for level in levels]
        bars3 = ax3.bar(range(len(levels)), oscillation_levels, color=colors, alpha=0.7)
        ax3.set_title(oscillation_title)
        ax3.set_ylabel('éœ‡è¡æ°´å¹³' if USE_CHINESE else 'Oscillation Level')
        ax3.set_xticks(range(len(levels)))
        ax3.set_xticklabels(levels, rotation=45)
        ax3.grid(True, alpha=0.3)
        
        # 4. å­¦ä¹ æ•ˆç‡å¯¹æ¯”
        ax4 = axes[1, 0]
        learning_efficiencies = [self.learning_analysis[level]['learning_efficiency'] for level in levels]
        bars4 = ax4.bar(range(len(levels)), learning_efficiencies, color=colors, alpha=0.7)
        ax4.set_title(efficiency_title)
        ax4.set_ylabel('å­¦ä¹ æ•ˆç‡' if USE_CHINESE else 'Learning Efficiency')
        ax4.set_xticks(range(len(levels)))
        ax4.set_xticklabels(levels, rotation=45)
        ax4.grid(True, alpha=0.3)
        
        # 5. åˆå§‹æ”¹è¿›å¹…åº¦å¯¹æ¯”
        ax5 = axes[1, 1]
        initial_improvements = [self.learning_analysis[level]['initial_improvement'] for level in levels]
        bars5 = ax5.bar(range(len(levels)), initial_improvements, color=colors, alpha=0.7)
        ax5.set_title(improvement_title)
        ax5.set_ylabel('åˆå§‹æ”¹è¿›' if USE_CHINESE else 'Initial Improvement')
        ax5.set_xticks(range(len(levels)))
        ax5.set_xticklabels(levels, rotation=45)
        ax5.grid(True, alpha=0.3)
        
        # 6. æ ‡å‡†åŒ–å­¦ä¹ æ›²çº¿å¯¹æ¯”
        ax6 = axes[1, 2]
        for i, level in enumerate(levels):
            curve = self.learning_analysis[level]['mean_curve']
            # æ ‡å‡†åŒ–åˆ°0-1èŒƒå›´
            normalized_curve = (curve - curve.min()) / (curve.max() - curve.min())
            x = np.arange(len(normalized_curve))
            ax6.plot(x, normalized_curve, color=colors[i], label=level, linewidth=2)
        
        ax6.set_title(curve_comparison_title)
        ax6.set_xlabel('è®­ç»ƒè½®æ•°' if USE_CHINESE else 'Training Episodes')
        ax6.set_ylabel('æ ‡å‡†åŒ–å¥–åŠ±' if USE_CHINESE else 'Normalized Reward')
        ax6.legend()
        ax6.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        output_path = os.path.join(self.results_dir, output_file)
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        debug_print(f"ğŸ“Š å¢å¼ºåˆ†æå›¾è¡¨å·²ä¿å­˜: {output_path}")
    
    def save_improvement_suggestions(self, output_file="improvement_suggestions.json"):
        """ä¿å­˜æ”¹è¿›å»ºè®®ä¸ºJSONæ–‡ä»¶"""
        debug_print(f"\nğŸ’¾ ä¿å­˜æ”¹è¿›å»ºè®®: {output_file}")
        
        suggestions_data = {
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'results_directory': self.results_dir,
            'total_levels_analyzed': len(self.improvement_suggestions),
            'suggestions_by_level': self.improvement_suggestions
        }
        
        # æ·»åŠ æ±‡æ€»ç»Ÿè®¡
        total_suggestions = sum(len(suggestions) for suggestions in self.improvement_suggestions.values())
        categories = {}
        for suggestions in self.improvement_suggestions.values():
            for suggestion in suggestions:
                category = suggestion['category']
                categories[category] = categories.get(category, 0) + 1
        
        suggestions_data['summary'] = {
            'total_suggestions': total_suggestions,
            'suggestions_by_category': categories
        }
        
        # ä¿å­˜æ–‡ä»¶
        output_path = os.path.join(self.results_dir, output_file)
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(suggestions_data, f, ensure_ascii=False, indent=2)
        
        debug_print(f"ğŸ’¾ æ”¹è¿›å»ºè®®å·²ä¿å­˜: {output_path}")
        debug_print(f"   æ€»è®¡ {total_suggestions} æ¡å»ºè®®ï¼Œæ¶µç›– {len(categories)} ä¸ªç±»åˆ«")
    
    def run_complete_analysis(self):
        """è¿è¡Œå®Œæ•´çš„å¤šç§å­åˆ†æ"""
        debug_print("ğŸš€ å¼€å§‹å®Œæ•´çš„å¤šç§å­å®éªŒåˆ†æ")
        debug_print("=" * 80)
        
        # åŠ è½½æ•°æ®
        self.load_all_results()
        
        if not self.experiment_data:
            debug_print("âŒ æ²¡æœ‰æ‰¾åˆ°å®éªŒæ•°æ®ï¼Œè¯·ç¡®ä¿å…ˆè¿è¡Œæ‰¹é‡å®éªŒ")
            return
        
        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        self.calculate_statistics()
        
        # åˆ†æå­¦ä¹ æ›²çº¿
        self.analyze_learning_curves()
        
        # ç”Ÿæˆæ”¹è¿›å»ºè®®
        self.generate_improvement_suggestions()
        
        # ç”ŸæˆæŠ¥å‘Š
        self.generate_statistical_report()
        
        # ç”Ÿæˆå›¾è¡¨
        self.plot_multi_seed_comparison()
        
        # ç”Ÿæˆå¢å¼ºç‰ˆå¯è§†åŒ–å›¾è¡¨
        self.plot_enhanced_analysis()
        
        # ä¿å­˜æ”¹è¿›å»ºè®®
        self.save_improvement_suggestions()
        
        debug_print("\nğŸ‰ å¢å¼ºç‰ˆå¤šç§å­åˆ†æå®Œæˆï¼")
        debug_print(f"ğŸ“ æŸ¥çœ‹ç»“æœç›®å½•: {self.results_dir}")
        debug_print("ğŸ“Š ä¸»è¦è¾“å‡ºæ–‡ä»¶:")
        debug_print("   - multi_seed_report.txt: è¯¦ç»†ç»Ÿè®¡æŠ¥å‘Šï¼ˆåŒ…å«æ”¹è¿›å»ºè®®ï¼‰")
        debug_print("   - multi_seed_comparison.png: åŸºç¡€å¯¹æ¯”å›¾è¡¨")
        debug_print("   - enhanced_analysis.png: å¢å¼ºåˆ†æå›¾è¡¨ï¼ˆå­¦ä¹ æ›²çº¿ç‰¹å¾ï¼‰")
        debug_print("   - improvement_suggestions.json: ç»“æ„åŒ–æ”¹è¿›å»ºè®®")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="å¤šç§å­å®éªŒç»“æœåˆ†æ")
    parser.add_argument("--results_dir", default="multi_seed_results",
                       help="å®éªŒç»“æœç›®å½•è·¯å¾„")
    
    args = parser.parse_args()
    
    analyzer = MultiSeedAnalyzer(args.results_dir)
    analyzer.run_complete_analysis()

if __name__ == "__main__":
    main()
