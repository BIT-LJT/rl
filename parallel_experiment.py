#!/usr/bin/env python3
"""
å¹¶è¡Œå®éªŒè„šæœ¬ - åŒæ—¶è¿è¡Œ6ä¸ªå®éªŒç­‰çº§
æ”¯æŒå¤šè¿›ç¨‹å¹¶è¡Œæ‰§è¡Œï¼Œæ¯ä¸ªç­‰çº§ç‹¬ç«‹è¿è¡Œï¼Œé¿å…ç›¸äº’å¹²æ‰°

ä½¿ç”¨æ–¹æ³•ï¼š
python parallel_experiment.py --seeds 42,123,2024 --episodes 3000
"""

import os
import subprocess
import multiprocessing as mp
import time
import json
import shutil
from datetime import datetime, timedelta
from pathlib import Path
import argparse
import psutil

from reward_config import RewardExperimentConfig
from utils import debug_print

class ParallelExperimentRunner:
    """å¹¶è¡Œå®éªŒè¿è¡Œå™¨"""
    
    def __init__(self, 
                 seeds=[42, 123, 2024, 888, 1337],
                 episodes_per_experiment=3000,
                 results_dir="parallel_experiment_results",
                 gpu_memory_limit=0.85):
        """
        åˆå§‹åŒ–å¹¶è¡Œå®éªŒè¿è¡Œå™¨
        
        Args:
            seeds: éšæœºç§å­åˆ—è¡¨
            episodes_per_experiment: æ¯ä¸ªå®éªŒçš„è®­ç»ƒå›åˆæ•°
            results_dir: ç»“æœå­˜å‚¨ç›®å½•
            gpu_memory_limit: GPUæ˜¾å­˜ä½¿ç”¨é™åˆ¶ï¼ˆ0-1ä¹‹é—´ï¼‰
        """
        self.seeds = seeds
        self.episodes_per_experiment = episodes_per_experiment
        self.results_dir = results_dir
        self.gpu_memory_limit = gpu_memory_limit
        
        # æ‰€æœ‰å®éªŒç­‰çº§
        self.experiment_levels = [
            RewardExperimentConfig.BASIC,
            RewardExperimentConfig.LOAD_EFFICIENCY,
            RewardExperimentConfig.ROLE_SPECIALIZATION,
            RewardExperimentConfig.COLLABORATION,
            RewardExperimentConfig.BEHAVIOR_SHAPING,
            RewardExperimentConfig.FULL
        ]
        
        # åˆ›å»ºç»“æœç›®å½•å’Œä¸´æ—¶æ–‡ä»¶ç›®å½•
        os.makedirs(results_dir, exist_ok=True)
        self.temp_dir = os.path.join(results_dir, "temp_files")
        os.makedirs(self.temp_dir, exist_ok=True)
        
        # çŠ¶æ€è·Ÿè¸ª
        self.start_time = None
        self.process_status = {}
        
    def run_single_level_all_seeds(self, level):
        """
        è¿è¡Œå•ä¸ªç­‰çº§çš„æ‰€æœ‰ç§å­å®éªŒ
        
        Args:
            level: å®éªŒç­‰çº§
            
        Returns:
            dict: è¿è¡Œç»“æœç»Ÿè®¡
        """
        process_id = os.getpid()
        debug_print(f"ğŸš€ è¿›ç¨‹ {process_id}: å¼€å§‹è¿è¡Œ {level.upper()} ç­‰çº§å®éªŒ")
        
        results = {
            'level': level,
            'process_id': process_id,
            'start_time': datetime.now().isoformat(),
            'seeds_results': [],
            'total_success': 0,
            'total_failed': 0
        }
        
        for seed in self.seeds:
            debug_print(f"   ğŸ² è¿›ç¨‹ {process_id}: è¿è¡Œç§å­ {seed}")
            
            try:
                # åˆ›å»ºç‹¬ç«‹çš„å®éªŒç›®å½•
                exp_dir = self._prepare_experiment_directory(level, seed, process_id)
                
                # ç”Ÿæˆé…ç½®æ–‡ä»¶ï¼ˆæ¯ä¸ªè¿›ç¨‹ä½¿ç”¨ç‹¬ç«‹çš„é…ç½®æ–‡ä»¶ï¼‰
                config_file = os.path.join(self.temp_dir, f"config_{level}_{process_id}.py")
                self._create_config_file(level, seed, exp_dir, config_file)
                
                # è¿è¡Œå®éªŒ
                success = self._run_experiment(config_file, process_id, level, seed)
                
                seed_result = {
                    'seed': seed,
                    'success': success,
                    'experiment_dir': exp_dir,
                    'timestamp': datetime.now().isoformat()
                }
                
                results['seeds_results'].append(seed_result)
                
                if success:
                    results['total_success'] += 1
                    debug_print(f"   âœ… è¿›ç¨‹ {process_id}: ç§å­ {seed} å®Œæˆ")
                else:
                    results['total_failed'] += 1
                    debug_print(f"   âŒ è¿›ç¨‹ {process_id}: ç§å­ {seed} å¤±è´¥")
                
                # æ¸…ç†é…ç½®æ–‡ä»¶
                if os.path.exists(config_file):
                    os.remove(config_file)
                    
            except Exception as e:
                debug_print(f"   ğŸ’¥ è¿›ç¨‹ {process_id}: ç§å­ {seed} å¼‚å¸¸: {e}")
                results['total_failed'] += 1
        
        results['end_time'] = datetime.now().isoformat()
        results['duration'] = (datetime.fromisoformat(results['end_time']) - 
                             datetime.fromisoformat(results['start_time'])).total_seconds()
        
        debug_print(f"ğŸ è¿›ç¨‹ {process_id}: {level.upper()} ç­‰çº§å®Œæˆ - æˆåŠŸ: {results['total_success']}, å¤±è´¥: {results['total_failed']}")
        return results
    
    def _prepare_experiment_directory(self, level, seed, process_id):
        """ä¸ºå®éªŒåˆ›å»ºç‹¬ç«‹ç›®å½•"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        exp_name = f"{level}_seed{seed}_proc{process_id}_{timestamp}"
        exp_dir = os.path.join(self.results_dir, level, exp_name)
        
        # åˆ›å»ºç›®å½•ç»“æ„
        os.makedirs(exp_dir, exist_ok=True)
        os.makedirs(os.path.join(exp_dir, "reward_plots"), exist_ok=True)
        os.makedirs(os.path.join(exp_dir, "trajectories"), exist_ok=True)
        
        return exp_dir
    
    def _create_config_file(self, level, seed, output_dir, config_filename):
        """åˆ›å»ºç‹¬ç«‹çš„é…ç½®æ–‡ä»¶"""
        config_content = f"""# Environment Settings
NUM_POINTS = 30
NUM_AGENTS = 5

# Training Settings
NUM_EPISODES = {self.episodes_per_experiment}
MAX_NUM_STEPS = 1500

BATCH_SIZE = 512  # é’ˆå¯¹11Gæ˜¾å­˜ä¼˜åŒ–
GAMMA = 0.999
UPDATE_EVERY = 1
CAPACITY = 100000

# Learning Rates
LR_ACTOR = 5e-5
LR_CRITIC = 5e-4

# Learning Rate Scheduling Settings
ENABLE_LR_SCHEDULING = True
LR_DECAY_TYPE = "exponential"
LR_DECAY_RATE = 0.995
LR_DECAY_EPISODES = 100
LR_MIN_RATIO = 0.1
LR_WARMUP_EPISODES = 50

# Exploration Noise Settings
NOISE_STD_START = 1.0
NOISE_STD_END = 0.01
NOISE_DECAY = 0.995

# Prioritized Experience Replay Settings
PER_ALPHA = 0.6
PER_BETA = 0.4
PER_BETA_INCREMENT = 0.001
PER_EPSILON = 1e-6

# Intelligent Conflict Resolution Settings
ICR_PRIORITY_WEIGHT = 1.0
ICR_AGENT_TYPE_WEIGHT = 1.0
ICR_DISTANCE_WEIGHT = 0.6
ICR_LOAD_WEIGHT = 0.4
ICR_URGENCY_WEIGHT = 0.8
ICR_ENERGY_WEIGHT = 0.2

# Visualization Settings
RENDER_INTERVAL = 1000

# Debug Settings
DEBUG_PRINT = False  # å¹¶è¡Œå®éªŒä¸­å¯ç”¨æ‰“å°ä»¥ä¾¿ç›‘æ§

# Random Seed Settings
RANDOM_SEED = {seed}

# Communication Settings
ENABLE_IMMEDIATE_COMMUNICATION = False

# Environment Settings
ENVIRONMENT_TYPE = "configurable"

# Agent Architecture Settings
USE_ENHANCED_CRITIC = True

# Incremental Reward Experiment Settings
from reward_config import RewardExperimentConfig
REWARD_EXPERIMENT_LEVEL = RewardExperimentConfig.{level.upper()}

# Output Directory Settings
OUTPUT_DIR = r"{output_dir}"
REWARD_PLOTS_DIR = r"{os.path.join(output_dir, 'reward_plots')}"
TRAJECTORIES_DIR = r"{os.path.join(output_dir, 'trajectories')}"

# æ™ºèƒ½ä½“å‚æ•°
agent_capacity = [10, 10, 10, 20, 20]
agent_speed = [40, 40, 40, 8, 8]
agent_energy_max = [80000, 80000, 80000, 120000, 120000]
fast_charge_time = 3 * 60
slow_charge_time = 8 * 60
"""
        
        with open(config_filename, "w", encoding="utf-8") as f:
            f.write(config_content)
    
    def _run_experiment(self, config_file, process_id, level, seed):
        """è¿è¡Œå•ä¸ªå®éªŒ"""
        try:
            # è®¾ç½®ç¯å¢ƒå˜é‡ï¼ŒæŒ‡å®šä½¿ç”¨çš„é…ç½®æ–‡ä»¶
            env = os.environ.copy()
            env['PYTHONPATH'] = os.getcwd()
            
            # åˆ›å»ºè¿è¡Œè„šæœ¬
            run_script = f"""
import sys
import importlib.util
import os

# åŠ¨æ€åŠ è½½æŒ‡å®šçš„é…ç½®æ–‡ä»¶
config_path = r"{config_file}"
spec = importlib.util.spec_from_file_location("config", config_path)
config = importlib.util.module_from_spec(spec)
sys.modules["config"] = config
spec.loader.exec_module(config)

# è¿è¡Œä¸»ç¨‹åº
from main import main
main()
"""
            
            script_file = os.path.join(self.temp_dir, f"run_{level}_{process_id}.py")
            with open(script_file, "w", encoding="utf-8") as f:
                f.write(run_script)
            
            # è¿è¡Œå®éªŒ
            result = subprocess.run(
                ["python", script_file],
                capture_output=True,
                text=True,
                env=env
            )
            
            # æ¸…ç†è¿è¡Œè„šæœ¬
            if os.path.exists(script_file):
                os.remove(script_file)
            
            return result.returncode == 0
            

        except Exception as e:
            debug_print(f"   ğŸ’¥ è¿›ç¨‹ {process_id}: è¿è¡Œå¼‚å¸¸: {e}")
            return False
    
    def run_parallel_experiments(self, max_processes=6):
        """å¹¶è¡Œè¿è¡Œæ‰€æœ‰å®éªŒç­‰çº§"""
        debug_print("ğŸš€ å¼€å§‹å¹¶è¡Œå®éªŒ")
        debug_print("=" * 80)
        debug_print(f"ğŸ“Š å®éªŒé…ç½®:")
        debug_print(f"   å®éªŒç­‰çº§: {len(self.experiment_levels)}")
        debug_print(f"   ç§å­æ•°é‡: {len(self.seeds)}")
        debug_print(f"   å¹¶è¡Œè¿›ç¨‹: {min(max_processes, len(self.experiment_levels))}")
        debug_print(f"   æ¯å®éªŒå›åˆæ•°: {self.episodes_per_experiment}")
        debug_print(f"   ç»“æœç›®å½•: {self.results_dir}")
        debug_print("=" * 80)
        
        self.start_time = datetime.now()
        
        # æ£€æŸ¥ç³»ç»Ÿèµ„æº
        self._check_system_resources()
        
        # åˆ›å»ºè¿›ç¨‹æ± å¹¶è¿è¡Œ
        actual_processes = min(max_processes, len(self.experiment_levels))
        
        with mp.Pool(processes=actual_processes) as pool:
            debug_print(f"ğŸ”„ å¯åŠ¨ {actual_processes} ä¸ªå¹¶è¡Œè¿›ç¨‹...")
            
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            results = pool.map(self.run_single_level_all_seeds, self.experiment_levels)
        
        # ä¿å­˜ç»“æœ
        self._save_parallel_results(results)
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        self._cleanup_temp_directory()
        
        # è¾“å‡ºæ€»ç»“
        self._print_summary(results)
        
        return results
    
    def _check_system_resources(self):
        """æ£€æŸ¥ç³»ç»Ÿèµ„æº"""
        # CPUæ ¸å¿ƒæ•°
        cpu_count = mp.cpu_count()
        debug_print(f"ğŸ’» ç³»ç»Ÿèµ„æº:")
        debug_print(f"   CPUæ ¸å¿ƒæ•°: {cpu_count}")
        
        # å†…å­˜ä½¿ç”¨æƒ…å†µ
        memory = psutil.virtual_memory()
        debug_print(f"   å†…å­˜ä½¿ç”¨: {memory.percent:.1f}% ({memory.available // (1024**3):.1f}GB å¯ç”¨)")
        
        # GPUä¿¡æ¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        try:
            import torch
            if torch.cuda.is_available():
                gpu_count = torch.cuda.device_count()
                debug_print(f"   GPUæ•°é‡: {gpu_count}")
                for i in range(gpu_count):
                    gpu_memory = torch.cuda.get_device_properties(i).total_memory
                    debug_print(f"   GPU {i}: {gpu_memory // (1024**3):.1f}GB")
        except ImportError:
            debug_print("   GPU: æœªå®‰è£…PyTorchï¼Œæ— æ³•æ£€æµ‹")
    
    def _save_parallel_results(self, results):
        """ä¿å­˜å¹¶è¡Œå®éªŒç»“æœ"""
        summary = {
            'start_time': self.start_time.isoformat(),
            'end_time': datetime.now().isoformat(),
            'total_duration': (datetime.now() - self.start_time).total_seconds(),
            'experiment_levels': self.experiment_levels,
            'seeds': self.seeds,
            'episodes_per_experiment': self.episodes_per_experiment,
            'results': results
        }
        
        results_file = os.path.join(self.results_dir, "parallel_experiment_summary.json")
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        debug_print(f"ğŸ“ å¹¶è¡Œå®éªŒç»“æœå·²ä¿å­˜: {results_file}")
    
    def _cleanup_temp_directory(self):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶ç›®å½•"""
        try:
            import shutil
            if os.path.exists(self.temp_dir):
                shutil.rmtree(self.temp_dir)
                debug_print(f"ğŸ§¹ å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶ç›®å½•: {self.temp_dir}")
        except Exception as e:
            debug_print(f"âš ï¸ æ¸…ç†ä¸´æ—¶ç›®å½•å¤±è´¥: {e}")
    
    def _print_summary(self, results):
        """æ‰“å°å®éªŒæ€»ç»“"""
        total_time = datetime.now() - self.start_time
        total_experiments = len(self.experiment_levels) * len(self.seeds)
        total_success = sum(r['total_success'] for r in results)
        
        debug_print("\n" + "ğŸ‰" * 80)
        debug_print("å¹¶è¡Œå®éªŒå®Œæˆæ€»ç»“")
        debug_print("ğŸ‰" * 80)
        debug_print(f"âœ… æˆåŠŸå®Œæˆ: {total_success}/{total_experiments}")
        debug_print(f"â±ï¸ æ€»è€—æ—¶: {total_time}")
        debug_print(f"ğŸ“ ç»“æœç›®å½•: {self.results_dir}")
        
        debug_print(f"\nğŸ“Š å„ç­‰çº§ç»“æœ:")
        for result in results:
            level = result['level']
            success = result['total_success']
            total = len(self.seeds)
            duration = timedelta(seconds=result['duration'])
            debug_print(f"   {level.upper()}: {success}/{total} ({duration})")
        
        debug_print(f"\nğŸ“Š ä¸‹ä¸€æ­¥: ä½¿ç”¨ python multi_seed_analyzer.py åˆ†æç»“æœ")
        debug_print("ğŸ‰" * 80)

def main():
    parser = argparse.ArgumentParser(description="å¹¶è¡Œè¿è¡Œ6ä¸ªå®éªŒç­‰çº§")
    parser.add_argument("--seeds", type=str, default="42,123,2024,888,1337",
                       help="éšæœºç§å­åˆ—è¡¨ï¼Œç”¨é€—å·åˆ†éš”")
    parser.add_argument("--episodes", type=int, default=3000,
                       help="æ¯ä¸ªå®éªŒçš„è®­ç»ƒå›åˆæ•°")
    parser.add_argument("--processes", type=int, default=6,
                       help="æœ€å¤§å¹¶è¡Œè¿›ç¨‹æ•°")
    parser.add_argument("--output", type=str, default="parallel_experiment_results",
                       help="ç»“æœè¾“å‡ºç›®å½•")
    
    args = parser.parse_args()
    
    # è§£æç§å­åˆ—è¡¨
    seeds = [int(s.strip()) for s in args.seeds.split(",")]
    
    runner = ParallelExperimentRunner(
        seeds=seeds,
        episodes_per_experiment=args.episodes,
        results_dir=args.output
    )
    
    runner.run_parallel_experiments(max_processes=args.processes)

if __name__ == "__main__":
    main()
