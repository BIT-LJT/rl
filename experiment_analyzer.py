"""
å¢é‡å¼å¥–åŠ±å®éªŒåˆ†æå·¥å…·

è¿™ä¸ªå·¥å…·å¸®åŠ©æ‚¨åˆ†æå’Œæ¯”è¾ƒä¸åŒå®éªŒç­‰çº§çš„è®­ç»ƒç»“æœï¼Œ
è¯†åˆ«æ¯ä¸ªå¥–åŠ±æ¨¡å—å¯¹åä½œç­–ç•¥çš„å…·ä½“å½±å“ã€‚

ä½¿ç”¨æ–¹æ³•ï¼š
1. è¿è¡Œä¸åŒç­‰çº§çš„å®éªŒ
2. ä½¿ç”¨æ­¤å·¥å…·åŠ è½½å’Œåˆ†æç»“æœ
3. ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Šå’Œå¯è§†åŒ–å›¾è¡¨
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import os
import json
from datetime import datetime
from utils import debug_print
from reward_config import RewardExperimentConfig, EXPERIMENT_CONFIGS

plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

class ExperimentAnalyzer:
    """å¢é‡å¼å¥–åŠ±å®éªŒåˆ†æå™¨"""
    
    def __init__(self, results_dir="experiment_results"):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            results_dir: å®éªŒç»“æœå­˜å‚¨ç›®å½•
        """
        self.results_dir = results_dir
        self.experiment_data = {}
        os.makedirs(results_dir, exist_ok=True)
    
    def save_experiment_result(self, experiment_level, episode_rewards, collaboration_analytics, 
                             training_config=None, final_metrics=None):
        """
        ä¿å­˜å®éªŒç»“æœ
        
        Args:
            experiment_level: å®éªŒç­‰çº§
            episode_rewards: æ¯è½®å¥–åŠ±è®°å½•
            collaboration_analytics: åä½œåˆ†ææ•°æ®
            training_config: è®­ç»ƒé…ç½®ä¿¡æ¯
            final_metrics: æœ€ç»ˆæ€§èƒ½æŒ‡æ ‡
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        result_data = {
            'experiment_level': experiment_level,
            'timestamp': timestamp,
            'episode_rewards': np.array(episode_rewards).tolist(),
            'collaboration_analytics': collaboration_analytics,
            'training_config': training_config or {},
            'final_metrics': final_metrics or {},
            'reward_config': EXPERIMENT_CONFIGS[experiment_level].get_reward_constants()
        }
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        filename = f"{experiment_level}_{timestamp}.json"
        filepath = os.path.join(self.results_dir, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(result_data, f, ensure_ascii=False, indent=2)
        
        debug_print(f"âœ… å®éªŒç»“æœå·²ä¿å­˜: {filepath}")
        
        # åŒæ—¶ä¿å­˜numpyæ ¼å¼çš„å¥–åŠ±æ•°æ®ï¼ˆå…¼å®¹ç°æœ‰åˆ†æå·¥å…·ï¼‰
        rewards_array = np.array(episode_rewards)
        np_filename = f"{experiment_level}_{timestamp}_rewards.npy"
        np_filepath = os.path.join(self.results_dir, np_filename)
        np.save(np_filepath, rewards_array)
        
        return filepath
    
    def load_experiment_results(self, experiment_levels=None):
        """
        åŠ è½½å®éªŒç»“æœ
        
        Args:
            experiment_levels: è¦åŠ è½½çš„å®éªŒç­‰çº§åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºåŠ è½½æ‰€æœ‰
        """
        if experiment_levels is None:
            experiment_levels = [RewardExperimentConfig.BASIC, RewardExperimentConfig.LOAD_EFFICIENCY,
                               RewardExperimentConfig.ROLE_SPECIALIZATION, RewardExperimentConfig.COLLABORATION,
                               RewardExperimentConfig.BEHAVIOR_SHAPING, RewardExperimentConfig.FULL]
        
        self.experiment_data = {}
        
        for level in experiment_levels:
            # æŸ¥æ‰¾è¯¥ç­‰çº§çš„æœ€æ–°ç»“æœæ–‡ä»¶
            level_files = [f for f in os.listdir(self.results_dir) 
                          if f.startswith(level) and f.endswith('.json')]
            
            if level_files:
                # é€‰æ‹©æœ€æ–°çš„æ–‡ä»¶
                latest_file = sorted(level_files)[-1]
                filepath = os.path.join(self.results_dir, latest_file)
                
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    self.experiment_data[level] = data
                    debug_print(f"âœ… å·²åŠ è½½ {level} å®éªŒç»“æœ: {latest_file}")
                except Exception as e:
                    debug_print(f"âŒ åŠ è½½ {level} å®éªŒç»“æœå¤±è´¥: {e}")
            else:
                debug_print(f"âš ï¸ æœªæ‰¾åˆ° {level} å®éªŒç»“æœ")
        
        return len(self.experiment_data)
    
    def generate_comparison_report(self, output_file="experiment_comparison_report.txt"):
        """ç”Ÿæˆå®éªŒå¯¹æ¯”æŠ¥å‘Š"""
        if not self.experiment_data:
            debug_print("âŒ æ²¡æœ‰åŠ è½½ä»»ä½•å®éªŒæ•°æ®ï¼Œè¯·å…ˆè°ƒç”¨ load_experiment_results()")
            return
        
        report_lines = []
        report_lines.append("=" * 80)
        report_lines.append("å¢é‡å¼å¥–åŠ±å®éªŒå¯¹æ¯”æŠ¥å‘Š")
        report_lines.append("=" * 80)
        report_lines.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append(f"å®éªŒæ•°é‡: {len(self.experiment_data)}")
        report_lines.append("")
        
        # ä¸ºæ¯ä¸ªå®éªŒç­‰çº§ç”Ÿæˆåˆ†æ
        for level in sorted(self.experiment_data.keys()):
            data = self.experiment_data[level]
            config = EXPERIMENT_CONFIGS[level]
            
            report_lines.append(f"ğŸ“Š å®éªŒç­‰çº§: {level.upper()}")
            report_lines.append("-" * 60)
            report_lines.append(f"æè¿°: {config.get_experiment_description()}")
            report_lines.append(f"é¢„æœŸè¡Œä¸º: {', '.join(config.get_expected_behaviors())}")
            report_lines.append("")
            
            # å¥–åŠ±åˆ†æ
            episode_rewards = np.array(data['episode_rewards'])
            if episode_rewards.ndim > 1:
                total_rewards_per_episode = np.sum(episode_rewards, axis=1)
                avg_reward_per_agent = np.mean(episode_rewards, axis=1)
            else:
                total_rewards_per_episode = episode_rewards
                avg_reward_per_agent = episode_rewards
            
            report_lines.append("ğŸ¯ æ€§èƒ½æŒ‡æ ‡:")
            report_lines.append(f"   æœ€ç»ˆå¹³å‡å¥–åŠ±: {np.mean(total_rewards_per_episode[-100:]):.2f}")
            report_lines.append(f"   å¥–åŠ±æ ‡å‡†å·®: {np.std(total_rewards_per_episode[-100:]):.2f}")
            report_lines.append(f"   è®­ç»ƒå›åˆæ•°: {len(total_rewards_per_episode)}")
            
            # åä½œåˆ†æ
            if 'collaboration_analytics' in data:
                analytics = data['collaboration_analytics']
                if analytics:
                    report_lines.append("")
                    report_lines.append("ğŸ¤ åä½œæŒ‡æ ‡:")
                    report_lines.append(f"   å†²çªç‡: {analytics.get('conflict_rate', 0):.3f}")
                    report_lines.append(f"   æ€»å†²çªæ¬¡æ•°: {analytics.get('total_conflicts', 0)}")
                    report_lines.append(f"   æ€»å†³ç­–æ¬¡æ•°: {analytics.get('total_decisions', 0)}")
            
            report_lines.append("")
            report_lines.append("")
        
        # è·¨å®éªŒå¯¹æ¯”
        if len(self.experiment_data) > 1:
            report_lines.append("ğŸ“ˆ è·¨å®éªŒå¯¹æ¯”åˆ†æ")
            report_lines.append("-" * 60)
            
            # æ€§èƒ½æå‡åˆ†æ
            performance_data = {}
            for level, data in self.experiment_data.items():
                episode_rewards = np.array(data['episode_rewards'])
                if episode_rewards.ndim > 1:
                    final_performance = np.mean(np.sum(episode_rewards[-100:], axis=1))
                else:
                    final_performance = np.mean(episode_rewards[-100:])
                performance_data[level] = final_performance
            
            # æŒ‰æ€§èƒ½æ’åº
            sorted_performance = sorted(performance_data.items(), key=lambda x: x[1], reverse=True)
            
            report_lines.append("æ€§èƒ½æ’åº (æœ€ç»ˆ100è½®å¹³å‡æ€»å¥–åŠ±):")
            for i, (level, perf) in enumerate(sorted_performance):
                report_lines.append(f"   {i+1}. {level.upper()}: {perf:.2f}")
            
            # å¢é‡æå‡åˆ†æ
            report_lines.append("")
            report_lines.append("å¢é‡æ•ˆæœåˆ†æ:")
            experiment_order = [RewardExperimentConfig.BASIC, RewardExperimentConfig.LOAD_EFFICIENCY,
                              RewardExperimentConfig.ROLE_SPECIALIZATION, RewardExperimentConfig.COLLABORATION,
                              RewardExperimentConfig.BEHAVIOR_SHAPING, RewardExperimentConfig.FULL]
            
            prev_perf = None
            for level in experiment_order:
                if level in performance_data:
                    current_perf = performance_data[level]
                    if prev_perf is not None:
                        improvement = current_perf - prev_perf
                        improvement_pct = (improvement / abs(prev_perf)) * 100 if prev_perf != 0 else 0
                        report_lines.append(f"   {level} vs å‰ä¸€çº§: {improvement:+.2f} ({improvement_pct:+.1f}%)")
                    prev_perf = current_perf
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = os.path.join(self.results_dir, output_file)
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_lines))
        
        debug_print(f"ğŸ“Š å¯¹æ¯”æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
        
        # ä¹Ÿæ‰“å°åˆ°æ§åˆ¶å°
        for line in report_lines:
            debug_print(line)
    
    def plot_performance_comparison(self, output_file="performance_comparison.png"):
        """ç»˜åˆ¶æ€§èƒ½å¯¹æ¯”å›¾"""
        if not self.experiment_data:
            debug_print("âŒ æ²¡æœ‰åŠ è½½ä»»ä½•å®éªŒæ•°æ®")
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('å¢é‡å¼å¥–åŠ±å®éªŒæ€§èƒ½å¯¹æ¯”', fontsize=16, fontweight='bold')
        
        # 1. å¥–åŠ±æ›²çº¿å¯¹æ¯”
        ax1 = axes[0, 0]
        for level, data in self.experiment_data.items():
            episode_rewards = np.array(data['episode_rewards'])
            if episode_rewards.ndim > 1:
                total_rewards = np.sum(episode_rewards, axis=1)
            else:
                total_rewards = episode_rewards
            
            # å¹³æ»‘å¤„ç†
            window_size = min(50, len(total_rewards) // 10)
            if window_size > 1:
                smoothed = pd.Series(total_rewards).rolling(window=window_size).mean()
                ax1.plot(smoothed, label=level.upper(), linewidth=2)
            else:
                ax1.plot(total_rewards, label=level.upper(), linewidth=2)
        
        ax1.set_title('è®­ç»ƒå¥–åŠ±æ›²çº¿å¯¹æ¯”')
        ax1.set_xlabel('è®­ç»ƒå›åˆ')
        ax1.set_ylabel('æ€»å¥–åŠ±')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. æœ€ç»ˆæ€§èƒ½å¯¹æ¯”
        ax2 = axes[0, 1]
        levels = []
        final_performances = []
        
        for level, data in self.experiment_data.items():
            episode_rewards = np.array(data['episode_rewards'])
            if episode_rewards.ndim > 1:
                final_perf = np.mean(np.sum(episode_rewards[-100:], axis=1))
            else:
                final_perf = np.mean(episode_rewards[-100:])
            
            levels.append(level.upper())
            final_performances.append(final_perf)
        
        bars = ax2.bar(levels, final_performances, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b'])
        ax2.set_title('æœ€ç»ˆæ€§èƒ½å¯¹æ¯” (æœ€å100è½®å¹³å‡)')
        ax2.set_ylabel('å¹³å‡æ€»å¥–åŠ±')
        ax2.tick_params(axis='x', rotation=45)
        
        # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, perf in zip(bars, final_performances):
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height + max(final_performances)*0.01,
                    f'{perf:.1f}', ha='center', va='bottom')
        
        # 3. å†²çªç‡å¯¹æ¯”
        ax3 = axes[1, 0]
        conflict_levels = []
        conflict_rates = []
        
        for level, data in self.experiment_data.items():
            if 'collaboration_analytics' in data and data['collaboration_analytics']:
                analytics = data['collaboration_analytics']
                conflict_rate = analytics.get('conflict_rate', 0)
                conflict_levels.append(level.upper())
                conflict_rates.append(conflict_rate)
        
        if conflict_rates:
            bars = ax3.bar(conflict_levels, conflict_rates, color='red', alpha=0.7)
            ax3.set_title('å†²çªç‡å¯¹æ¯”')
            ax3.set_ylabel('å†²çªç‡')
            ax3.tick_params(axis='x', rotation=45)
            
            for bar, rate in zip(bars, conflict_rates):
                height = bar.get_height()
                ax3.text(bar.get_x() + bar.get_width()/2., height + max(conflict_rates)*0.01,
                        f'{rate:.3f}', ha='center', va='bottom')
        else:
            ax3.text(0.5, 0.5, 'æš‚æ— å†²çªæ•°æ®', ha='center', va='center', transform=ax3.transAxes)
            ax3.set_title('å†²çªç‡å¯¹æ¯”')
        
        # 4. æ¨¡å—å¯ç”¨çŠ¶æ€
        ax4 = axes[1, 1]
        modules = ['è½½é‡æ•ˆç‡', 'è§’è‰²ä¸“ä¸šåŒ–', 'åä½œæ¨¡å—', 'è¡Œä¸ºå¡‘é€ ', 'é«˜çº§æƒ©ç½š']
        module_keys = ['enable_load_efficiency', 'enable_role_specialization', 
                      'enable_collaboration', 'enable_behavior_shaping', 'enable_advanced_penalties']
        
        experiment_order = [RewardExperimentConfig.BASIC, RewardExperimentConfig.LOAD_EFFICIENCY,
                          RewardExperimentConfig.ROLE_SPECIALIZATION, RewardExperimentConfig.COLLABORATION,
                          RewardExperimentConfig.BEHAVIOR_SHAPING, RewardExperimentConfig.FULL]
        
        # åˆ›å»ºæ¨¡å—å¯ç”¨çŸ©é˜µ
        module_matrix = []
        level_labels = []
        
        for level in experiment_order:
            if level in self.experiment_data:
                config = EXPERIMENT_CONFIGS[level]
                row = []
                for key in module_keys:
                    row.append(1 if getattr(config, key, False) else 0)
                module_matrix.append(row)
                level_labels.append(level.upper())
        
        if module_matrix:
            im = ax4.imshow(module_matrix, cmap='RdYlGn', aspect='auto')
            ax4.set_xticks(range(len(modules)))
            ax4.set_xticklabels(modules, rotation=45, ha='right')
            ax4.set_yticks(range(len(level_labels)))
            ax4.set_yticklabels(level_labels)
            ax4.set_title('å¥–åŠ±æ¨¡å—å¯ç”¨çŠ¶æ€')
            
            # æ·»åŠ æ–‡æœ¬æ ‡æ³¨
            for i in range(len(level_labels)):
                for j in range(len(modules)):
                    text = 'âœ“' if module_matrix[i][j] else 'âœ—'
                    ax4.text(j, i, text, ha="center", va="center", 
                           color="black" if module_matrix[i][j] else "red", fontweight='bold')
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        output_path = os.path.join(self.results_dir, output_file)
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        debug_print(f"ğŸ“Š æ€§èƒ½å¯¹æ¯”å›¾å·²ç”Ÿæˆ: {output_path}")
    
    def analyze_reward_module_impact(self):
        """åˆ†ææ¯ä¸ªå¥–åŠ±æ¨¡å—çš„å½±å“"""
        if len(self.experiment_data) < 2:
            debug_print("âŒ éœ€è¦è‡³å°‘2ä¸ªå®éªŒç»“æœè¿›è¡Œå¯¹æ¯”åˆ†æ")
            return
        
        debug_print("\n" + "ğŸ”" * 60)
        debug_print("å¥–åŠ±æ¨¡å—å½±å“åˆ†æ")
        debug_print("ğŸ”" * 60)
        
        # æŒ‰å®éªŒé¡ºåºåˆ†æ
        experiment_order = [RewardExperimentConfig.BASIC, RewardExperimentConfig.LOAD_EFFICIENCY,
                          RewardExperimentConfig.ROLE_SPECIALIZATION, RewardExperimentConfig.COLLABORATION,
                          RewardExperimentConfig.BEHAVIOR_SHAPING, RewardExperimentConfig.FULL]
        
        prev_data = None
        prev_level = None
        
        for level in experiment_order:
            if level not in self.experiment_data:
                continue
                
            current_data = self.experiment_data[level]
            
            if prev_data is not None:
                # è®¡ç®—æ€§èƒ½å˜åŒ–
                prev_rewards = np.array(prev_data['episode_rewards'])
                curr_rewards = np.array(current_data['episode_rewards'])
                
                if prev_rewards.ndim > 1:
                    prev_perf = np.mean(np.sum(prev_rewards[-100:], axis=1))
                else:
                    prev_perf = np.mean(prev_rewards[-100:])
                
                if curr_rewards.ndim > 1:
                    curr_perf = np.mean(np.sum(curr_rewards[-100:], axis=1))
                else:
                    curr_perf = np.mean(curr_rewards[-100:])
                
                performance_change = curr_perf - prev_perf
                performance_change_pct = (performance_change / abs(prev_perf)) * 100 if prev_perf != 0 else 0
                
                # åˆ†ææ–°å¢çš„æ¨¡å—
                prev_config = EXPERIMENT_CONFIGS[prev_level]
                curr_config = EXPERIMENT_CONFIGS[level]
                
                new_modules = []
                if not prev_config.enable_load_efficiency and curr_config.enable_load_efficiency:
                    new_modules.append("è½½é‡æ•ˆç‡æ¨¡å—")
                if not prev_config.enable_role_specialization and curr_config.enable_role_specialization:
                    new_modules.append("è§’è‰²ä¸“ä¸šåŒ–æ¨¡å—")
                if not prev_config.enable_collaboration and curr_config.enable_collaboration:
                    new_modules.append("åä½œæ¨¡å—")
                if not prev_config.enable_behavior_shaping and curr_config.enable_behavior_shaping:
                    new_modules.append("è¡Œä¸ºå¡‘é€ æ¨¡å—")
                if not prev_config.enable_advanced_penalties and curr_config.enable_advanced_penalties:
                    new_modules.append("é«˜çº§æƒ©ç½šæ¨¡å—")
                
                debug_print(f"\nğŸ“ˆ {prev_level.upper()} â†’ {level.upper()}")
                debug_print(f"   æ–°å¢æ¨¡å—: {', '.join(new_modules) if new_modules else 'æ— '}")
                debug_print(f"   æ€§èƒ½å˜åŒ–: {performance_change:+.2f} ({performance_change_pct:+.1f}%)")
                
                if performance_change > 0:
                    debug_print(f"   å½±å“è¯„ä¼°: âœ… æ­£é¢å½±å“ï¼Œæ€§èƒ½æå‡")
                elif performance_change < -50:  # æ˜¾è‘—ä¸‹é™
                    debug_print(f"   å½±å“è¯„ä¼°: âŒ è´Ÿé¢å½±å“ï¼Œæ€§èƒ½æ˜¾è‘—ä¸‹é™")
                else:
                    debug_print(f"   å½±å“è¯„ä¼°: âš ï¸ è½»å¾®å½±å“æˆ–éœ€è¦æ›´å¤šè®­ç»ƒ")
                
                # åä½œæŒ‡æ ‡å˜åŒ–
                if 'collaboration_analytics' in prev_data and 'collaboration_analytics' in current_data:
                    prev_analytics = prev_data['collaboration_analytics']
                    curr_analytics = current_data['collaboration_analytics']
                    
                    if prev_analytics and curr_analytics:
                        prev_conflict_rate = prev_analytics.get('conflict_rate', 0)
                        curr_conflict_rate = curr_analytics.get('conflict_rate', 0)
                        conflict_change = curr_conflict_rate - prev_conflict_rate
                        
                        debug_print(f"   å†²çªç‡å˜åŒ–: {prev_conflict_rate:.3f} â†’ {curr_conflict_rate:.3f} ({conflict_change:+.3f})")
            
            prev_data = current_data
            prev_level = level
        
        debug_print("ğŸ”" * 60)

# ä¾¿æ·ä½¿ç”¨å‡½æ•°
def analyze_latest_experiments():
    """åˆ†ææœ€æ–°çš„å®éªŒç»“æœ"""
    analyzer = ExperimentAnalyzer()
    
    # åŠ è½½æ‰€æœ‰å®éªŒç»“æœ
    loaded_count = analyzer.load_experiment_results()
    
    if loaded_count == 0:
        debug_print("âŒ æœªæ‰¾åˆ°ä»»ä½•å®éªŒç»“æœæ–‡ä»¶")
        return
    
    debug_print(f"âœ… å·²åŠ è½½ {loaded_count} ä¸ªå®éªŒç»“æœ")
    
    # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    analyzer.generate_comparison_report()
    
    # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
    analyzer.plot_performance_comparison()
    
    # åˆ†ææ¨¡å—å½±å“
    analyzer.analyze_reward_module_impact()
    
    debug_print(f"\nğŸ¯ åˆ†æå®Œæˆï¼ç»“æœä¿å­˜åœ¨ {analyzer.results_dir} ç›®å½•ä¸­")

if __name__ == "__main__":
    # æ¼”ç¤ºç”¨æ³•
    analyze_latest_experiments()
