# 🧪 增量式奖励实验指南

这份指南将帮助您系统地分析复杂奖励函数对多智能体协作策略的影响，通过增量式实验发现每个奖励模块的贡献。

## 📋 实验概述

### 为什么需要增量式实验？

您的项目包含了非常复杂的奖励函数，虽然这能引导出高级的协作行为，但在模型不收敛时很难调试。增量式实验能够：

- ✅ **系统验证**：确保每个奖励模块都能正确引导期望的行为
- ✅ **影响量化**：精确测量每个模块对最终性能的贡献
- ✅ **调试简化**：快速定位导致性能问题的奖励模块
- ✅ **策略洞察**：深入理解不同奖励如何塑造协作策略

### 实验等级设计

我们设计了6个递进的实验等级：

| 等级 | 名称 | 描述 | 核心目标 |
|-----|------|------|----------|
| 🟢 **BASIC** | 基础实验 | 只有采集奖励和基础惩罚 | 验证基本学习能力 |
| 🔵 **LOAD_EFFICIENCY** | 载重效率 | +满载奖励和空载惩罚 | 学习高效载重策略 |
| 🟡 **ROLE_SPECIALIZATION** | 角色专业化 | +快速/重载智能体专业化 | 学习分工协作 |
| 🟠 **COLLABORATION** | 协作模块 | +冲突解决和协作奖励 | 学习避免冲突 |
| 🟣 **BEHAVIOR_SHAPING** | 行为塑造 | +接近奖励和智能行为 | 学习精细化行为 |
| 🔴 **FULL** | 完整实验 | 所有奖励模块 | 学习最复杂策略 |

## 🚀 快速开始

### 1. 配置实验

编辑 `config.py` 文件：

```python
# 启用可配置奖励环境
ENVIRONMENT_TYPE = "configurable"

# 设置实验等级（从BASIC开始）
REWARD_EXPERIMENT_LEVEL = RewardExperimentConfig.BASIC

# 关闭调试输出（可选，提升训练速度）
DEBUG_PRINT = False

# 推荐的训练配置
NUM_EPISODES = 2000  # 基础实验可以用较少轮次
```

### 2. 运行实验

```bash
python main.py
```

### 3. 分析结果

训练完成后，运行分析工具：

```bash
python experiment_analyzer.py
```

或在Python中：

```python
from experiment_analyzer import analyze_latest_experiments
analyze_latest_experiments()
```

## 📊 详细实验流程

### 阶段1：基础验证 (BASIC)

**目标**：验证智能体能学会最基本的任务采集

**配置**：
```python
REWARD_EXPERIMENT_LEVEL = RewardExperimentConfig.BASIC
NUM_EPISODES = 1500
```

**期望行为**：
- 智能体能找到并采集任务点
- 基本的能量管理（避免耗尽）
- 载重后返回基地

**成功标准**：
- 任务完成率 > 80%
- 奖励曲线明显上升
- 无频繁的能量耗尽

**常见问题**：
- 如果智能体学不会基本采集，检查学习率和网络架构
- 如果能量经常耗尽，可能需要调整能量消耗参数

### 阶段2：载重效率 (LOAD_EFFICIENCY)

**目标**：学习高效的载重策略

**配置**：
```python
REWARD_EXPERIMENT_LEVEL = RewardExperimentConfig.LOAD_EFFICIENCY
NUM_EPISODES = 2000
```

**期望行为**：
- 避免空载返回基地
- 尽量满载或高载重后返回
- 载重利用率提升

**关键指标**：
- 平均载重利用率
- 空载返回次数
- 满载奖励获得频次

**成功标准**：
- 载重利用率 > 0.7
- 空载返回 < 20%
- 性能比BASIC提升10%+

### 阶段3：角色专业化 (ROLE_SPECIALIZATION)

**目标**：学习基于智能体类型的分工协作

**配置**：
```python
REWARD_EXPERIMENT_LEVEL = RewardExperimentConfig.ROLE_SPECIALIZATION
NUM_EPISODES = 2500
```

**期望行为**：
- 快速智能体(0-2)优先处理高优先级任务
- 重载智能体(3-4)专注大载重任务
- 明显的角色分工

**关键指标**：
- 快速智能体高优先级任务比例
- 重载智能体载重效率
- 专业化差距指标

**成功标准**：
- 快速智能体高优先级比例 > 0.6
- 重载智能体载重率 > 快速智能体
- 专业化差距 > 0.1

### 阶段4：协作模块 (COLLABORATION)

**目标**：学习避免冲突的协作策略

**配置**：
```python
REWARD_EXPERIMENT_LEVEL = RewardExperimentConfig.COLLABORATION
NUM_EPISODES = 3000
```

**期望行为**：
- 冲突频率显著下降
- 智能的冲突解决
- 协作处理高优先级任务

**关键指标**：
- 冲突率
- 冲突解决成功率
- 协作效率

**成功标准**：
- 冲突率 < 0.1
- 比前一阶段冲突减少50%+
- 协作任务完成效率提升

### 阶段5：行为塑造 (BEHAVIOR_SHAPING)

**目标**：学习精细化的行为模式

**配置**：
```python
REWARD_EXPERIMENT_LEVEL = RewardExperimentConfig.BEHAVIOR_SHAPING
NUM_EPISODES = 3500
```

**期望行为**：
- 路径优化（受接近奖励引导）
- 智能的状态感知行为
- 任务完成后的最优选择

**关键指标**：
- 路径效率
- 智能行为奖励获得情况
- 精细化行为评分

**成功标准**：
- 平均路径长度减少
- 智能行为奖励频繁获得
- 整体行为更加精细

### 阶段6：完整实验 (FULL)

**目标**：学习最复杂的协作策略

**配置**：
```python
REWARD_EXPERIMENT_LEVEL = RewardExperimentConfig.FULL
NUM_EPISODES = 4000
```

**期望行为**：
- 所有先前行为的综合
- 复杂的多智能体协作
- 高度优化的任务分配

**关键指标**：
- 综合协作效率
- 多维度平衡评分
- 与所有前序实验的对比

## 📈 结果分析

### 自动分析工具

运行 `python experiment_analyzer.py` 将生成：

1. **文本报告** (`experiment_comparison_report.txt`)
   - 各实验等级的详细性能指标
   - 增量效果分析
   - 模块影响评估

2. **可视化图表** (`performance_comparison.png`)
   - 训练奖励曲线对比
   - 最终性能柱状图
   - 冲突率变化
   - 模块启用状态矩阵

3. **影响分析**
   - 每个新增模块对性能的具体影响
   - 正面/负面效果识别
   - 优化建议

### 手动分析指标

#### 性能指标
- **最终平均奖励**：最后100轮的平均总奖励
- **收敛速度**：达到稳定性能所需的轮次
- **稳定性**：后期奖励的标准差

#### 协作指标
- **冲突率**：任务冲突次数 / 总决策次数
- **载重利用率**：平均载重 / 最大载重
- **角色专业化差距**：快速vs重载智能体的任务分配差异

#### 效率指标
- **任务完成率**：成功采集的任务比例
- **路径效率**：平均移动距离
- **能量利用率**：有效使用的能量比例

## 🔍 问题诊断

### 常见问题及解决方案

#### 问题1：某个等级性能大幅下降

**可能原因**：
- 新增的奖励模块与已有模块冲突
- 奖励权重设置不当
- 需要更多训练时间

**解决方案**：
1. 检查 `reward_config.py` 中的奖励权重
2. 增加训练轮次
3. 调整奖励参数

#### 问题2：智能体行为不符合预期

**诊断步骤**：
1. 开启调试输出：`DEBUG_PRINT = True`
2. 观察训练过程中的奖励获得情况
3. 检查相应奖励模块的实现

#### 问题3：冲突率没有下降

**可能原因**：
- 协作模块的奖励权重太小
- 冲突解决算法需要优化
- 智能体还没学会协作策略

**解决方案**：
1. 增加协作相关奖励的权重
2. 延长训练时间
3. 分析冲突解决的具体过程

## ⚙️ 高级配置

### 自定义实验等级

您可以创建自己的实验配置：

```python
# 在 reward_config.py 中添加
class CustomRewardConfig(RewardExperimentConfig):
    def __init__(self):
        super().__init__()
        # 自定义模块启用状态
        self.enable_load_efficiency = True
        self.enable_role_specialization = False
        self.enable_collaboration = True
        # ... 其他配置
```

### 批量实验脚本

创建脚本自动运行所有等级：

```python
# batch_experiment.py
import subprocess
from reward_config import RewardExperimentConfig

levels = [RewardExperimentConfig.BASIC, RewardExperimentConfig.LOAD_EFFICIENCY, 
          RewardExperimentConfig.ROLE_SPECIALIZATION, RewardExperimentConfig.COLLABORATION,
          RewardExperimentConfig.BEHAVIOR_SHAPING, RewardExperimentConfig.FULL]

for level in levels:
    print(f"Running experiment: {level}")
    # 修改config.py中的REWARD_EXPERIMENT_LEVEL
    # 运行训练
    subprocess.run(["python", "main.py"])
    print(f"Completed: {level}")

# 最后运行分析
subprocess.run(["python", "experiment_analyzer.py"])
```

## 📝 实验记录模板

建议为每个实验等级创建详细记录：

```
实验等级：BASIC
日期：2024-01-XX
配置：
- NUM_EPISODES: 1500
- LR_ACTOR: 5e-5
- LR_CRITIC: 5e-4

结果：
- 最终性能：XXX
- 收敛轮次：XXX
- 关键观察：XXX

问题：
- XXX

下一步：
- XXX
```

## 🎯 预期成果

完成完整的增量式实验后，您将获得：

1. **清晰的模块影响图谱**：了解每个奖励模块的具体贡献
2. **优化的奖励配置**：基于实验结果的最佳奖励参数
3. **协作策略洞察**：深入理解多智能体协作的演化过程
4. **调试能力提升**：快速定位和解决奖励函数问题的能力

## 🤝 注意事项

1. **耐心训练**：复杂的协作策略需要足够的训练时间
2. **多次运行**：每个等级建议运行2-3次确保结果稳定
3. **参数调整**：根据实验结果适当调整学习率和奖励权重
4. **环境一致**：确保所有实验使用相同的环境参数

---

🎉 **祝您实验顺利！通过系统的增量式分析，您将完全掌握复杂奖励函数的设计精髓。**
