"""
å¯é…ç½®å¥–åŠ±çš„ç¯å¢ƒç±» - æ”¯æŒå¢é‡å¼å¥–åŠ±å®éªŒ

è¿™ä¸ªç¯å¢ƒç±»ç»§æ‰¿è‡ªSamplingEnv2_0ï¼Œä½†å¯ä»¥æ ¹æ®reward_configä¸­çš„å®éªŒç­‰çº§
åŠ¨æ€è°ƒæ•´å¥–åŠ±å‡½æ•°ï¼Œæ”¯æŒå¢é‡å¼å¥–åŠ±å®éªŒã€‚

ä½¿ç”¨æ–¹æ³•ï¼š
1. åœ¨config.pyä¸­è®¾ç½®REWARD_EXPERIMENT_LEVEL
2. åœ¨main.pyä¸­ä½¿ç”¨SamplingEnvConfigurableæ›¿æ¢SamplingEnv2_0
3. è¿›è¡Œå¢é‡å¼å®éªŒï¼Œè§‚å¯Ÿä¸åŒå¥–åŠ±æ¨¡å—çš„å½±å“
"""

import numpy as np
from env import SamplingEnv2_0
from utils import debug_print
import config
from reward_config import EXPERIMENT_CONFIGS

class SamplingEnvConfigurable(SamplingEnv2_0):
    """å¯é…ç½®å¥–åŠ±çš„é‡‡æ ·ç¯å¢ƒ"""
    
    def __init__(self, num_points=30, num_agents=5):
        super().__init__(num_points, num_agents)
        
        # è·å–å½“å‰å®éªŒé…ç½®
        self.reward_config = EXPERIMENT_CONFIGS[config.REWARD_EXPERIMENT_LEVEL]
        self.reward_constants = self.reward_config.get_reward_constants()
        
        # åˆå§‹åŒ–é¢å¤–éœ€è¦çš„ç»Ÿè®¡å±æ€§
        self.agent_task_completion = np.zeros(self.num_agents, dtype=int)  # æ¯ä¸ªæ™ºèƒ½ä½“å®Œæˆçš„ä»»åŠ¡æ•°
        self.agent_high_priority_tasks = np.zeros(self.num_agents, dtype=int)  # æ¯ä¸ªæ™ºèƒ½ä½“å®Œæˆçš„é«˜ä¼˜å…ˆçº§ä»»åŠ¡æ•°
        
        # è¾“å‡ºå®éªŒä¿¡æ¯
        debug_print(f"\nğŸ§ª å¯åŠ¨å¢é‡å¼å¥–åŠ±å®éªŒ")
        debug_print(f"   å®éªŒç­‰çº§: {config.REWARD_EXPERIMENT_LEVEL.upper()}")
        debug_print(f"   æè¿°: {self.reward_config.get_experiment_description()}")
        debug_print(f"   é¢„æœŸè¡Œä¸º: {', '.join(self.reward_config.get_expected_behaviors())}")
        debug_print(f"   å…³é”®æŒ‡æ ‡: {', '.join(self.reward_config.get_key_metrics())}")
        
        # è¾“å‡ºå¯ç”¨çš„å¥–åŠ±æ¨¡å—
        self._print_enabled_modules()
    
    def reset(self):
        """é‡ç½®ç¯å¢ƒï¼ŒåŒ…æ‹¬é¢å¤–çš„ç»Ÿè®¡å±æ€§"""
        obs = super().reset()
        
        # é‡ç½®é¢å¤–çš„ç»Ÿè®¡å±æ€§
        self.agent_task_completion = np.zeros(self.num_agents, dtype=int)
        self.agent_high_priority_tasks = np.zeros(self.num_agents, dtype=int)
        
        return obs
    
    def _print_enabled_modules(self):
        """è¾“å‡ºå½“å‰å¯ç”¨çš„å¥–åŠ±æ¨¡å—"""
        debug_print(f"\nğŸ›ï¸ å¥–åŠ±æ¨¡å—çŠ¶æ€:")
        debug_print(f"   åŸºç¡€å¥–åŠ±: âœ… (æ°¸è¿œå¯ç”¨)")
        debug_print(f"   è½½é‡æ•ˆç‡: {'âœ…' if self.reward_config.enable_load_efficiency else 'âŒ'}")
        debug_print(f"   è§’è‰²ä¸“ä¸šåŒ–: {'âœ…' if self.reward_config.enable_role_specialization else 'âŒ'}")
        debug_print(f"   åä½œæ¨¡å—: {'âœ…' if self.reward_config.enable_collaboration else 'âŒ'}")
        debug_print(f"   è¡Œä¸ºå¡‘é€ : {'âœ…' if self.reward_config.enable_behavior_shaping else 'âŒ'}")
        debug_print(f"   é«˜çº§æƒ©ç½š: {'âœ…' if self.reward_config.enable_advanced_penalties else 'âŒ'}")
    
    def step(self, actions, current_step, max_steps):
        """
        å¯é…ç½®çš„stepå‡½æ•°ï¼Œæ ¹æ®å®éªŒç­‰çº§åº”ç”¨ä¸åŒçš„å¥–åŠ±æ¨¡å—
        """
        rewards = np.zeros(self.num_agents)
        actions = np.array(actions)
        
        # ç»Ÿè®¡æ€»å†³ç­–æ¬¡æ•°
        self.total_decisions += len([a for a in actions if a < self.num_points])
        
        # è·å–å¥–åŠ±å¸¸é‡
        R = self.reward_constants
        
        # === åŸºç¡€æ—¶é—´æ­¥æƒ©ç½šï¼ˆæ°¸è¿œå¯ç”¨ï¼‰===
        rewards += R['P_TIME_STEP']
        
        # === å¤„ç†å……ç”µçŠ¶æ€ ===
        for i in range(self.num_agents):
            if self.agent_charging_status[i] > 0:
                self.agent_charging_status[i] -= 1
                if self.agent_charging_status[i] == 0:
                    self.agent_energy[i] = self.agent_energy_max[i]
        
        # === å†²çªè§£å†³ï¼ˆåä½œæ¨¡å—ï¼‰===
        if self.reward_config.enable_collaboration:
            rewards = self._handle_conflicts_with_rewards(actions, rewards)
        else:
            # ç®€å•å†²çªå¤„ç†ï¼šéšæœºé€‰æ‹©ä¸€ä¸ªæ™ºèƒ½ä½“
            self._handle_conflicts_simple(actions)
        
        # === å¤„ç†æ™ºèƒ½ä½“åŠ¨ä½œ ===
        for i, target in enumerate(actions):
            if self.agent_charging_status[i] > 0:
                continue
                
            prev_pos = self.agent_positions[i].copy()
            self.agent_paths[i].append(prev_pos)
            
            # === å¤„ç†è¿”å›åŸºåœ°åŠ¨ä½œ ===
            if target == self.num_points + 1:
                rewards[i] += self._handle_return_home(i, R)
                
            # === å¤„ç†å……ç”µåŠ¨ä½œ ===  
            elif target == self.num_points:
                rewards[i] += self._handle_charging(i, R)
                
            # === å¤„ç†ä»»åŠ¡ç‚¹åŠ¨ä½œ ===
            elif target < self.num_points:
                # è®°å½•ä»»åŠ¡è´£ä»»åˆ†é…
                self._assign_task_responsibility(target, i, current_step)
                rewards[i] += self._handle_task_collection(i, target, R)
            
            # === è¡Œä¸ºå¡‘é€ å¥–åŠ±ï¼ˆå¦‚æœå¯ç”¨ï¼‰===
            if self.reward_config.enable_behavior_shaping:
                rewards[i] += self._calculate_shaping_rewards(i, target, prev_pos, R)
        
        # === æ—¶é—´çª—å£å¤„ç† ===
        self._update_time_windows(R, rewards)
        
        # === æ£€æŸ¥ä»»åŠ¡å®ŒæˆçŠ¶æ€ ===
        obs = self._get_obs()
        all_tasks_processed = np.all(self.done_points == 1)
        all_agents_at_base = np.all([np.array_equal(pos, self.central_station) for pos in self.agent_positions])
        mission_accomplished = all_tasks_processed and all_agents_at_base
        
        # === è¶…æ—¶æ£€æŸ¥ ===
        is_timeout = current_step >= max_steps - 1
        tasks_not_finished_in_time = False
        
        if all_tasks_processed:
            if not hasattr(self, '_tasks_completed_step'):
                self._tasks_completed_step = current_step
            if current_step - self._tasks_completed_step > 100:
                tasks_not_finished_in_time = True
        
        done = mission_accomplished or is_timeout or tasks_not_finished_in_time
        
        # === ç»ˆå±€å¥–åŠ±å¤„ç† ===
        if done:
            rewards += self._calculate_final_rewards(mission_accomplished, is_timeout, 
                                                   tasks_not_finished_in_time, R)
        
        return obs, rewards, done
    
    def _handle_conflicts_with_rewards(self, actions, rewards):
        """å¤„ç†å†²çªå¹¶åº”ç”¨åä½œå¥–åŠ±"""
        task_targets = actions[actions < self.num_points]
        unique_targets, counts = np.unique(task_targets, return_counts=True)
        conflicted_targets = unique_targets[counts > 1]
        
        assigned_tasks = set()
        
        for target_idx in conflicted_targets:
            competing_agent_indices = np.where(actions == target_idx)[0]
            self.conflict_count += 1
            
            # æ™ºèƒ½å†²çªè§£å†³
            winner_idx = self._resolve_conflict_intelligently(target_idx, competing_agent_indices)
            assigned_tasks.add(target_idx)
            
            # ä¸ºè´¥è€…åˆ†é…æ›¿ä»£ä»»åŠ¡
            losers = [idx for idx in competing_agent_indices if idx != winner_idx]
            for agent_idx in losers:
                alternative = self._find_best_alternative_task_intelligent(agent_idx, assigned_tasks)
                if alternative is not None:
                    actions[agent_idx] = alternative
                    assigned_tasks.add(alternative)
                    rewards[agent_idx] += self.reward_constants['P_CONFLICT'] * 0.3  # å‡è½»å†²çªæƒ©ç½š
                else:
                    actions[agent_idx] = self.num_points
                    rewards[agent_idx] += self.reward_constants['P_CONFLICT']
        
        return rewards
    
    def _handle_conflicts_simple(self, actions):
        """ç®€å•å†²çªå¤„ç†ï¼šéšæœºåˆ†é…"""
        task_targets = actions[actions < self.num_points]
        unique_targets, counts = np.unique(task_targets, return_counts=True)
        conflicted_targets = unique_targets[counts > 1]
        
        for target_idx in conflicted_targets:
            competing_indices = np.where(actions == target_idx)[0]
            winner = np.random.choice(competing_indices)
            
            for idx in competing_indices:
                if idx != winner:
                    actions[idx] = self.num_points  # è®©è´¥è€…å›å®¶
    
    def _handle_return_home(self, agent_id, R):
        """å¤„ç†è¿”å›åŸºåœ°çš„åŠ¨ä½œ"""
        reward = 0.0
        
        # ç§»åŠ¨åˆ°åŸºåœ°
        dist = self._distance(self.agent_positions[agent_id], self.central_station)
        energy_cost = dist * self.energy_cost_factor[agent_id]
        
        if self.agent_energy[agent_id] < energy_cost:
            return R['P_ENERGY_DEPLETED']
        
        self.agent_positions[agent_id] = self.central_station
        self.agent_energy[agent_id] -= energy_cost
        
        # è½½é‡å¥–åŠ±å¤„ç†
        if self.agent_loads[agent_id] > 0:
            load_ratio = self.agent_loads[agent_id] / self.agent_capacity[agent_id]
            
            # åŸºç¡€è¿”å›å¥–åŠ±
            reward += R['R_RETURN_HOME_COEFF'] * self.agent_loads[agent_id]
            
            # è½½é‡æ•ˆç‡æ¨¡å—å¥–åŠ±
            if self.reward_config.enable_load_efficiency:
                # æ»¡è½½å¥–åŠ±
                if load_ratio >= 0.6:
                    reward += R['R_FULL_LOAD_BONUS']
                
                # å¤§è½½é‡æ™ºèƒ½ä½“é¢å¤–å¥–åŠ±
                if agent_id >= 3 and load_ratio >= 0.8:
                    reward += R['R_ROLE_CAPACITY_BONUS']
                
                # é«˜å®¹é‡æ™ºèƒ½ä½“ä½è½½æƒ©ç½š
                if agent_id >= 3 and load_ratio < 0.6:
                    penalty_intensity = 1.0 - load_ratio
                    low_load_penalty = R['P_LOW_LOAD_HEAVY_AGENT'] * penalty_intensity
                    reward += low_load_penalty
            
            # è®°å½•è½½é‡åˆ©ç”¨ç‡
            self.agent_load_utilization[agent_id].append(load_ratio)
            
            # æ¸…ç©ºè½½é‡å’Œæ ·æœ¬
            self.agent_loads[agent_id] = 0
            self.agent_samples[agent_id] = []
        else:
            # ç©ºè½½è¿”å›å¤„ç†
            if self.reward_config.enable_load_efficiency:
                # ç©ºè½½æƒ©ç½š
                agent_type = "é‡è½½" if agent_id >= 3 else "å¿«é€Ÿ"
                final_penalty = R['P_EMPTY_RETURN']
                reward += final_penalty
                
                debug_print(f"   âŒ ç©ºè½½å›å®¶ ({agent_type}æ™ºèƒ½ä½“, æƒ©ç½š: {final_penalty:.1f})")
        
        return reward
    
    def _handle_charging(self, agent_id, R):
        """å¤„ç†å……ç”µåŠ¨ä½œ"""
        if not np.array_equal(self.agent_positions[agent_id], self.central_station):
            return 0.0  # ä¸åœ¨åŸºåœ°æ— æ³•å……ç”µ
        
        if self.agent_energy[agent_id] >= self.agent_energy_max[agent_id] * 0.9:
            return 0.0  # èƒ½é‡å……è¶³æ— éœ€å……ç”µ
        
        # å¼€å§‹å……ç”µ
        charge_time = self.fast_charge_time if agent_id < 3 else self.slow_charge_time
        self.agent_charging_status[agent_id] = charge_time
        self.agent_charge_counts[agent_id] += 1
        
        return 0.0  # å……ç”µæœ¬èº«ä¸ç»™å¥–åŠ±
    
    def _handle_task_collection(self, agent_id, target, R):
        """å¤„ç†ä»»åŠ¡é‡‡é›†åŠ¨ä½œ"""
        reward = 0.0
        
        # è·å–ä»»åŠ¡ä¼˜å…ˆçº§ï¼ˆåœ¨å¼€å§‹å°±è·å–ï¼Œç”¨äºç»Ÿè®¡ï¼‰
        priority = self.priority[target]
        
        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å·²å®Œæˆ
        if self.done_points[target] == 1:
            if self.reward_config.enable_advanced_penalties:
                reward += R['P_INVALID_TASK_ATTEMPT']
            else:
                reward += -10.0  # è½»å¾®æƒ©ç½š
            return reward
        
        # æ£€æŸ¥æ˜¯å¦ä¼šè¶…è½½
        if self.agent_loads[agent_id] + self.samples[target] > self.agent_capacity[agent_id]:
            if self.reward_config.enable_advanced_penalties:
                reward += R['P_OVERLOAD_ATTEMPT']
            else:
                reward += -10.0  # è½»å¾®æƒ©ç½š
            return reward
        
        # ç§»åŠ¨åˆ°ä»»åŠ¡ç‚¹
        dist = self._distance(self.agent_positions[agent_id], self.points[target])
        energy_cost = dist * self.energy_cost_factor[agent_id]
        
        if self.agent_energy[agent_id] < energy_cost:
            return R['P_ENERGY_DEPLETED']
        
        # æ‰§è¡Œç§»åŠ¨å’Œé‡‡é›†
        self.agent_positions[agent_id] = self.points[target]
        self.agent_energy[agent_id] -= energy_cost
        self.agent_loads[agent_id] += self.samples[target]
        self.agent_samples[agent_id].append(target)
        self.done_points[target] = 1
        self.successfully_collected_points[target] = 1
        
        # åŸºç¡€é‡‡é›†å¥–åŠ±
        reward += R['R_COLLECT_BASE']
        
        # è§’è‰²ä¸“ä¸šåŒ–å¥–åŠ±
        if self.reward_config.enable_role_specialization:
            if priority >= 4 and agent_id < 3:  # å¿«é€Ÿæ™ºèƒ½ä½“å¤„ç†é«˜ä¼˜å…ˆçº§
                reward += R['R_FAST_AGENT_HIGH_PRIORITY']
                
                # æ—¶é—´å¥–åŠ±ï¼šåŸºäºå‰©ä½™æ—¶é—´çª—å£ç»™äºˆå¥–åŠ±
                remaining_time = self.time_windows[target]
                # æ ¹æ®ä»»åŠ¡ä¼˜å…ˆçº§è·å–åˆå§‹æ—¶é—´çª—å£
                initial_time = (3*60) if priority == 1 else ((10*60) if priority == 2 else (30*60))
                time_utilization = remaining_time / initial_time
                
                # å¦‚æœåœ¨æ—¶é—´çª—å£çš„å‰50%å®Œæˆï¼Œç»™äºˆæ—¶é—´å¥–åŠ±
                if time_utilization > 0.5:
                    time_bonus = R['R_ROLE_SPEED_BONUS'] * time_utilization
                    reward += time_bonus
            
            elif priority >= 4 and agent_id >= 3:  # é‡è½½æ™ºèƒ½ä½“å¤„ç†é«˜ä¼˜å…ˆçº§
                reward += R['P_HEAVY_AGENT_HIGH_PRIORITY']
            
            elif priority == 3 and agent_id >= 3 and self.samples[target] >= 3:
                # é‡è½½æ™ºèƒ½ä½“å¤„ç†é«˜è½½é‡ä½ä¼˜å…ˆçº§ä»»åŠ¡
                reward += R['R_ROLE_CAPACITY_BONUS'] * 0.5
        
        # è®°å½•ä»»åŠ¡å®Œæˆä¿¡æ¯
        self.agent_task_completion[agent_id] += 1
        if priority >= 4:
            self.agent_high_priority_tasks[agent_id] += 1
        
        return reward
    
    def _calculate_shaping_rewards(self, agent_id, target, prev_pos, R):
        """è®¡ç®—è¡Œä¸ºå¡‘é€ å¥–åŠ±"""
        if not self.reward_config.enable_behavior_shaping:
            return 0.0
        
        reward = 0.0
        current_pos = self.agent_positions[agent_id]
        
        # æ¥è¿‘ç›®æ ‡ä»»åŠ¡ç‚¹çš„å¥–åŠ±
        if target < self.num_points and not self.done_points[target]:
            target_pos = self.points[target]
            prev_dist = self._distance(prev_pos, target_pos)
            curr_dist = self._distance(current_pos, target_pos)
            
            if curr_dist < prev_dist:
                approach_reward = (prev_dist - curr_dist) * R['R_COEFF_APPROACH_TASK']
                reward += min(approach_reward, R['REWARD_SHAPING_CLIP'])
        
        # æ¥è¿‘åŸºåœ°çš„å¥–åŠ±ï¼ˆæœ‰è½½é‡æˆ–ä½èƒ½é‡æ—¶ï¼‰
        elif target == self.num_points + 1:
            base_dist_prev = self._distance(prev_pos, self.central_station)
            base_dist_curr = self._distance(current_pos, self.central_station)
            
            if base_dist_curr < base_dist_prev:
                # æœ‰è½½é‡æ—¶çš„æ¥è¿‘å¥–åŠ±
                if self.agent_loads[agent_id] > 0:
                    approach_reward = (base_dist_prev - base_dist_curr) * R['R_COEFF_APPROACH_HOME_LOADED']
                    reward += min(approach_reward, R['REWARD_SHAPING_CLIP'])
                
                # ä½èƒ½é‡æ—¶çš„æ¥è¿‘å¥–åŠ±
                energy_ratio = self.agent_energy[agent_id] / self.agent_energy_max[agent_id]
                if energy_ratio < 0.3:
                    approach_reward = (base_dist_prev - base_dist_curr) * R['R_COEFF_APPROACH_HOME_LOW_ENERGY']
                    reward += min(approach_reward, R['REWARD_SHAPING_CLIP'])
        
        return reward
    
    def _update_time_windows(self, R, rewards):
        """æ›´æ–°æ—¶é—´çª—å£å¹¶å¤„ç†è¶…æ—¶"""
        for i in range(self.num_points):
            if self.done_points[i] == 0:
                self.time_windows[i] = max(0, self.time_windows[i] - 1)
                
                if self.time_windows[i] <= 0:
                    # ä»»åŠ¡è¶…æ—¶
                    assigned_agent = self._get_last_assigned_agent(i)
                    if assigned_agent is not None:
                        timeout_penalty = R['P_TIMEOUT_BASE'] * (4 - self.priority[i])
                        rewards[assigned_agent] += timeout_penalty
                        debug_print(f"â° ä»»åŠ¡ç‚¹ {i} (ä¼˜å…ˆçº§:{self.priority[i]}) è¶…æ—¶ï¼Œæ™ºèƒ½ä½“ {assigned_agent} æ‰¿æ‹…è´£ä»»ï¼Œæƒ©ç½š: {timeout_penalty:.1f}")
                        
                        # æ·»åŠ ä»»åŠ¡åˆ†é…å†å²ä¿¡æ¯ç”¨äºè°ƒè¯•
                        if hasattr(self, 'task_assignment_history') and i in self.task_assignment_history:
                            history = self.task_assignment_history[i]
                            debug_print(f"   ğŸ“‹ ä»»åŠ¡åˆ†é…å†å²: {history}")
                    else:
                        debug_print(f"â° ä»»åŠ¡ç‚¹ {i} (ä¼˜å…ˆçº§:{self.priority[i]}) è¶…æ—¶ï¼Œä½†æ— æ³•æ‰¾åˆ°è´Ÿè´£çš„æ™ºèƒ½ä½“")
                    
                    self.done_points[i] = 1  # æ ‡è®°ä¸ºå·²å¤„ç†ï¼ˆè¶…æ—¶ï¼‰
    
    def _calculate_final_rewards(self, mission_accomplished, is_timeout, tasks_not_finished_in_time, R):
        """è®¡ç®—ç»ˆå±€å¥–åŠ±"""
        final_rewards = np.zeros(self.num_agents)
        
        if mission_accomplished:
            # ä»»åŠ¡å®Œæˆå¥–åŠ±
            collected_count = np.sum(self.successfully_collected_points)
            processed_count = np.sum(self.done_points)
            timeout_count = processed_count - collected_count
            
            if timeout_count == 0:
                # å®Œç¾å®Œæˆ
                adjusted_reward = R['FINAL_BONUS_ACCOMPLISHED']
            else:
                # éƒ¨åˆ†è¶…æ—¶ï¼ŒæŒ‰æ¯”ä¾‹è°ƒæ•´
                timeout_penalty_ratio = timeout_count / len(self.done_points)
                adjusted_reward = R['FINAL_BONUS_ACCOMPLISHED'] * (1.0 - timeout_penalty_ratio * 0.3)
            
            # æŒ‰è´¡çŒ®åˆ†é…å¥–åŠ±
            agent_contributions = np.array([self.agent_task_completion[i] for i in range(self.num_agents)])
            total_contribution = np.sum(agent_contributions)
            
            if total_contribution > 0:
                contribution_ratios = agent_contributions / total_contribution
                final_rewards = adjusted_reward * contribution_ratios
            else:
                equal_reward = adjusted_reward / self.num_agents
                final_rewards.fill(equal_reward)
        
        elif is_timeout and self.reward_config.enable_advanced_penalties:
            # è¶…æ—¶æƒ©ç½š
            num_unprocessed = np.sum(self.done_points == 0)
            if num_unprocessed > 0:
                final_penalty = -50.0 * num_unprocessed  # ä½¿ç”¨å›ºå®šæƒ©ç½šå€¼
                final_rewards.fill(final_penalty / self.num_agents)
            
            # æœªè¿”å›åŸºåœ°çš„é¢å¤–æƒ©ç½š
            for i in range(self.num_agents):
                if not np.array_equal(self.agent_positions[i], self.central_station):
                    final_rewards[i] += -100.0
        
        return final_rewards
    
    def get_experiment_summary(self):
        """è·å–å½“å‰å®éªŒçš„æ€»ç»“ä¿¡æ¯"""
        analytics = self.get_collaboration_analytics()
        
        summary = {
            'experiment_level': config.REWARD_EXPERIMENT_LEVEL,
            'description': self.reward_config.get_experiment_description(),
            'enabled_modules': {
                'load_efficiency': self.reward_config.enable_load_efficiency,
                'role_specialization': self.reward_config.enable_role_specialization,
                'collaboration': self.reward_config.enable_collaboration,
                'behavior_shaping': self.reward_config.enable_behavior_shaping,
                'advanced_penalties': self.reward_config.enable_advanced_penalties,
            },
            'key_metrics': self.reward_config.get_key_metrics(),
            'collaboration_analytics': analytics
        }
        
        return summary
    
    def _assign_task_responsibility(self, task_id, agent_id, current_step):
        """
        åˆ†é…ä»»åŠ¡è´£ä»»ç»™æ™ºèƒ½ä½“ï¼ˆç»§æ‰¿è‡ªçˆ¶ç±»çš„åŠŸèƒ½ï¼‰
        """
        # ç¡®ä¿è´£ä»»è¿½è¸ªç³»ç»Ÿå·²åˆå§‹åŒ–
        if not hasattr(self, 'task_assignments'):
            self.task_assignments = {}
            self.task_assignment_history = {}
            self.task_assignment_timestamp = {}
        
        # è®°å½•å½“å‰è´Ÿè´£è¯¥ä»»åŠ¡çš„æ™ºèƒ½ä½“
        self.task_assignments[task_id] = agent_id
        self.task_assignment_timestamp[task_id] = current_step
        
        # è®°å½•åˆ†é…å†å²ï¼ˆç”¨äºè°ƒè¯•å’Œåˆ†æï¼‰
        if task_id not in self.task_assignment_history:
            self.task_assignment_history[task_id] = []
        self.task_assignment_history[task_id].append((agent_id, current_step))
    
    def _get_last_assigned_agent(self, point_id):
        """
        è·å–æœ€åè¢«åˆ†é…ç»™è¯¥ä»»åŠ¡çš„æ™ºèƒ½ä½“ï¼ˆç»§æ‰¿è‡ªçˆ¶ç±»çš„åŠŸèƒ½ï¼‰
        """
        # ä¼˜å…ˆä½¿ç”¨ä»»åŠ¡è´£ä»»åˆ†é…è®°å½•
        if hasattr(self, 'task_assignments') and point_id in self.task_assignments:
            return self.task_assignments[point_id]
        
        # å¦‚æœæ²¡æœ‰åˆ†é…è®°å½•ï¼Œä½¿ç”¨å®é™…è®¿é—®è€…è®°å½•ï¼ˆå‘åå…¼å®¹ï¼‰
        return self.point_last_visitor.get(point_id, None)
